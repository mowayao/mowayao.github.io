<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Page 4 | Mowayao&#39;s Blog</title>
  <meta name="author" content="Mowayao">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Mowayao&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110229492-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Mowayao&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class="fa fa-user"></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>一往无前虎山行<span class="blink-fast">∎</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">
      <i class="fa fa-heart blink-slow"></i>
      一往无前虎山行
</div>    
		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/" >Single Shot Scale-invariant Face Detector</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-01  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>The authors propose to tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besides, they try to improve the recall rate of small faces by a scale compensation anchor matching strategy. Max-out background label is used to reduce the false positive rate of small faces.</p>
<p>Key points:</p>
<ul>
<li>VGG net (throgh Pool5 layer) and some extra convolutional layers</li>
<li>Anchor  is 1:1 aspect ratio (face annotation)</li>
<li>two stages to improve the anchor matching strategy<ul>
<li>stage one: decrese the jaccord overlap threshold from 0.5 to 0.35</li>
<li>stage two: decrese the threshold to 0.1 and sort to select the top-N</li>
</ul>
</li>
<li>max-out operation is performed on the background label scores</li>
</ul>
<p>model architecture:</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-01%20%E4%B8%8B%E5%8D%887.46.53.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/10/20/A-List-of-Saliency-Detection-Papers/" >A List of Saliency Detection Papers</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-10-20  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<ol>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Deep%20Spatial%20Contextual%20Long-term%20Recurrent%20Convolutional%20Network%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Fast%20and%20Compact%20Saliency%20Score%20Regression%20Network%20Based%20on%20Fully%20Convolutional%20Network.pdf" target="_blank" rel="external">A Fast and Compact Saliency Score Regression Network Based on Fully Convolutional Network</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Amulet.pdf" target="_blank" rel="external">Amulet</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/DHSNet:%20Deep%20Hierarchical%20Saliency%20Network%20for%20Salient%20Object%20Detection%20.pdf" target="_blank" rel="external">DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Group-wise%20Deep%20Co-saliency%20Detection.pdf" target="_blank" rel="external">Group-wise Deep Co-saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Large-Scale%20Optimization%20of%20Hierarchical%20Features%20for%20Saliency%20Prediction%20in%20Natural%20Images.pdf" target="_blank" rel="external">Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Learning%20Uncertain%20Convolutional%20Features%20for%20Accurate%20Saliency%20Detection.pdf" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/PiCANet.pdf" target="_blank" rel="external">PiCANet</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Recurrent%20Attentional%20Networks%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">Recurrent Attentional Networks for Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/SalGAN:%20Visual%20Saliency%20Prediction%20with%20Generative%20Adversarial%20Networks.pdf" target="_blank" rel="external">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Forward%20and%20Backward%20Cues%20in%20Deep-CNNs.pdf" target="_blank" rel="external">Saliency Detection by Forward and Backward Cues in Deep-CNNs</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Multi-Context%20Deep%20Learning.pdf" target="_blank" rel="external">Saliency Detection by Multi-Context Deep Learning.pdf</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Shallow%20and%20Deep%20Convolutional%20Networks%20for%20Saliency%20Prediction.pdf" target="_blank" rel="external">Shallow and Deep Convolutional Networks for Saliency Prediction</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Supervised%20Adversarial%20Networks%20for%20Image%20Saliency%20Detection.pdf" target="_blank" rel="external">Supervised Adversarial Networks for Image Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Two-Stream%20Convolutional%20Networks%20for%20Dynamic%20Saliency%20Prediction.pdf" target="_blank" rel="external">Two-Stream Convolutional Networks for Dynamic Saliency Prediction</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Detection%20Based%20on%20Multiscale%20Deep%20CNN%20Features.pdf" target="_blank" rel="external">Visual Saliency Detection Based on Multiscale Deep CNN Features</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Prediction%20Using%20a%20Mixture%20of%20Deep%20Neural%20Networks.pdf" target="_blank" rel="external">Visual Saliency Prediction Using a Mixture of Deep Neural Networks</a></li>
</ol>

	
	</div>
  <a type="button" href="/2017/10/20/A-List-of-Saliency-Detection-Papers/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/09/05/图个新鲜/" >图个新鲜</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-09-05  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/2017-09-05%2017-15-19%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/09/05/图个新鲜/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/09/05/overview-of-object-detection/" >Overview of Object Detection</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-09-05  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>主要有三个步骤：</p>
<ol>
<li>用selective search提取可能的objects<ol>
<li>使用<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph Based Image Segmentation</a>中的方法来得到region</li>
<li>得到所有region之间两两的相似度</li>
<li>合并最像的两个region</li>
<li>重新计算新合并region与其他region的相似度</li>
<li>重复上述过程直到整张图片都聚合成一个大的region</li>
<li>使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果</li>
</ol>
</li>
<li>用CNN提取特征</li>
<li>用SVM对区域进行分类</li>
</ol>
<p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/rcnn.jpg" alt="[Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; 2014.](https://arxiv.org/abs/1311.2524)"></p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>在feature map加入RoI Pooling,然今后做分类和回归（位置），这样可以end-to-end的训练了，缺点是依然依赖于selective search</p>
<p>每一个RoI都有一个四元组$（r,c,h,w）$表示，其中$（r，c）$表示左上角，而$（h，w）$则代表高度和宽度。这一层使用最大池化（max pooling）来将RoI区域转化成固定大小的$H<em>W$的特征图。假设一个RoI的窗口大小为$h</em>w$,则转换成$H<em>W$之后，每一个网格都是一个$h/H </em> w/W$大小的子网，利用最大池化将这个子网中的值映射到$H*W$窗口即可。Pooling对每一个特征图通道都是独立的</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/ROI.png" alt=""></p>
<p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/fastrcnn.jpg" alt=""></p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>加入region proposal network，为了代替selective search使得模型能够完全的end-to-end的训练。</p>
<p>这样的话就存在４个loss:</p>
<ol>
<li>RPN分类：是否是object</li>
<li>RPN box坐标回归</li>
<li>object分类</li>
<li>最终的坐标回归</li>
</ol>
<p><img src="http://shartoo.github.io/images/blog/rcnn9.png" alt=""></p>
<p>Anchor:</p>
<p>Anchors是一组大小固定的参考窗口：三种尺度{ $128^2，256^2，512^2$ }×三种长宽比{1:1，1:2，2:1}，如下图所示，<strong>表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小</strong>，相当于模板，对任意图像任意滑窗位置都是这9种模板。<strong>继而根据图像大小计算滑窗中心点对应原图区域的中心点</strong>，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，由此原图位置并根据与Ground Truth重复率贴上正负标签，让RPN学习该Anchors是否有物体即可。对于每个滑窗位置，产生<strong>k=9</strong>个anchor对于一个大小为$W*H$的卷积feature map，总共会产生$WHk$个anchor。</p>
<p><img src="http://shartoo.github.io/images/blog/rcnn12.png" alt=""></p>
<p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/fasterrcnn.jpg" alt="[Ren, Shaoqing, et al. &quot;Faster R-CNN: Towards real-time object detection with region proposal networks.&quot; 2015.](https://arxiv.org/abs/1506.01497)"></p>
<p><img src="http://img.blog.csdn.net/20160414164536029" alt=""></p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><strong>同时采用lower和upper的feature map做检测</strong></p>
<p>假设每个feature map cell有k个default box，那么对于每个default box都需要预测c个类别score和4个offset，那么如果一个feature map的大小是$m\times n$，也就是有<strong>$m\times n$</strong>个feature map cell，那么这个feature map就一共有$（c+4)\times k\times m\times n$ 个输出。这些输出个数的含义是：采用$3\times3$的卷积核对该层的feature map卷积时卷积核的个数，包含两部分：数量$c\times k\times m\times n$是confidence输出，表示每个default box的confidence，也就是类别的概率；数量$4\times k\times m\times n$是localization输出，表示每个default box回归后的坐标）。训练中还有一个东西：<strong>prior box</strong>，是指实际中选择的default box（每一个feature map cell 不是k个default box都取）。</p>
<ul>
<li>feature map cell 就是将 feature map 切分成 8×8 或者 4×4 之后的一个个格子；</li>
<li>而 default box 就是每一个格子上，一系列固定大小的 box，即图中虚线所形成的一系列 boxes。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20160918092529925" alt=""></p>
<p><img src="http://img.blog.csdn.net/20160918092701558" alt=""></p>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p><img src="http://upload-images.jianshu.io/upload_images/75110-91ee171b49f3ea20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>YOLO首先将图像分为S×S的格子（grid cell）。如果一个目标的中心落入格子，该格子就负责检测该目标。每一个格子（grid cell）预测bounding boxes和该boxes的置信值（confidence score）。置信值代表box包含一个目标的置信度。然后，我们定义置信值为。如果没有目标，置信值为零。另外，我们希望预测的置信值和ground truth的intersection over union (IOU)相同。</p>
<p>每一个bounding box包含5个值：$x，y，w，h$和confidence。$（x，y）$代表与格子相关的box的中心。$（w，h）$为与全图信息相关的box的宽和高。confidence代表预测boxes的IOU和gound truth。</p>
<p>每个格子（grid cell）预测条件概率值C($Pr(Class_i|Object) $)。概率值C代表了格子包含一个目标的概率，每一格子只预测一类概率。在测试时，每个box通过类别概率和box置信度相乘来得到特定类别置信分数：<br>$$<br>Pr(Class_i|Object) \cdot Pr(Object)\cdot IOU_{pred}^{truth} = Pr(Class_i)\cdot IOU_{pred}^{truth}<br>$$<br>它将图片划分为S×S的网格，对于每个网格单元预测边界框(B)、边界框的置信度以及类别概率(C)，因此这些预测值可以表示为S×S×(B∗5+C)的张量。</p>

	
	</div>
  <a type="button" href="/2017/09/05/overview-of-object-detection/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/18/如何在圆内均匀采样/" >如何在圆内均匀采样</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>这个问题之前在面试网易游戏的时候碰到过，当时只想了一个很朴素(naive)的做法，现在有时间重新推导了一下：</p>
<p>假设圆是一个单元圆，半径为1, 面积为$\pi$。</p>
<p>当时面试的时候想的方法是用一个长度为1的外接正方形来代替采样，如果在圆内，则返回，不然继续采样，这样的采样方法明显是不稳定的，但是期望的采样步数很容易计算，是$\frac{4}{\pi}$。</p>
<p>后来想到说可以先采样角度，每个角度确定一个半径，再在半径上均匀采样，面试官有提示说在半径上是均匀的吗？ 明显在半径上采样是不均匀的，因为在半径上的每一个点对应的周长是不一样的！！</p>
<p>正确的姿势是这样的：</p>
<p>我们计算长度为$r(0\le r\le1)$的概率为$p(r)$:</p>
<p>首先我们需要计算落在$r\sim r+\Delta r$的概率，很直观，就是面积/$\pi$，然后因为均匀采样所以需要除以$\Delta r$求得$p(r)$也就是：<br>$$<br>p(r\sim r+\Delta r) = \lim_{\Delta r\rightarrow0}\frac{\pi(r+\Delta r)^2-\pi r^2}{\pi}<br>$$</p>
<p>$$<br>p(r) = \lim_{\Delta r\rightarrow0}\frac{\pi(r+\Delta r)^2-\pi r^2}{\pi \Delta r}=2r<br>$$</p>
<p>接下来就是求$p(r)$的CDF,$P(r)$，积分即可：<br>$$<br>P(r) = \int_0^r p(x) dx=r^2<br>$$<br>然后求得它的逆函数$P^{-1}(r)$:</p>
<p>$$P^{-1}(r) = \sqrt r$$</p>
<p>算到这里，答案呼之欲出，我们用一个随机变量$\zeta$ 在$[0,1]$均匀采样，然后在通过$r = \sqrt \zeta$求得r,为什么是均匀的呢，只要把$\sqrt \zeta$带入$P(r)$就可以发现$P(r)=\zeta$。</p>

	
	</div>
  <a type="button" href="/2017/05/18/如何在圆内均匀采样/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/17/一个离散概率分布中采样/" >一个离散概率分布中采样</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-17  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>先考虑一个简单的例子：</p>
<p>一个n-sided dice. 其每一面是均匀的，也就是每一面的概率都是$\frac{1}{n}$,我们可以把区间[0,1)分成n份，采样的过程就可以变成：从[0,1)采样得到x，然后返回$\lfloor n\times x\rfloor$。</p>
<blockquote>
<h4 id="Algorithm-Simulating-a-Fair-Die"><a href="#Algorithm-Simulating-a-Fair-Die" class="headerlink" title="Algorithm: Simulating a Fair Die"></a>Algorithm: Simulating a Fair Die</h4><ol>
<li>Generate a uniformly-random value xx in the range [0,1).</li>
<li>Return $⌊x\times n⌋$</li>
</ol>
</blockquote>
<h2 id="朴素的做法"><a href="#朴素的做法" class="headerlink" title="朴素的做法"></a>朴素的做法</h2><p>但如果不是均匀的呢？假设给定各个离散值的概率p(x)，先计算CDF，即 P(x)。然后从[0,1)采样，得到a，我们需要确定a所在的区间，朴素的做法就是二分搜索P（x）（单调性）。所以时间复杂度为$O(\log(n))$</p>
<h2 id="The-Alias-Method"><a href="#The-Alias-Method" class="headerlink" title="The Alias Method"></a>The Alias Method</h2><p>如果想要用$O(1)$的时间采样呢？</p>
<p>考虑如下的情况：</p>
<p>有四个概率：$\frac{1}{2}, \frac{1}{3}, \frac{1}{12},\frac{1}{12}$</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodInitialProbabilities.png" alt=""></p>
<p>首先，将他们归一化，用均值做归一化操作</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodScaled.png" alt=""></p>
<p>先画一个$1\times 4$的矩形：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup.png" alt=""></p>
<p>可以看到$\frac{1}{2}, \frac{1}{3}$并不是完全在矩形内，如果我们允许将自身的矩阵切除，然后补到其他区域内？例如将$\frac{1}{2}$切掉一部分补到最后那个区域：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup2.png" alt=""></p>
<p>到现在，还是在矩阵之外的块，接下来，把$\frac{1}{2}$切掉足够的部分补到第三个中：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup3.png" alt=""></p>
<p>最后：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup4.png" alt=""></p>
<p>完美！</p>
<p>从上述可以看出有几条非常赞的性质：</p>
<ol>
<li>每个概率对应的面积都没有改变，随之对应的就是每个bar都是满的，这样保证了我每次采样都会命中！</li>
<li>每个bar最多有两种颜色。</li>
</ol>
<p>alias method主要依赖于两张表，一张概率表P还有一张alias表 Alias</p>
<p>构建完上述的表格以后，如何采样呢？</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/completedAliasSetup.png" alt=""></p>
<p>首先对每列进行采样，列确定后，再采样，利用P和alias。过程非常简单，时间效率是$O(1)$</p>
<p>接下来就是证明这个alias表和P表是否一定存在！</p>
<blockquote>
<p><strong>Theorem:</strong> Given k width-one rectangles of heights $h_0,h_1,…,h_{k−1}$ such that $\sum_{i=0}^{k-1}h_i=k$, there is a way of cutting the rectangles and distributing them into k columns, each of which has height 1, such that each column contains at most two different rectangles and the $i$th column contains at least one piece of the $i$th rectangle.</p>
</blockquote>
<p>证明：</p>
<p>当k=1的时候，很明显是成立的。</p>
<p>假设当$k=x$的时候成立，那么我们就需要证明$k=x+1$时，是否满足。<br>考虑任一个宽度为$x+1$的矩形，高度分别是：$h_0, h_1, …, h_{k}$, 且满足$\sum_{i = 0}^{k}{h_i} =  x+ 1$,假设一些高度$h_l \le 1$ 还有一些$h_g\ge 1$。不可能同时大于0或者小于0。</p>
<p>接下来就是用$h_g$把$h_l$填满，这样我们就只剩下$x$个未解决的。所以。。成立！</p>
<p>具体的做法如下：</p>
<blockquote>
<h4 id="Algorithm-Naive-Alias-Method"><a href="#Algorithm-Naive-Alias-Method" class="headerlink" title="Algorithm: Naive Alias Method"></a>Algorithm: Naive Alias Method</h4><ul>
<li>Initialization:<ol>
<li>Multiply each probability $p_i$ by n.</li>
<li>Create arrays Alias and Prob, each of size n.</li>
<li>For j=1 to n−1:<ol>
<li>Find a probability pl satisfying $p_l\le1$.</li>
<li>Find a probability $p_g$ (with $l\ne g$) satisfying $p_g\ge1$</li>
<li>Set $Prob[l]=p_l$.</li>
<li>Set $Alias[l]=g$.</li>
<li>Remove $p_l$ from the list of initial probabilities.</li>
<li>Set $p_g:=p_g−(1−p_l)$.</li>
</ol>
</li>
<li>Let i be the last probability remaining, which must have weight 1.</li>
<li>Set $Prob[i]=1$.</li>
</ol>
</li>
<li>Generation:<ol>
<li>Generate a fair die roll from an n-sided die; call the side i.</li>
<li>Flip a biased coin that comes up heads with probability $Prob[i]$.</li>
<li>If the coin comes up “heads,” return i.</li>
<li>Otherwise, return $Alias[i]$.</li>
</ol>
</li>
</ul>
</blockquote>

	
	</div>
  <a type="button" href="/2017/05/17/一个离散概率分布中采样/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/12/cnn-case-study/" >CNN Case Study</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-12  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>结构：</p>
<p>CONV1-&gt;MAX POOL1-&gt;NORM1-&gt;CONV2-&gt;MAX POOL2-&gt;NORM-&gt;CONV3-&gt;CONV4-&gt;CONV5-&gt;MAX POOL3-&gt;FC6-&gt;FC7-&gt;FC8</p>
<p>输入： $227\times 227\times 3$ 的图像</p>
<p>第一层（CONV1），96个$11\times11$的卷积和，stride为4，,因为(227-11)/4+1=55</p>
<p>参数的大小是$11\times11\times3\times96=35K$,输出为$55\times55\times96$</p>
<p>第二层（MAX POOL1）， $3\times 3$， 步数为2，因为(55-3)/2+1=27,所以，输出为$27\times27\times96$</p>
<p>第三层（NORM1）</p>
<p>第四层CONV2，256个$5\times5$的卷积和，stride为1，pad为2，因为（27-5+2*2）/1+1= 27，所以输出为$27\times27\times256$</p>
<p>第五层（MAX POOL2）， $3\times 3$， stride为2，因为(27-3)/2+1=13,所以，输出为$13\times13\times256$</p>
<p>第六层 （NORM2）</p>
<p>第七层（CONV3），384个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times384$</p>
<p>第八层（CONV4），384个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times384$</p>
<p>第九层（CONV5），256个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times256$</p>
<p>第十层（MAX POOL2）， $3\times 3$， 步数为2，因为(13-3)/2+1=6,所以，输出为$6\times6\times256$</p>
<p>第十一层（FC6），4096个neurons</p>
<p>第十二层（FC7）， 4096个neurons</p>
<p>第十三层（FC8）， 1000个neurons</p>
<h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.11.48.png" alt=""></p>
<p>为什么要使用小的卷积核（$3\times3$ conv）?</p>
<p>因为3个$3\times3$stride为1的conv堆起来的receptive field是和$7\times7$的conv layer是一样的，这样的话网络可以更深，同时非线性能力提高，而且前者参数数量为：$3*(3^2C^2)$ 后者为 $7^2C^2$,前者数量较少。</p>
<p>INPUT: $[224\times224\times3]$        memory:  $224<em>224</em>3$=150K   params: 0</p>
<p>CONV3-64:$ [224\times224\times64] $ memory:  $224<em>224</em>64$=3.2M   params:$ (3<em>3</em>3)*64 = 1,728$</p>
<p>CONV3-64:$ [224\times224\times64]$  memory:  $224<em>224</em>64$ =3.2M   params: $(3<em>3</em>64)*64 = 36,864$</p>
<p>POOL2: $[112\times112\times64]$  memory:  $112<em>112</em>64$=800K   params: 0</p>
<p>CONV3-128: $[112\times112\times128]$  memory:  $112<em>112</em>128$=1.6M   params: $(3<em>3</em>64)*128 = 73,728$</p>
<p>CONV3-128: $[112\times112\times128] $ memory: $112<em>112</em>128$=1.6M   params:$ (3<em>3</em>128)*128 = 147,456 $</p>
<p>POOL2:$ [56\times56\times128]$  memory:  $56<em>56</em>128=400K$   params: 0</p>
<p>CONV3-256:$ [56\times56\times256] $ memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>128)*256 = 294,912$</p>
<p>CONV3-256:$ [56\times56\times256]$  memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>256)*256 = 589,824 $</p>
<p>CONV3-256: $[56\times56\times256] $ memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>256)*256 = 589,824$</p>
<p>POOL2: $[28\times28\times256]$  memory:  $28<em>28</em>256=200K$   params: 0</p>
<p>CONV3-512:$ [28\times28\times512] $ memory:  $28<em>28</em>512=400K$   params:$ (3<em>3</em>256)*512 = 1,179,648$</p>
<p>CONV3-512: $[28\times28\times512]$  memory: $ 28<em>28</em>512=400K $  params:$ (3<em>3</em>512)*512 = 2,359,296$</p>
<p>CONV3-512:$ [28\times28\times512]$  memory: $ 28<em>28</em>512=400K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p>
<p>POOL2:$ [14\times14\times512]$  memory:  $14<em>14</em>512=100K$   params: 0 </p>
<p>CONV3-512:$ [14\times14\times512]$  memory: $ 14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p>
<p>CONV3-512:$ [14\times14\times512] $ memory: $ 14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296 $</p>
<p>CONV3-512: $[14\times14\times512]$  memory:  $14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p>
<p>POOL2: $[7\times7\times512] $ memory:  $7<em>7</em>512=25K$  params: 0</p>
<p>FC: $[1\times1\times4096]$  memory:  4096  params: $7<em>7</em>512*4096 = 102,760,448 $</p>
<p>FC: $[1\times1\times4096]$  memory:  4096  params: $4096*4096 = 16,777,216$</p>
<p>FC: $[1\times1\times1000]$  memory:  1000 params: $4096*1000 = 4,096,000 $</p>
<p>总结一下：对于一张图片来说，需要花费的内存是24M*4 bytes = 96MB，而总共的参数有138M </p>
<p>VGG的FC7的特征非常棒！通常用来提特征。</p>
<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>22层，有高效的inception module，没有FC层</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.15.37.png" alt=""></p>
<p>重点分析一下Inception module,下图是一个朴素的inception module</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.12.08.png" alt=""></p>
<p>采用的并行filter运算，有多个receptive field size的卷积，（$1\times1$,$3\times3$,$5\times5$）,从左向右第一个输出的size为$28\times28\times128$，第二个输出的size为$28\times28\times192$,第三个为$28\times28\times96$，第四个为$28\times28\times256$，concate以后的size为：$28\times28\times672$</p>
<p>缺点就是卷积运算过多：</p>
<p>$1\times1$ conv, 128=&gt;   $28\times28\times128\times1\times1\times256$</p>
<p> $3\times3$ conv, 192=&gt; $28\times28\times192\times3\times3\times256$</p>
<p> $5\times5$ conv, 96=&gt; $28\times28\times96\times5\times5\times256$</p>
<p>总共需要854M次运算</p>
<p>而且，最终的输出太大了！我们需要减少feature depth,可以用$1\times1$的卷积（$1\times1$ conv “bottleneck” layers）来解决，例如一个$56\times56\times64$的feature map经过32个$1\times1$以后，得到$56\times56\times32$,这样做就是将深度投影到较低的维度，（feature map的组合）</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.12.36.png" alt=""></p>
<p>卷积运算的数量：</p>
<p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$</p>
<p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$</p>
<p>$[1\times1 conv, 128] $ $28\times28\times128\times1\times1\times256$</p>
<p>$[3\times3 conv, 192]$  $28\times28\times192\times3\times3\times64$</p>
<p>$[5\times5 conv, 96]$  $28\times28\times96\times5\times5\times64$</p>
<p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$ </p>
<p>Total: 358M ops</p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><ul>
<li>每一层CONV层接BN</li>
<li>没有dropout</li>
</ul>
<p>具体见：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.17.12.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/05/12/cnn-case-study/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/11/机器学习中的向量化/" >机器学习中的向量化</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-11  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>以KNN算法为例，在训练过程中，也就是计算$X_{training}$和$X_{test}$的之间的距离的时候，可以用向量化，也就是矩阵运算加速，这里我们假设 $X_{train}\in R^{n\times d}$ ,$X_{test}\in R^{m\times d}$,那么我们就先需要计算一个$n\times m$ 的矩阵，最朴素的做法就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">num_test = X.shape[<span class="number">0</span>]</div><div class="line">num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">dists = np.zeros((num_test, num_train))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_train):</div><div class="line">        dists[i, j] = np.sqrt(np.sum(np.square(X[i]-self.X_train[j])))</div><div class="line">        <span class="keyword">return</span> dists</div></pre></td></tr></table></figure>
<p>如果用向量化呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">num_test = X.shape[<span class="number">0</span>]</div><div class="line">num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">dists = np.sqrt(<span class="number">-2</span>*np.dot(X, self.X_train.T) + np.sum(np.square(self.X_train), axis=<span class="number">1</span>) + np.sum(np.square(X), axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>推导的过程是这样的：</p>
<p>对于第i个$X_{test}$和第j个$X_{train}$的距离来说，可以先将平方展开，可以发现由三部分组成，分别是$X_{test}$的平方，$X_{train}$的平方，两者的乘积。对于第三部分的分析比较简单就是$X_{train}X_{test}^T$, 而对于前面两部分的分析其实就是对于第二个维度求和，然后加到目标矩阵相应的维度即可。</p>
<p>看一下时间的对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Let's compare how fast the implementations are</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_function</span><span class="params">(f, *args)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Call a function f with args and return the time (in seconds) that it took to execute.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">import</span> time</div><div class="line">    tic = time.time()</div><div class="line">    f(*args)</div><div class="line">    toc = time.time()</div><div class="line">    <span class="keyword">return</span> toc - tic</div><div class="line"></div><div class="line">two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)</div><div class="line">print(<span class="string">'Two loop version took %f seconds'</span> % two_loop_time)</div><div class="line">no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)</div><div class="line">print(<span class="string">'No loop version took %f seconds'</span> % no_loop_time)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Two loop version took 21.228456 seconds</div><div class="line">No loop version took 0.182820 seconds</div></pre></td></tr></table></figure>
<p>提升非常明显</p>
<h2 id="Multi-class-Support-Vector-Machine"><a href="#Multi-class-Support-Vector-Machine" class="headerlink" title="Multi-class Support Vector Machine"></a>Multi-class Support Vector Machine</h2><p>那么它的loss function是：<br>$$<br>Li=\sum_{j≠yi}\max(0,s_j−s_{yi}+\Delta)<br>$$<br>$s_j = f(x_i, W)_j$, 这个loss function(hinge loss)其实保证的是label的score是最大的，否则不存在loss，还有梯度。</p>
<p>再转换一下：<br>$$<br>L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)<br>$$<br><img src="http://cs231n.github.io/assets/margin.jpg" alt=""></p>
<p>那么对于$X_{train}\in R^{N\times D}$, $W\in R^{D\times M}$，它的loss和梯度计算过程(朴素方法)如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Structured SVM loss function, naive implementation (with loops).</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></div><div class="line"><span class="string">  of N examples.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></div><div class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></div><div class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></div><div class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">  - reg: (float) regularization strength</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - loss as single float</span></div><div class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></div><div class="line"></div><div class="line">  <span class="comment"># compute the loss and the gradient</span></div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">    scores = X[i].dot(W)</div><div class="line">    correct_class_score = scores[y[i]]</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">      <span class="keyword">if</span> j == y[i]:</div><div class="line">        <span class="keyword">continue</span></div><div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div><div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">        loss += margin</div><div class="line"></div><div class="line">  <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></div><div class="line">  <span class="comment"># to be an average instead so we divide by num_train.</span></div><div class="line">  loss /= num_train</div><div class="line"></div><div class="line">  <span class="comment"># Add regularization to the loss.</span></div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">    scores = X[i].dot(W)</div><div class="line">    correct_class_score = scores[y[i]]</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">      <span class="keyword">if</span> j == y[i]:</div><div class="line">        <span class="keyword">continue</span></div><div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div><div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">        dW[:, j] += X[i]</div><div class="line">        dW[:, y[i]] -= X[i]</div><div class="line">  dW /= num_train</div><div class="line">  dW += reg * W</div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure>
<p>向量化之后就可以这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Structured SVM loss function, vectorized implementation.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs and outputs are the same as svm_loss_naive.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  scores = (X.dot(W)).T</div><div class="line">  margins = np.maximum(<span class="number">0</span>, scores-scores[y, range(num_train)]+<span class="number">1</span>)</div><div class="line">  margins[y, range(num_train)] = <span class="number">0</span></div><div class="line">  loss += np.sum(margins) / num_train</div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</div><div class="line"></div><div class="line">  D = np.zeros_like(margins)</div><div class="line">  D[margins&gt;<span class="number">0</span>] = <span class="number">1</span></div><div class="line">  D[y, range(num_train)] = -np.sum(margins&gt;<span class="number">0</span>, axis=<span class="number">0</span>)</div><div class="line">  dW += np.dot(D, X).T</div><div class="line">  dW /= num_train</div><div class="line">  dW += reg  * W</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure>
<p>比较一下效率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">tic = time.time()</div><div class="line">_, grad_naive = svm_loss_naive(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'Naive loss and gradient: computed in %fs'</span> % (toc - tic))</div><div class="line"></div><div class="line">tic = time.time()</div><div class="line">_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'Vectorized loss and gradient: computed in %fs'</span> % (toc - tic))</div><div class="line"></div><div class="line"><span class="comment"># The loss is a single number, so it is easy to compare the values computed</span></div><div class="line"><span class="comment"># by the two implementations. The gradient on the other hand is a matrix, so</span></div><div class="line"><span class="comment"># we use the Frobenius norm to compare them.</span></div><div class="line">difference = np.linalg.norm(grad_naive - grad_vectorized, ord=<span class="string">'fro'</span>)</div><div class="line">print(<span class="string">'difference: %f'</span> % difference)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Naive loss and gradient: computed in 0.141285s</div><div class="line">Vectorized loss and gradient: computed in 0.007416s</div><div class="line">difference: 0.000000</div></pre></td></tr></table></figure>
<p>提升非常大</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>softmax分类器用的是交叉熵（cross entropy）,有以下的形式：<br>$$<br>L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}<br>$$<br>对这个这个loss函数，可以有两种解释：</p>
<p>信息论的视角，也就是真的分布p和预测分布q之间的交叉熵：<br>$$<br>H(p,q) = - \sum_x p(x) \log q(x)<br>$$<br>p是这样一个向量：只有一个元素是1（$y_i$的位置），其他全是0。q就是模型输出的分布，所以两者相乘，得到上面的结果</p>
<p>还有就是概率的解释，例如下面的表达式：<br>$$<br>P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }<br>$$<br>因此，我们就可以用最大似然估计（MLE）来求解，也就是最小化负的正确标签的log似然。</p>
<p>softmax函数定义了每个类别的概率估计。</p>
<p>朴素的求法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></div><div class="line"><span class="string">  of N examples.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></div><div class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></div><div class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></div><div class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">  - reg: (float) regularization strength</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - loss as single float</span></div><div class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></div><div class="line"><span class="string">  """</span></div><div class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  dW = np.zeros_like(W)</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">    score = X[i].dot(W)</div><div class="line">    score -= np.max(score)</div><div class="line">    exp_score = np.exp(score)</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">      dW[:, j] += X[i] * exp_score[j] / np.sum(exp_score)</div><div class="line">    dW[:, y[i]] -= X[i]</div><div class="line">    loss += -score[y[i]] + np.log(np.sum(exp_score))</div><div class="line">  loss /= num_train</div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W*W)</div><div class="line"></div><div class="line">  dW /= num_train</div><div class="line">  dW += reg * W</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure>
<p>向量化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Softmax loss function, vectorized version.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  dW = np.zeros_like(W)</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  scores = X.dot(W).T</div><div class="line">  scores -= np.max(scores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">  exp_scores = np.exp(scores)</div><div class="line">  loss += np.sum(-scores[y, range(num_train)]) + np.sum(np.log(np.sum(exp_scores, axis=<span class="number">0</span>)))</div><div class="line">  loss /= num_train</div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W*W)</div><div class="line">  D = exp_scores / np.sum(exp_scores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">  D[y, range(num_train)] -= <span class="number">1.0</span></div><div class="line">  dW += D.dot(X).T</div><div class="line">  dW /= num_train</div><div class="line">  dW += reg * W</div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure>
<p>对比一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Now that we have a naive implementation of the softmax loss function and its gradient,</span></div><div class="line"><span class="comment"># implement a vectorized version in softmax_loss_vectorized.</span></div><div class="line"><span class="comment"># The two versions should compute the same results, but the vectorized version should be</span></div><div class="line"><span class="comment"># much faster.</span></div><div class="line">tic = time.time()</div><div class="line">loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'naive loss: %e computed in %fs'</span> % (loss_naive, toc - tic))</div><div class="line"></div><div class="line"><span class="keyword">from</span> cs231n.classifiers.softmax <span class="keyword">import</span> softmax_loss_vectorized</div><div class="line">tic = time.time()</div><div class="line">loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'vectorized loss: %e computed in %fs'</span> % (loss_vectorized, toc - tic))</div><div class="line"></div><div class="line"><span class="comment"># As we did for the SVM, we use the Frobenius norm to compare the two versions</span></div><div class="line"><span class="comment"># of the gradient.</span></div><div class="line">grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord=<span class="string">'fro'</span>)</div><div class="line">print(<span class="string">'Loss difference: %f'</span> % np.abs(loss_naive - loss_vectorized))</div><div class="line">print(<span class="string">'Gradient difference: %f'</span> % grad_difference)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">naive loss: 2.332690e+00 computed in 0.397665s</div><div class="line">vectorized loss: 2.332690e+00 computed in 0.007957s</div><div class="line">Loss difference: 0.000000</div><div class="line">Gradient difference: 0.000000</div></pre></td></tr></table></figure>
<p>效果依然显著</p>
<h2 id="Softmax和SVM的对比"><a href="#Softmax和SVM的对比" class="headerlink" title="Softmax和SVM的对比"></a>Softmax和SVM的对比</h2><p><img src="http://cs231n.github.io/assets/svmvssoftmax.png" alt=""></p>
<p>其实两者性能上的差异非常小，对于SVM来说，如果正确的类的分数已经比其他类高了，那么它的loss为0，同时也没有梯度。而softmax则一直会有梯度，除非概率分布变成one-hot的形式且预测和标签相同。</p>
<blockquote>
<p>the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better.</p>
</blockquote>

	
	</div>
  <a type="button" href="/2017/05/11/机器学习中的向量化/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/03/18/C-测试cache大小/" >C++测试cache大小</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-03-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>思路其实不难，利用cache的性质，如果连续内存能够放到cache，那么随机访问的速度会比较快，反之会较慢，枚举cache的大小即可。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ctime&gt;</span></span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">KB</span><span class="params">(<span class="keyword">int</span> a)</span> </span>&#123;</div><div class="line">	<span class="keyword">return</span> a &lt;&lt; <span class="number">10</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line"></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> kb = <span class="number">1</span>; kb &lt; <span class="number">15</span>; kb++) &#123;</div><div class="line">		<span class="keyword">int</span> sz = KB(<span class="number">1</span>&lt;&lt;kb);</div><div class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; a(sz, <span class="number">1</span>);</div><div class="line">		<span class="keyword">int</span> begin = clock();</div><div class="line">		<span class="keyword">int</span> haha = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; (<span class="number">1</span>&lt;&lt;<span class="number">25</span>); j++) &#123;</div><div class="line">			<span class="keyword">int</span> idx = random()%sz;<span class="comment">//随机存取</span></div><div class="line">			haha += a[idx];</div><div class="line">			idx = random()%sz;</div><div class="line">			haha += a[idx];</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">int</span> end = clock();</div><div class="line">		<span class="keyword">double</span> elapsed_secs = <span class="keyword">double</span>(end - begin) / CLOCKS_PER_SEC;<span class="comment">//时钟</span></div><div class="line">		<span class="built_in">cout</span> &lt;&lt; (<span class="number">1</span>&lt;&lt;kb) &lt;&lt; <span class="string">" KB "</span> &lt;&lt; elapsed_secs &lt;&lt; <span class="string">" sec"</span>&lt;&lt; <span class="built_in">endl</span>;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>输出是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">2 KB 1.07965 sec</div><div class="line">4 KB 1.08865 sec</div><div class="line">8 KB 1.08106 sec</div><div class="line">16 KB 1.08479 sec</div><div class="line">32 KB 1.08296 sec</div><div class="line">64 KB 1.09078 sec</div><div class="line">128 KB 1.09005 sec</div><div class="line">256 KB 1.08952 sec</div><div class="line">512 KB 1.16065 sec</div><div class="line">1024 KB 1.23121 sec</div><div class="line">2048 KB 1.37515 sec</div><div class="line">4096 KB 2.31725 sec</div><div class="line">8192 KB 3.23205 sec</div><div class="line">16384 KB 3.77682 sec</div></pre></td></tr></table></figure></p>
<p>cache大小大约为256KB，查看了一下系统属性</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.19.16.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/03/18/C-测试cache大小/#more" class="btn btn-default more">Read More</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
    
     <a href="/page/3/" type="button" class="btn btn-default"><i class="fa fa-arrow-circle-o-left"></i> Prev</a>
      

        <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
 
          <a type="button" class="btn btn-default disabled">Next<i class="fa fa-arrow-circle-o-right"></i></a>
        

  
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/algorithms/">algorithms<span>21</span></a></li>
		
			<li><a href="/categories/competition/">competition<span>1</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/face-detection/">face detection<span>1</span></a></li>
		
			<li><a href="/tags/GAN/">GAN<span>1</span></a></li>
		
			<li><a href="/tags/notes/">notes<span>6</span></a></li>
		
			<li><a href="/tags/classifcation/">classifcation<span>8</span></a></li>
		
			<li><a href="/tags/classification/">classification<span>2</span></a></li>
		
			<li><a href="/tags/杂/">杂<span>1</span></a></li>
		
			<li><a href="/tags/Deep-Learning/">Deep Learning<span>1</span></a></li>
		
			<li><a href="/tags/summary/">summary<span>2</span></a></li>
		
			<li><a href="/tags/RNN/">RNN<span>1</span></a></li>
		
			<li><a href="/tags/deep-learning/">deep learning<span>27</span></a></li>
		
			<li><a href="/tags/math/">math<span>2</span></a></li>
		
			<li><a href="/tags/paper-notes/">paper notes<span>4</span></a></li>
		
			<li><a href="/tags/computer-vision/">computer vision<span>4</span></a></li>
		
			<li><a href="/tags/machine-learning/">machine learning<span>1</span></a></li>
		
			<li><a href="/tags/paper/">paper<span>1</span></a></li>
		
			<li><a href="/tags/classfication/">classfication<span>3</span></a></li>
		
			<li><a href="/tags/object-detection/">object detection<span>5</span></a></li>
		
			<li><a href="/tags/algorithms/">algorithms<span>2</span></a></li>
		
			<li><a href="/tags/segmentation/">segmentation<span>2</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2018/04/07/YOLO&amp;YOLOv2笔记/" ><i class="fa fa-file-o"></i>YOLO&amp;YOLOv2笔记</a>
      </li>
    
      <li>
        <a href="/2018/04/02/Faster-R-CNN笔记/" ><i class="fa fa-file-o"></i>Faster R-CNN笔记</a>
      </li>
    
      <li>
        <a href="/2018/03/22/Fast-R-CNN笔记/" ><i class="fa fa-file-o"></i>Fast R-CNN笔记</a>
      </li>
    
      <li>
        <a href="/2018/03/20/SPPNet笔记/" ><i class="fa fa-file-o"></i>SPPNet笔记</a>
      </li>
    
      <li>
        <a href="/2018/03/13/RCNN笔记/" ><i class="fa fa-file-o"></i>RCNN笔记</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/mowayao" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.weibo.com/mowayao" title="My weibo account." target="_blank"]);">My Weibo</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 Mowayao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$'], ['\[','\]'] ], 
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
   </html>
