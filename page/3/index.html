<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Page 3 | Mowayao&#39;s Blog</title>
  <meta name="author" content="Mowayao">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Mowayao&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110229492-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Mowayao&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class="fa fa-user"></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>一往无前虎山行<span class="blink-fast">∎</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">
      <i class="fa fa-heart blink-slow"></i>
      一往无前虎山行
</div>    
		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/18/AlexNet算法笔记/" >AlexNet算法笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：ImageNet Classification with Deep Convolutional Neural Networks</p>
<p>链接：<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
<p>AlexNet是发表在NIPS 2012的一篇文章，可以称作是深度学习的经典之作，获得了ImageNet LSVRC-2010的冠军，达到了15.3%的top-5 error。</p>
<p><strong>模型结构：</strong></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndsfv7yy4j31a80fu0y3.jpg" alt=""></p>
<p>下面是模型的具体描述：</p>
<p>$[227\times227\times3]$ 输入<br>$[55\times55\times96]$ CONV1: 96 $11\times11$ filters at stride 4, pad 0   <u>(227-11)/4+1 = 55</u><br>$[27\times27\times96]$  MAX POOL1: $3\times3$ filters at stride 2   <u>(55-3)/2+1=27</u><br>$[27\times27\times96]$ NORM1: Normalization layer<br>$[27\times27\times256]$ CONV2: 256 $5\times5$ filters at stride 1, pad 2   <u>(27+2*2-5)/1 + 1=27</u><br>$[13\times13\times256]$ MAX POOL2: $3\times3$ filters at stride 2   <u>(27-3)/2+1=13</u><br>$[13\times13\times256]$ NORM2: Normalization layer<br>$[13\times13\times384]$ CONV3: 384 $3\times3$ filters at stride 1, pad 1<br>$[13\times13\times384]$ CONV4: 384 $3\times3$ filters at stride 1, pad 1<br>$[13\times13\times256]$ CONV5: 256 $3\times3$ filters at stride 1, pad 1<br>$[6\times6\times256]$ MAX POOL3: $3\times3$ filters at stride 2    <u>(13-3)/2+1=6</u><br>$[4096]$ FC6: 4096 neurons<br>$[4096]$ FC7: 4096 neurons<br>$[1000]$ FC8: 1000 neurons (class scores)</p>
<p>包含了5层卷积层和3层全连接层。</p>
<p><strong>创新点：</strong></p>
<ol>
<li><p>第一次使用了ReLU激活函数。传统的sigmoid和tanh激活函数的问题在于梯度容易饱和，造成训练困难，下图是sigmoid函数的梯度。而$f(x)=\max(0,x)$看出，ReLU是一个非线性激活函数，而且它的梯度不会饱和，当x&gt;0的时候，梯度一直是1，这样和sigmoid和tanh函数相比，加快了训练的速度。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnhlvvatogj30my0egtbr.jpg" alt=""></p>
</li>
<li><p>使用了Norm Layer，对局部区域进行归一化，对相同空间位置上相邻深度的卷积做归一化。$b_{x,y}^i=\frac{a_{x,y}^i}{(k+\alpha\sum_{j=\max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{i,j})^2)^\beta}$,其中$a_{x,y}^i$表示的是第i个通道的卷积核在$(x,y)$位置处的输出结果，随后经过ReLU激活函数作用。a是每一个神经元的激活值，n是kernel的大小，N是kernel总数，k,alpha,beta都是预设的hyper-parameters.，$k=2,n=5,\alpha=1e-4,\beta=0.75$。从公式可以看出，给原来的激活值$a$加了一个权重，生成了新的激活值b,也就是在不同map的同一空间位置进行了归一化，提高了计算效率。但是这些值为什么这么设置就不得而知了。</p>
</li>
<li><p>大量的数据增强，水平翻转，镜像等。调整RGB channel的值，对数据集所有图像的RGB值做PCA变换，完成去噪功能，同时为了保证图像的多样性，在特征值上加了一个随机的尺度因子，每一轮重新生成一个尺度因子，起到了正则化的作用。</p>
</li>
<li><p>Dropout, hidden layer的输出有0.5的几率会被置为0，那些被droped的点不会参与forward pass和backprogation，这样起到了正则化的作用。需要注意的是，在测试过程中，需要将输出乘上0.5。这是因为在训练的过程中，我们只选择了其中的一半，训练出来的结果相当于原来方法的两倍，所以当测试的时候需要乘上0.5来消除这个影响。</p>
</li>
</ol>
<p><strong>训练细节：</strong></p>
<ul>
<li>batch size为128，momentum为0.9，weight decay为0.0005，其实weight decay是l2正则是有区别的，详细可见：<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1711.05101.pdf</a></li>
<li>初始的learning rate设为1e-2, 当验证集的正确率停止的时候乘0.1</li>
</ul>
<p><strong>实验结果：</strong></p>
<p>最终的实验结果见Table 1。可以发现，CNN的结果在Top-1 error和Top-5上都超出了传统方法一大截。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndtpbwzn9j30ke0bcabv.jpg" alt=""></p>
<p>Table 2就是模型ensemble的结果。Averaging the predictions of five similar CNNs gives an error rate of 16.4%。Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndtpewjn8j30te0egq6a.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/18/AlexNet算法笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/18/VGGNet算法笔记/" >VGGNet算法笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：Very Deep Convolutional Networks for Large-Scale Image Recognition</p>
<p>论文链接：<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="external">https://arxiv.org/abs/1409.1556</a></p>
<p>这篇文章发表在ICLR 2015上,作者是Karen Simonyan和Andrew Zisserman，其算法获得了ImageNet ILSVRC-2014的localization task的冠军和classification task的第二名。文章通过堆叠$3\times 3$的卷积，ReLU, $2\times 2$的max pooling逐渐加深网络的深度。所以，它的特点就是连续的Conv运算比较多，计算量比较大(与AlexNet相比)，同时也提高了模型的感受野，能够提取更high-level的特征。VGGNet被提出以后被应用在各种任务中，例如物体分类，物体检测(object proposal生成)，语义分割，特征提取(image retrieval)等任务，都取得了非常好的效果。</p>
<p>Table 1是其VGG Net各个变种的网络结构参数，从左到右分别是A，A-LRN，B，C，D，E这6种，各个模型的深度分别是：11，11，13，16，16，19。可以发现，作者其实将整个网络分成两个部分，第一个部分是卷积层，第二个部分是全连接层，卷积层又分成了5个卷积组，卷积组的feature maps的深度从64逐渐增加到512，所以这5个卷积组的feature maps的深度分别是64，128，256，512，512，每个卷积组后面都会加一个$2\times 2$的non-overlapping的max pooling来降低feature maps的维度。</p>
<p>除此之外，为了 在不影响感受野的前提下，提高决策函数的非线性能力(increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers)，作者还在结构C中加入了$1\times1$的卷积。$1\times1$的卷积也被应用到很多的网络结构中，例如Google Net，Network in Network等。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndp5zk8eaj30t80qcaff.jpg" alt=""></p>
<p>以结构D为例，分析一下消耗的内存和模型的参数量：</p>
<table>
<thead>
<tr>
<th></th>
<th>内存</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input:$224\times 224\times3$</td>
<td>$224\times 224\times3=150k$</td>
<td>0</td>
</tr>
<tr>
<td>Conv3-64:$224\times 224\times64$</td>
<td>$224\times 224\times64=3.2M$</td>
<td>$3\times3\times3\times64=1728$</td>
</tr>
<tr>
<td>Conv3-64:$224\times 224\times64$</td>
<td>$224\times 224\times64=3.2M$</td>
<td>$3\times3\times64\times64=36864$</td>
</tr>
<tr>
<td>maxpool:$112\times112\times64$</td>
<td>$112\times112\times64=800K$</td>
<td>0</td>
</tr>
<tr>
<td>Conv3-128:$112\times112\times128$</td>
<td>$112\times112\times128=1.6M$</td>
<td>$3\times3\times64\times128=73728$</td>
</tr>
<tr>
<td>Conv3-128:$112\times112\times128$</td>
<td>$112\times112\times128=1.6M$</td>
<td>$3\times3\times128\times128=147456$</td>
</tr>
<tr>
<td>maxpool:$56\times56\times128$</td>
<td>$56\times56\times128=400K$</td>
<td>0</td>
</tr>
<tr>
<td>Conv3-256:$56\times56\times256$</td>
<td>$56\times56\times256=800K$</td>
<td>$3\times3\times128\times256=294912$</td>
</tr>
<tr>
<td>Conv3-256:$56\times56\times256$</td>
<td>$56\times56\times256=800K$</td>
<td>$3\times3\times256\times256=589824$</td>
</tr>
<tr>
<td>Conv3-256:$56\times56\times256$</td>
<td>$56\times56\times256=800K$</td>
<td>$3\times3\times256\times256=589824$</td>
</tr>
<tr>
<td>maxpool:$28\times28\times256$</td>
<td>$28\times28\times256=200K$</td>
<td>0</td>
</tr>
<tr>
<td>Conv3-512:$28\times28\times512$</td>
<td>$28\times28\times512=400K$</td>
<td>$3\times3\times256\times512=1179648$</td>
</tr>
<tr>
<td>Conv3-512:$28\times28\times512$</td>
<td>$28\times28\times512=400K$</td>
<td>$3\times3\times512\times512=2359296$</td>
</tr>
<tr>
<td>Conv3-512:$28\times28\times512$</td>
<td>$28\times28\times512=400K$</td>
<td>$3\times3\times512\times512=2359296$</td>
</tr>
<tr>
<td>maxpool:$14\times14\times512$</td>
<td>$14\times14\times512=100K$</td>
<td>0</td>
</tr>
<tr>
<td>Conv3-512:$14\times14\times512$</td>
<td>$14\times14\times512=100K$</td>
<td>$3\times3\times512\times512=2359296$</td>
</tr>
<tr>
<td>Conv3-512:$14\times14\times512$</td>
<td>$14\times14\times512=100K$</td>
<td>$3\times3\times512\times512=2359296$</td>
</tr>
<tr>
<td>Conv3-512:$14\times14\times512$</td>
<td>$14\times14\times512=100K$</td>
<td>$3\times3\times512\times512=2359296$</td>
</tr>
<tr>
<td>maxpool:$7\times7\times512$</td>
<td>$7\times7\times512=25K$</td>
<td>0</td>
</tr>
<tr>
<td>FC-4096 $1\times1\times4096$</td>
<td>4096</td>
<td>$25088\times4096=102760448$</td>
</tr>
<tr>
<td>FC-4096 $1\times1\times4096$</td>
<td>4096</td>
<td>$4096\times4096=102760448$</td>
</tr>
<tr>
<td>FC-1000 $1\times1\times1000$</td>
<td>1000</td>
<td>$4096\times1000=4096000$</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>各个模型的具体参数量可以见Table 2。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndqy7970tj30jy030gm2.jpg" alt=""></p>
<p>除此之外，作者还解释了为什么不用$5\times5$和$7\times7$的卷积，是因为一个$5\times5$卷积的感受野和两个连续的 $3\times3$的卷积是相同的，一个$7\times7$的卷积的感受野等价于3个连续的$3\times3$的卷积，一个$7\times 7$的卷积需要$7^2C^2$的参数，而3个连续的$3\times3$卷积需要$3(3^2C^2)$,所以用$3\times3$卷积的意义在于保证感受野的同时，可以降低参数数量和增加模型深度来提高模型的非线性能力，模型容量(model capacity)和模型复杂度(model complexity)。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fnhm3fh4d0j30jg0swdkp.jpg" alt=""></p>
<p><strong>训练策略：</strong></p>
<p>训练的时候，作者先将图像scale到S（S大于等于224），然后再crop得到$224\times224$的图像。</p>
<p>In our experiments, we evaluated models trained at two fixed scales: S = 256 and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}​$.</p>
<p><strong>实验结果：</strong></p>
<p>作者在ILSVRC-2012 dataset做了模型性能的评估。各个模型评估的结果见Table 3。我们可以发现从左到右随着深度的加深，模型的错误率逐渐降低，VGG 19的效果最好，取得了25.5%的 top-1 val error和8.0%的top-5 val. error。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndqzo17l7j30qy0bstax.jpg" alt=""></p>
<p>作者也分析了各个图片尺度对结果的影响。作者对比了两个策略：单尺度策略和多尺度策略。单尺度策略是在训练集上选择尺寸S，测试集的大小为${S-32,S,S+32}$。而多尺度策略是选择尺度[$S_{min}$;$S_{max}$]，然后测试集的尺度为${S_{min},0.5(S_{min}+S_{max}),S_{max}}$，可以发现后者的效果会比前者更好一点。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndqzqx423j30rs0aomz8.jpg" alt=""></p>
<p>训练的图像大小为S，测试的大小为Q。</p>
<p>dense就是用 fully-convolutional替代fully connected，这样就不需要将测试图像rescale到相同的尺度。multi-crop，顾名思义，就是sample多个crop来进行分类，在评估dense和multi-crop时(见Table 5)，发现这两者是可以互补的。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndrhfe79qj30t607uq4q.jpg" alt=""></p>
<p>最后就是需要将各个模型进行融合，做最后的ensemble。通过将最后输出的softmax其平均，得到最后的概率分布。Table 6就是最终模型fusion的结果。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fndrhr8gxjj30um09u0uz.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/18/VGGNet算法笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/05/Comic-Generation/" >Comic Generation</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-05  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>最近写了一下李宏毅的MLDS 2017的<a href="https://www.csie.ntu.edu.tw/~yvchen/f106-adl/A4" target="_blank" rel="external">HW4-Comics Generation</a>，正好总结一下GAN以及assignment的做法。</p>
<h2 id="Basic-Idea-of-GAN"><a href="#Basic-Idea-of-GAN" class="headerlink" title="Basic Idea of GAN"></a>Basic Idea of GAN</h2><p>给定数据分布：$P_{data}(x)$</p>
<p>我们有一个分布$P_G(x;\theta)$</p>
<p>从$P_{data}(x)$采样m个样本${x_1,x_2,…x_m}$</p>
<p>我们的目的是找到这样的$\theta$使得分布$P_G(x;\theta)$尽可能的和$P_{data}(x)$接近</p>
<p>如果给定参数$\theta$，我们就可以计算产生这一对样本的似然：<br>$$<br>L=\prod_{i=1}^mP_G(x_i;\theta)<br>$$<br>然后找到$\theta^\ast$最大化似然L：<br>$$<br>\theta^\ast = arg \max_\theta\prod_{i=1}^mP_G(x_i;\theta)=arg\max_\theta\log\prod_{i=1}^mP_G(x_i;\theta) =arg\max_\theta\sum_{i=1}^m\log P_G(x_i;\theta) \ \approx arg\max_\theta E_{x\sim P_{data}}[\log P_G(x;\theta)]<br>$$<br>现在，我们可以用NN来模拟$P_G(x;\theta)$<br>$$<br>P_G(x) = \int_z P_{prior}(z) I_{[G(z)=x]}dz<br>$$<br>z服从unit gaussian，但是这样似然明显很难计算！</p>
<ul>
<li>Generator G<ul>
<li>G is a function, input z, output x</li>
<li>Given a prior distribution$ P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G</li>
</ul>
</li>
<li>Discriminator D<ul>
<li>D is a function, input x, output scalar</li>
<li>Evaluate the “difference” between $P_G(x)$ and $P_{data}(x)$</li>
</ul>
</li>
</ul>
<p>目的是找到最佳的G：<br>$$<br>G^\ast = arg\min_G\max_DV(G,D)<br>$$</p>
<p>$$<br>V= E_{x\sim P_{data}}[\log D(x)] + E_{x\sim P_G}[\log (1-D(x))]<br>$$</p>
<p>下面是将上述问题转化为：</p>
<p>首先最优的D：<br>$$<br>P_{data}(x)\log D(x) + P_G(x) \log (1-D(x))<br>$$</p>
<p>$$<br>f(D) = a\log(D) + b\log(1-D)<br>$$</p>
<p>求极值，得到：<br>$$<br>D^\ast(x) =\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}<br>$$<br>所以<br>$$<br>\max_DV(G,D) = V(G,D^*)=E_{x\sim P_{data}}[\log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x\sim P_G}[\log\frac{P_G(x)}{P_{data}(x)+P_G(x)}] \ = -2\log 2+E_{x\sim P_{data}}[\log\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}] + E_{x\sim P_G}[\log\frac{P_G(x)}{(P_{data}(x)+P_G(x))/2}] \ =-2\log 2 + KL(P_{data}(x)||\frac{P_{data}(x)+P_G(x)}{2}) + KL(P_{G}(x)||\frac{P_{data}(x)+P_G(x)}{2})<br>$$<br>将分母项 $P_{data}(x)+P_G(x)$ 除以2，那么整个式子就需要减去 $2\log\frac{1}{2}$ ，这就等价成了JS散度，定义了两个分布的相似性：</p>
<p>$$<br>JSD(P||Q) = \frac{1}{2}KL(P||M)+\frac{1}{2}KL(Q||M), M = \frac{1}{2}(P+Q)<br>$$</p>
<h3 id="一些tricks"><a href="#一些tricks" class="headerlink" title="一些tricks:"></a>一些tricks:</h3><p>有时候在训练的时候会碰到discriminator loss几乎一直是平的（0），这样就会让discriminator的作用变小（telling little information），也就意味着$P_{data}$和$P_{G}$几乎没有overlap，这是因为两者都是low dim manifold in high-dim space。</p>
<ul>
<li>add noise，增加两个分布的接触点或面，而且noise要随机事件decay。</li>
<li>​</li>
</ul>
<h3 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h3><p>z是噪声,y是条件，在初始的GAN加入了额外的条件y，y可以是任何形式的额外信息，包括类的属性等。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fn60wm16qej30oe0ki40z.jpg" alt=""></p>
<h3 id="FGAN"><a href="#FGAN" class="headerlink" title="FGAN"></a>FGAN</h3><p>用f-divergence代替原始的KL divergence</p>
<p>f-divergence, f is convex:<br>$$<br>D_f(P||Q) = \int_xq(x)f(\frac{p(x)}{q(x)})dx<br>$$<br>例如：<br>$$<br>f(x) = x\log x \ f(x) = -\log x \ f(x) = (x-1)^2<br>$$</p>
<h4 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate"></a>Fenchel Conjugate</h4><p>$$<br>f^\ast(t) = \max_{x\in dom(f)}(xt-f(x))<br>$$</p>
<p>$$<br>f(x) = \max_{t\in dom(f^<em>)}{xt-f^\ast(t} \ D_f(P||Q) = \int_x q(x)(\max_{t\in dom(f^</em>)}{\frac{p(x)}{q(x)}t-f^\ast(t)}dx<br>$$</p>
<p>$$<br>D_f(P||Q) \ge \int_x q(x)(x\frac{p(x)}{q(x)}D(x)-f^\ast(D(x)))dx \ = \int_xp(x)D(x)dx-\int_x q(x)f^\ast(D(x))dx \ =\max_DE_{x_\sim P}(D(x))-E_{x\sim Q}f^\ast(D(x))<br>$$</p>
<p>D is a function whose input is x and output is t</p>
<p>这就相当于定义了一个新的V(G,D)</p>
<h3 id="LSGAN（Least-Squares-GANs）"><a href="#LSGAN（Least-Squares-GANs）" class="headerlink" title="LSGAN（Least Squares GANs）"></a>LSGAN（Least Squares GANs）</h3><p>使用最小二乘损失函数代替了GAN的损失函数,事实上，作者认为使用JS散度并不能拉近真实分布和生成分布之间的距离，使用最小二乘可以将图像的分布尽可能的接近决策边界<br>$$<br>\min_DV_{LSGAN}(D) = \frac{1}{2}E_{x\sim p_{data}(x)}[(D(x)-b)^2]+\frac{1}{2}E_{x\sim p_{z}(z)}[(D(G(z))-a)^2]<br>$$</p>
<p>$$<br>\min_GV_{LSGAN}(G)= \frac{1}{2}E_{x\sim p_{data}(x)}[(D(G(z))-c)^2]<br>$$</p>
<h3 id="infoGAN"><a href="#infoGAN" class="headerlink" title="infoGAN"></a>infoGAN</h3><p>$$<br>\min_{G,Q}\max_DV_{infoGAN}(D,G,Q) = V(D,G) - \lambda L_I(G,Q)<br>$$</p>
<p>其中，$L_1(G,Q)=E_{c\sim P(c),x\sim G(z,c)}[\log Q(c|x)]+H(c)$</p>
<p>也就是：<br>$$<br>L_{D,Q}=L_D^{GAN} - \lambda L_1(c,c’) \ L_{G} = L_G^{GAN} - \lambda L_1(c,c’)<br>$$</p>
<p>###WGAN</p>
<p>$$<br>L_D^{WGAN} = E[D(x)]-E[D(G(z))]<br>$$</p>
<p>$$<br>L_G^{WGAN} = E[D(G(Z))]<br>$$</p>
<p>$$<br>W_D\leftarrow clip_by_value(W_D, -0.01, 0.01)<br>$$</p>

	
	</div>
  <a type="button" href="/2018/01/05/Comic-Generation/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/12/18/CS224n-assignment1/" >CS224n assignment1</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-12-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>(a) 证明softmax(x) = softmax(x+c), 这样就可以把c设为$\max(x)$来保证数值计算的稳定性</p>
<p>$$<br>softmax(x)_i = \frac{e^{x_i}}{\sum_je^{x_j}}<br>$$</p>
<p>$$<br>(softmax(x+c))_i = \frac{\exp(x_i+c)}{\sum_{j=1}\exp(x_j+c)}=\ \frac{\exp(x_i)\exp(c)}{\exp(c)\sum_{j=1}\exp(x_j)} = \frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}<br>$$</p>
<p>(b) 实现q1_softmax.py: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""Compute the softmax function for each row of the input x.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    x -- A N dimensional vector or M x N dimensional numpy matrix.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    x -- You are allowed to modify x in-place</span></div><div class="line"><span class="string">    """</span></div><div class="line">    orig_shape = x.shape</div><div class="line"></div><div class="line">    <span class="keyword">if</span> len(x.shape) &gt; <span class="number">1</span>:</div><div class="line">        <span class="comment"># Matrix</span></div><div class="line">        x -= np.max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">        x = np.exp(x) / np.sum(np.exp(x), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># Vector</span></div><div class="line">        x -= np.max(x)</div><div class="line">        x = np.exp(x) / np.sum(np.exp(x))</div><div class="line"></div><div class="line">    <span class="keyword">assert</span> x.shape == orig_shape</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<h2 id="Neural-Network-Basics"><a href="#Neural-Network-Basics" class="headerlink" title="Neural Network Basics"></a>Neural Network Basics</h2><p>(a) 推导一下sigmoid函数的导数：<br>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$</p>
<p>$$<br>\sigma^\prime(x) = \sigma(x)(1 − \sigma(x))<br>$$</p>
<p>(b) 推导一下softmax函数的导数：<br>$$<br>CE(y,\hat{y}) = -\sum_i y_i \log(\hat{y}_i), \hat{y} = softmax(\theta)<br>$$<br>k是目标类<br><span>$$\frac{\partial CE(y,\hat{y})}{\partial \theta_i} =  \left\{
\begin{align} 
&amp;\hat{y_i} - 1,i=k \\ 
&amp;\hat{y_i}, otherwise
\end{align}
\right.$$</span><!-- Has MathJax --><br>等价于：</p>
<p>$$<br>\frac{\partial CE(y,\hat{y})}{\partial \theta} = \hat{y} -y<br>$$</p>
<p>(c) x是一层神经网络的输入，推导x的梯度也就是$\frac{\partial J}{\partial x}$, $J = CE(y, \hat{y})$，神经网络的隐藏层激活函数是$sigmoid$，而最后一层的是$softmax$</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-18%20%E4%B8%8B%E5%8D%8812.15.18.png" alt=""><br>$$<br>z1 = xW_1 + b_1, h = sigmoid(z_1), z_2=hW_2 + b_2, \hat{y} = softmax(z_2),<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial x} = \frac{\partial J}{\partial z_2} \frac{\partial z_2}{\partial h}\frac{\partial h}{\partial z_1}\frac{\partial z_1}{\partial x}<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial z_2} = \hat{y} -y<br>$$</p>
<p>$$<br>\frac{\partial z_2}{\partial h} = W_2<br>$$</p>
<p>$$<br>\frac{\partial h}{\partial z_1} = sigmoid(z_1) (1-sigmoid(z_1))<br>$$</p>
<p>$$<br>\frac{\partial z_1}{\partial x} = W_1<br>$$</p>
<p>(d) 上个网络的参数个数, 输入的维度是$D_x$,输出的维度是$D_y$, 隐藏层是H：<br>$$<br>D_x \cdot H + H + H \cdot D_y + D_y<br>$$<br>(e) 实现q2 sigmoid.py:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the sigmoid function for the input here.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    x -- A scalar or numpy array.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    s -- sigmoid(x)</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    s = <span class="number">1</span> / (<span class="number">1</span>+np.exp(-x))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> s</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the gradient for the sigmoid function here. Note that</span></div><div class="line"><span class="string">    for this implementation, the input s should be the sigmoid</span></div><div class="line"><span class="string">    function value of your original input x.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    s -- A scalar or numpy array.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    ds -- Your computed gradient.</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    ds = s * (<span class="number">1</span>-s)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> ds</div></pre></td></tr></table></figure>
<p>(f) 实现梯度检查: q2 gradcheck.py<br>$$<br>\frac{\partial J(\theta)}{\partial \theta} = \lim_{\epsilon\rightarrow0}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradcheck_naive</span><span class="params">(f, x)</span>:</span></div><div class="line">    <span class="string">""" Gradient check for a function f.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    f -- a function that takes a single argument and outputs the</span></div><div class="line"><span class="string">         cost and its gradients</span></div><div class="line"><span class="string">    x -- the point (numpy array) to check the gradient at</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    rndstate = random.getstate()</div><div class="line">    random.setstate(rndstate)</div><div class="line">    fx, grad = f(x) <span class="comment"># Evaluate function value at original point</span></div><div class="line">    h = <span class="number">1e-4</span>        <span class="comment"># Do not change this!</span></div><div class="line"></div><div class="line">    <span class="comment"># Iterate over all indexes in x</span></div><div class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</div><div class="line">        ix = it.multi_index</div><div class="line"></div><div class="line">        <span class="comment"># Try modifying x[ix] with h defined above to compute</span></div><div class="line">        <span class="comment"># numerical gradients. Make sure you call random.setstate(rndstate)</span></div><div class="line">        <span class="comment"># before calling f(x) each time. This will make it possible</span></div><div class="line">        <span class="comment"># to test cost functions with built in randomness later.</span></div><div class="line"></div><div class="line">        old_xix = x[ix]</div><div class="line">        x[ix] = old_xix + h</div><div class="line">        random.setstate(rndstate)</div><div class="line">        fp = f(x)[<span class="number">0</span>]</div><div class="line">        x[ix] = old_xix - h</div><div class="line">        random.setstate(rndstate)</div><div class="line">        fm = f(x)[<span class="number">0</span>]</div><div class="line">        x[ix] = old_xix</div><div class="line">        <span class="comment">#random.setstate(rndstate)</span></div><div class="line">        numgrad = (fp-fm) / (<span class="number">2</span>*h)</div><div class="line">        <span class="comment"># Compare gradients</span></div><div class="line">        reldiff = abs(numgrad - grad[ix]) / max(<span class="number">1</span>, abs(numgrad), abs(grad[ix]))</div><div class="line">        <span class="keyword">if</span> reldiff &gt; <span class="number">1e-5</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Gradient check failed."</span></div><div class="line">            <span class="keyword">print</span> <span class="string">"First gradient error found at index %s"</span> % str(ix)</div><div class="line">            <span class="keyword">print</span> <span class="string">"Your gradient: %f \t Numerical gradient: %f"</span> % (</div><div class="line">                grad[ix], numgrad)</div><div class="line">            <span class="keyword">return</span></div><div class="line"></div><div class="line">        it.iternext() <span class="comment"># Step to next dimension</span></div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Gradient check passed!"</span></div></pre></td></tr></table></figure>
<p>(g) 实现: q2 neural.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Forward and backward propagation for a two-layer sigmoidal network</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Compute the forward propagation and for the cross entropy cost,</span></div><div class="line"><span class="string">    and backward propagation for the gradients for all parameters.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    data -- M x Dx matrix, where each row is a training example.</span></div><div class="line"><span class="string">    labels -- M x Dy matrix, where each row is a one-hot vector.</span></div><div class="line"><span class="string">    params -- Model parameters, these are unpacked for you.</span></div><div class="line"><span class="string">    dimensions -- A tuple of input dimension, number of hidden units</span></div><div class="line"><span class="string">                  and output dimension</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    <span class="comment">### Unpack network parameters (do not modify)</span></div><div class="line">    ofs = <span class="number">0</span></div><div class="line">    Dx, H, Dy = (dimensions[<span class="number">0</span>], dimensions[<span class="number">1</span>], dimensions[<span class="number">2</span>])</div><div class="line"></div><div class="line">    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))</div><div class="line">    ofs += Dx * H</div><div class="line">    b1 = np.reshape(params[ofs:ofs + H], (<span class="number">1</span>, H))</div><div class="line">    ofs += H</div><div class="line">    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))</div><div class="line">    ofs += H * Dy</div><div class="line">    b2 = np.reshape(params[ofs:ofs + Dy], (<span class="number">1</span>, Dy))</div><div class="line"></div><div class="line">    </div><div class="line">    z1 = np.dot(data, W1) + b1</div><div class="line">    h1 = sigmoid(z1)</div><div class="line">    z2 = np.dot(h1, W2) + b2</div><div class="line">    y = softmax(z2)</div><div class="line"></div><div class="line">    cost = -np.sum(labels * np.log(y))</div><div class="line"></div><div class="line">    gradz2 = y - labels</div><div class="line"></div><div class="line"></div><div class="line">    gradW2 = np.dot(h1.T, gradz2)</div><div class="line">    gradb2 = np.sum(gradz2, axis=<span class="number">0</span>).reshape((<span class="number">1</span>, Dy))</div><div class="line"></div><div class="line">    gradh1 = np.dot(gradz2, W2.T)</div><div class="line">    gradz1 = gradh1 * sigmoid_grad(h1)</div><div class="line"></div><div class="line">    gradW1 = np.dot(data.T, gradz1)</div><div class="line">    gradb1 = np.sum(gradz1, axis=<span class="number">0</span>).reshape((<span class="number">1</span>, H))</div><div class="line"></div><div class="line">    <span class="keyword">assert</span> gradW1.shape == W1.shape</div><div class="line">    <span class="keyword">assert</span> gradW2.shape == W2.shape</div><div class="line">    <span class="comment">### Stack gradients (do not modify)</span></div><div class="line">    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),</div><div class="line">        gradW2.flatten(), gradb2.flatten()))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, grad</div></pre></td></tr></table></figure>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>主要包括word embeeding中的两个模型： Skip-gram和CBOW</p>
<ol>
<li>skipgram:Predict context words given target (position independent)</li>
</ol>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-18%20%E4%B8%8B%E5%8D%883.47.29.png" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fml280dpabj313q0to1kx.jpg" alt=""></p>
<ol>
<li>Predict target word from bag-of-words context</li>
</ol>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fml67e6yo4j30fa0cpdht.jpg" alt=""></p>
<p>(a) 求skipgram的关于$v_c$和$\mu_w$的梯度： </p>
<p>$$<br>\hat{y}_o= p(o|c)=\frac{\exp(\mu_o^T v_c)}{\sum_{w=1}^W\exp(\mu_w^T v_c)}<br>$$</p>
<p>o表示输出词的下标，c表示的是中心词的下标，$u_o$表示输出向量</p>
<p>预测的词向量$v_c$代表第c个中心词，$w$表示的是第w个词, i表示目标。<br>$$<br>J_{softmax-CE}(o,v_c, U) = CE(y, \hat{y}), U= [u_1,u_2,…,u_W]<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial v_c} = -u_i + \sum_{w=1}^Wu_w\hat{y}_w = U(\hat{y}-y)<br>$$</p>
<span>$$\frac{\partial J}{\partial u_w} =  \left\{
\begin{align} 
&amp;(\hat{y_w} - 1)v_c,w=o \\ 
&amp;\hat{y_w}v_c, otherwise
\end{align}
\right.$$</span><!-- Has MathJax -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmaxCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset)</span>:</span></div><div class="line">    <span class="string">""" Softmax cost function for word2vec models</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    predicted -- numpy ndarray, predicted word vector (\hat&#123;v&#125; in</span></div><div class="line"><span class="string">                 the written component)</span></div><div class="line"><span class="string">    target -- integer, the index of the target word</span></div><div class="line"><span class="string">    outputVectors -- "output" vectors (as rows) for all tokens</span></div><div class="line"><span class="string">    dataset -- needed for negative sampling, unused here.   </span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    cost -- cross entropy cost for the softmax word prediction</span></div><div class="line"><span class="string">    gradPred -- the gradient with respect to the predicted word</span></div><div class="line"><span class="string">           vector</span></div><div class="line"><span class="string">    grad -- the gradient with respect to all the other word</span></div><div class="line"><span class="string">           vectors</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    """</span></div><div class="line">    out = np.dot(outputVectors, predicted)</div><div class="line">    score = out[target]</div><div class="line">    exp_sum = np.sum(np.exp(out))</div><div class="line">    cost = np.log(exp_sum) - score</div><div class="line">    margin = np.exp(out) / np.sum(np.exp(out))</div><div class="line">    margin[target] -= <span class="number">1</span> </div><div class="line">    gradPred = np.dot(margin.T, outputVectors)</div><div class="line">    grad = np.dot(margin, predicted.T)</div><div class="line">    <span class="keyword">return</span> cost, gradPred, grad</div></pre></td></tr></table></figure>
<p>(b) negative sampling:  更新全词表的代价有点大，从而负采样K个，更新。$v_c$是预测的词向量，$o$是期望输出词<br>$$<br>J_{neg-sample}(o,v_c,U) = -\log(\sigma(u_o^Tv_c)) - \sum_{k=1}^K \log(\sigma(-u_k^Tv_c))<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial v_c} =(\sigma(u_o^Tv_c)-1)u_o-\sum_{k=1}^K(\sigma(-u_k^Tv_c)-1)u_k<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial u_o} =(\sigma(u_o^Tv_c)-1)v_c<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial u_k} =-(\sigma(-u_k^Tv_c)-1)v_c, k = 1,2,…,K<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNegativeSamples</span><span class="params">(target, dataset, K)</span>:</span></div><div class="line">    <span class="string">""" Samples K indexes which are not the target """</span></div><div class="line"></div><div class="line">    indices = [<span class="keyword">None</span>] * K</div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> xrange(K):</div><div class="line">        newidx = dataset.sampleTokenIdx()</div><div class="line">        <span class="keyword">while</span> newidx == target:</div><div class="line">            newidx = dataset.sampleTokenIdx()</div><div class="line">        indices[k] = newidx</div><div class="line">    <span class="keyword">return</span> indices</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset,</span></span></div><div class="line"><span class="function"><span class="params">                               K=<span class="number">10</span>)</span>:</span></div><div class="line">    <span class="string">""" Negative sampling cost function for word2vec models</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Implement the cost and gradients for one predicted word vector</span></div><div class="line"><span class="string">    and one target word vector as a building block for word2vec</span></div><div class="line"><span class="string">    models, using the negative sampling technique. K is the sample</span></div><div class="line"><span class="string">    size.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Note: See test_word2vec below for dataset's initialization.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments/Return Specifications: same as softmaxCostAndGradient</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    <span class="comment"># Sampling of indices is done for you. Do not modify this if you</span></div><div class="line">    <span class="comment"># wish to match the autograder and receive points!</span></div><div class="line">    indices = [target]</div><div class="line">    indices.extend(getNegativeSamples(target, dataset, K))</div><div class="line"></div><div class="line">    labels = -np.ones((K+<span class="number">1</span>,))</div><div class="line">    labels[<span class="number">0</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">    out = np.dot(outputVectors[indices], predicted) * labels</div><div class="line">    </div><div class="line">    scores = sigmoid(out)</div><div class="line">    cost = -np.sum(np.log(scores))</div><div class="line"></div><div class="line">    d = labels * (scores<span class="number">-1</span>)</div><div class="line">    gradPred = np.dot(d.reshape((<span class="number">1</span>, <span class="number">-1</span>)), outputVectors[indices]).flatten()</div><div class="line">    gradtemp = np.dot(d.reshape((<span class="number">-1</span>, <span class="number">1</span>)), predicted.reshape((<span class="number">1</span>,<span class="number">-1</span>)))</div><div class="line">    grad = np.zeros_like(outputVectors)</div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(K+<span class="number">1</span>):</div><div class="line">        grad[indices[k]] += gradtemp[k,:]</div><div class="line">    <span class="keyword">return</span> cost, gradPred, grad</div></pre></td></tr></table></figure>
<p>(c) 推导skip gram和CBOW的梯度：</p>
<p>给定一系列的上下文单词$[word_{c-m},…,word_{c-1},word_c,word_{c+1},…,word_{c+m}]$</p>
<p>输入词向量为$v_k$,输出词向量为$u_k$, $\hat{v}=v_c$</p>
<p>这里， skip gram的cost函数为：<br>$$<br>J_{skip_gram}(word_{c-m…c+m}) = \sum_{-m\le j\le m, j\ne0} F(w_{c+j}, v_c)<br>$$</p>
<p>$$<br>\frac{\partial J_{skip_gram}(word_{c-m…c+m})}{\partial U} =\sum_{-m\le j\le m, j\ne0} \frac{\partial F(w_{c+j}, v_c)}{\partial U}<br>$$</p>
<p>$$<br>\frac{\partial J_{skip_gram}(word_{c-m…c+m})}{\partial v_c} =\sum_{-m\le j\le m, j\ne0} \frac{\partial F(w_{c+j}, v_c)}{\partial v_c}<br>$$</p>
<p>$$<br>\frac{\partial J_{skip_gram}(word_{c-m…c+m})}{\partial v_j} =0, j \ne c<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">skipgram</span><span class="params">(currentWord, C, contextWords, tokens, inputVectors, outputVectors,</span></span></div><div class="line"><span class="function"><span class="params">             dataset, word2vecCostAndGradient=softmaxCostAndGradient)</span>:</span></div><div class="line">    <span class="string">""" Skip-gram model in word2vec</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    currrentWord -- a string of the current center word</span></div><div class="line"><span class="string">    C -- integer, context size</span></div><div class="line"><span class="string">    contextWords -- list of no more than 2*C strings, the context words</span></div><div class="line"><span class="string">    tokens -- a dictionary that maps words to their indices in</span></div><div class="line"><span class="string">              the word vector list</span></div><div class="line"><span class="string">    inputVectors -- "input" word vectors (as rows) for all tokens</span></div><div class="line"><span class="string">    outputVectors -- "output" word vectors (as rows) for all tokens</span></div><div class="line"><span class="string">    word2vecCostAndGradient -- the cost and gradient function for</span></div><div class="line"><span class="string">                               a prediction vector given the target</span></div><div class="line"><span class="string">                               word vectors, could be one of the two</span></div><div class="line"><span class="string">                               cost functions you implemented above.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    cost -- the cost function value for the skip-gram model</span></div><div class="line"><span class="string">    grad -- the gradient with respect to the word vectors</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    cost = <span class="number">0.0</span></div><div class="line">    gradIn = np.zeros(inputVectors.shape)</div><div class="line">    gradOut = np.zeros(outputVectors.shape)</div><div class="line"></div><div class="line">    </div><div class="line">    center = tokens[currentWord]</div><div class="line">    predicted = inputVectors[center]</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> target_word <span class="keyword">in</span> contextWords:</div><div class="line">        target = tokens[target_word]</div><div class="line">        cost_i, gradPred, grad = word2vecCostAndGradient(predicted, target, outputVectors, dataset)</div><div class="line">        cost += cost_i</div><div class="line">        gradIn[center] += gradPred</div><div class="line">        gradOut += grad</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, gradIn, gradOut</div></pre></td></tr></table></figure>
<p>而CBOW有点不同，首先：<br>$$<br>\hat{v} = \sum_{-m\le j\le m, j\ne0} v_{c+j}<br>$$<br>它的cost函数为：<br>$$<br>J_{CBOW}(word_{c-m…c+m})=F(w_c, \hat{v})<br>$$</p>
<p>$$<br>\frac{\partial J_{CBOW}(word_{c-m…c+m})}{\partial U} = \frac{\partial F(w_c, v_c)}{\partial U}<br>$$</p>
<p>$$<br>\frac{\partial J_{CBOW}(word_{c-m…c+m})}{\partial v_j} = \frac{\partial F(w_c, v_c)}{\partial \hat{v}}, j\in{c-m,…,c-1,c+1,…,c+m}<br>$$</p>
<p>$$<br>\frac{\partial J_{CBOW}(word_{c-m…c+m})}{\partial v_j} =0, j\notin{c-m,…,c-1,c+1,…,c+m}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cbow</span><span class="params">(currentWord, C, contextWords, tokens, inputVectors, outputVectors,</span></span></div><div class="line"><span class="function"><span class="params">         dataset, word2vecCostAndGradient=softmaxCostAndGradient)</span>:</span></div><div class="line">    <span class="string">"""CBOW model in word2vec</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Implement the continuous bag-of-words model in this function.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments/Return specifications: same as the skip-gram model</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    cost = <span class="number">0.0</span></div><div class="line">    gradIn = np.zeros(inputVectors.shape)</div><div class="line">    gradOut = np.zeros(outputVectors.shape)</div><div class="line"></div><div class="line">    </div><div class="line">    target = tokens[currentWord]</div><div class="line">    target_vec = inputVectors[target]</div><div class="line">    source_idx = map(<span class="keyword">lambda</span> x: tokens[x], contextWords)</div><div class="line">    predicted = np.sum(inputVectors[source_idx], axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">    cost, gradPred, gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> source_idx:</div><div class="line">        gradIn[idx] += gradPred</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, gradIn, gradOut</div></pre></td></tr></table></figure>

	
	</div>
  <a type="button" href="/2017/12/18/CS224n-assignment1/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/12/09/Network-Architecture-of-Deblurring/" >Network Architecture of Deblurring</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-12-09  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>Wieschollek P, Hirsch M, Schölkopf B, et al. Learning Blind Motion Deblurring. arXiv preprint arXiv:1708.04208, 2017. <a href="https://github.com/cgtuebingen/learning-blind-motion-deblurring" target="_blank" rel="external">Codes</a>, <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wieschollek_Learning_Blind_Motion_ICCV_2017_paper.pdf" target="_blank" rel="external">Paper</a></p>
<p>这篇文章主要是针对视频的去噪，利用前几帧的信息来帮助预测当前帧，用到一些常用的skip-connection的结构来结合low-level and high resolution的feature map。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.07.49.png" alt=""></p>
<p>Wang L, Li Y, Wang S. DeepDeblur: Fast one-step blurry face images restoration. arXiv preprint arXiv:1711.09515, 2017.</p>
<p>这篇文章主要针对的是人脸的运动噪声去模糊，其中kernel是人工模拟的，利用高斯过程生成，网络结构的话就是利用多个inception module和resnet的结构。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.15.21.png" alt=""></p>
<p>Kupyn O, Budzan V, Mykhailych M, et al. DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks. arXiv preprint arXiv:1711.07064, 2017. </p>
<p>这篇文章的结构比较接单，就是利用多个ResBlocks来作为generater，然后在discriminator loss中加入critic loss（用Wasserstein GAN）和perceptual loss(features dissimilarity)。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.18.01.png" alt=""></p>
<p>Noroozi M, Chandramouli P, Favaro P. Motion Deblurring in the Wild. arXiv preprint arXiv:1701.01486, 2017. </p>
<p>主要利用了mutli-scale和skip-connection</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.57.08.png" alt=""></p>
<p>Nah S, Kim T H, Lee K M. Deep multi-scale convolutional neural network for dynamic scene deblurring. arXiv preprint arXiv:1612.02177, 2016. </p>
<p>这篇文章主要用到了一些残差学习的方法，不仅用了ResBlock，还将小尺度的结果作为残差传给大尺度，简化学习的难度。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%886.00.27.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/12/09/Network-Architecture-of-Deblurring/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/12/05/Generate-Motion-Blur/" >Generate Motion Blur</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-12-05  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>本文主要介绍几种常用的人工合成运动噪声的方法：</p>
<p>###基于spline平滑的方法</p>
<p>在一个$n\times n$大小的矩阵内，随机采样6个点，再用三阶的spline平滑拟合，这样采样得到若干个在矩阵内的整数点，这些整数点上的值，再用高斯采样得到，然后就是归一化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_kernel_spline</span><span class="params">(steps, n_samples)</span>:</span></div><div class="line"></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(n_samples):</div><div class="line">		psz = <span class="number">24</span>  <span class="comment">##矩阵大小</span></div><div class="line">		kern = np.zeros((psz, psz))</div><div class="line">		x = np.random.randint(<span class="number">1</span>, psz+<span class="number">1</span>, (steps,))</div><div class="line">		y = np.random.randint(<span class="number">1</span>, psz+<span class="number">1</span>, (steps,))</div><div class="line">		</div><div class="line">		x = interpolate.spline(xk=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps), yk=x, xnew=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps*<span class="number">5000</span>))</div><div class="line">		y = interpolate.spline(xk=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps), yk=y, xnew=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps*<span class="number">5000</span>))</div><div class="line"></div><div class="line"></div><div class="line">		x = np.round(np.maximum(<span class="number">1</span>, np.minimum(psz, x)))</div><div class="line"></div><div class="line">		y = np.round(np.maximum(<span class="number">1</span>, np.minimum(psz, y)))</div><div class="line"></div><div class="line">		idxs = (x<span class="number">-1</span>) * psz + y</div><div class="line"></div><div class="line">		idxs = np.unique(idxs).astype(int)</div><div class="line"></div><div class="line">		wt = np.maximum(<span class="number">0</span>, np.random.randn(idxs.shape[<span class="number">0</span>],) * <span class="number">0.5</span> + <span class="number">1</span>)</div><div class="line">		<span class="keyword">if</span> np.sum(wt) == <span class="number">0</span>:</div><div class="line">			<span class="keyword">continue</span></div><div class="line">		wt /= np.sum(wt)</div><div class="line">		<span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(idxs):</div><div class="line">			x = idx % psz</div><div class="line">			y = idx / psz</div><div class="line">			kern[x, y] = wt[i]</div></pre></td></tr></table></figure>
<h3 id="基于高斯过程的方法"><a href="#基于高斯过程的方法" class="headerlink" title="基于高斯过程的方法"></a>基于高斯过程的方法</h3><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Probability_theory" target="_blank" rel="external">probability theory</a> and <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank" rel="external">statistics</a>, a <strong>Gaussian process</strong> is a particular kind of statistical model where <a href="https://en.wikipedia.org/wiki/Random_variate" target="_blank" rel="external">observations</a> occur in a continuous domain, e.g. time or space. In a Gaussian process, every point in some continuous input space is associated with a <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="external">normally distributed</a> <a href="https://en.wikipedia.org/wiki/Random_variable" target="_blank" rel="external">random variable</a>. Moreover, every finite collection of those random variables has a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" target="_blank" rel="external">multivariate normal distribution</a>, i.e. every finite <a href="https://en.wikipedia.org/wiki/Linear_combination" target="_blank" rel="external">linear combination</a> of them is normally distributed. The distribution of a Gaussian process is the <a href="https://en.wikipedia.org/wiki/Joint_distribution" target="_blank" rel="external">joint distribution</a> of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.</p>
</blockquote>
<p>高斯过程其实就是多元高斯分布的无限维度扩展，我们通过观察无限维度的数据的子集（这些子集也服从多元高斯分布），然后构造函数来对数据进行建模。</p>
<p>例如，我们需要测量一年中每天中午的温度（温度明显是一个有连续空间的变量），这里GP就是一个函数f, 输入${x_n}_{n=1}^{365}$, $f(x_n)$就是每天温度的预测值。 GP函数主要包含两部分： mean function, $m(x)$和kernel function, $k(x, x^\prime)$。</p>
<p>我们需要对x坐标和y坐标进行采样：<br>$$<br>f_x(t), f_y(t) \sim GP(0, k(t, t’)), k(t,t’) = \sigma_f^2(1+\frac{\sqrt(5)|t-t’|}{l}+\frac{5(t-t’)^2}{3l^2})\exp(-\frac{\sqrt 5|t-t’|}{l})<br>$$<br>这里，$l=0.3$, $\sigma_f=0.25$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(x1, x2)</span>:</span></div><div class="line">	sigma_f = <span class="number">1.</span>/<span class="number">4</span></div><div class="line">	l = <span class="number">0.3</span></div><div class="line">	delta = np.abs(x1-x2)</div><div class="line">	<span class="keyword">return</span> sigma_f * sigma_f * (<span class="number">1</span>+np.sqrt(<span class="number">5</span>)*delta/l + <span class="number">5</span> * delta*delta/(<span class="number">3</span>*l*l)) * np.exp(-np.sqrt(<span class="number">5</span>)*delta/l)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(xs)</span>:</span></div><div class="line">	<span class="keyword">return</span> [[kernel(x1, x2) <span class="keyword">for</span> x2 <span class="keyword">in</span> xs] <span class="keyword">for</span> x1 <span class="keyword">in</span> xs]</div></pre></td></tr></table></figure>

	
	</div>
  <a type="button" href="/2017/12/05/Generate-Motion-Blur/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/24/capsule/" >Introduction to Capsule Network</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-24  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p><a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">Dynamic Routing Between Capsules</a></p>
<p>这是Hinton发表在NIPS2017的一篇文章，提出了capsule的概念。</p>
<p>其实可以把capsule看成是neuron的一个特殊形式，neuron的输出是一个scalar，而capsule则会输出vector。除此之外，neuron可以detect到一个特定的pattern，但是这又存在很大的局限性，会有pattern冗余，例如：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.11.12.png" alt=""></p>
<p>所以capsule输出vector就可以避免这样的情况，输出特征v的每个维度表示的是对应pattern的特性，以上图为例，可能某个表示鸟嘴方向的维度，分别对应1和-1。</p>
<p>再以人脸为例，传统的CNN可能可以detect到眼睛的pattern, 嘴巴的pattern等，只能表示它的存在，但是无法表示五官的属性，例如相对位置，大小，相对角度等等。而向量的大小表示的是整个pattern的概率，或者可以叫做confidence, 例如下图：</p>
<p><img src="https://jhui.github.io/assets/capsule/face4.jpg" alt=""><img src="https://jhui.github.io/assets/capsule/face5.jpg" alt=""></p>
<p>具体的计算过程见下图：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.15.27.png" alt=""></p>
<p>$$<br>u^1=W^1V^1, u^2=W^2v^2 \ s=c_1u^1 \ v=Squash(s) , v = \frac{|s|}{1+|s|^2}\frac{s}{|s|}<br>$$<br>接下来就是核心，dynamic routing:</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.51.30.png" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.24.52.png" alt=""></p>
<p>和传统CNN简单粗暴的max pooling不同的是它动态地调整routing的系数，系数是在testing的时候online地决定的，调整的方法就是通过T次迭代，根据aggrement，其实就是提高越相关的v的系数。 上图中，如果$a^r$和$u^i$相关性较强的话，就可以得到更大的$b_i$。</p>
<p>也可以将dynamic routing的过程看成是一个不断排除outlier的一个过程，例如现在$u^1$,$u^2$很接近，而$u^3$与他们差距很大，他们两个队最终的$a^r$贡献很大，那么随着不断迭代，$u^3$就被消除了。</p>

	
	</div>
  <a type="button" href="/2017/11/24/capsule/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/21/FIXING-WEIGHT-DECAY-REGULARIZATION-IN-ADAM/" >Fixing  Weight Decay Regularization in Adam</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-21  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p><a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="external">文章的链接</a></p>
<p>首先，文章理清了l2正则和weight decay的区别，它们并不是对等的。 weight decay可以表示成:</p>
<p>$$<br>x_{t+1} = (1-w_t)x_{t}-\alpha_t\nabla f_t(x_t)<br>$$</p>
<p>而l2正则的表示是：</p>
<p>$$<br>f_{t,reg(x_t)} = f_t(x_t)+ \frac{w_t}{2} |x_t|_2^2<br>$$</p>
<p>所以：</p>
<p>$$<br>\nabla f_{t,reg(x_t)} = \nabla f_t(x_t)+w_tx_t<br>$$</p>
<p>注意到weight decay的系数只有$w_t$，那么，在大部分框架中，例如tensorflow, keras, pytorch等把weight decay和l2正则等价了，</p>
<p>我们切换到SGD Mometum中来：因求完梯度以后，需要累加mometum，在$x_t$前面就存在了三个参数：$\alpha$学习率,$w_t$,$\eta_t$平滑系数。那么就和weight decay不对等了，当然可以把这三者乘积看成一个系数，但是这样还是削弱了原本的weight decay（系数变小了）。</p>
<p>因此，作者把传统的SGD with momentum做了以下修改：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.17.09.png" alt=""></p>
<p>简单总结一下： 就是除了在梯度计算中加入weight decay，在mometum也加入了weight decay。这样就增强了weight decay的作用。</p>
<p>除了对SGD with Mometum有影响，作者还对Adam进行了修改：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.26.39.png" alt=""></p>
<p>看一下Adam的公式：</p>
<p>$$x_t = x_{t-1} - \eta_t\alpha \frac{\beta_1m_{t-1}+(1-\beta_1)g_t}{\sqrt{\beta_2v_{t-1}+(1-\beta_2)g_t^2+\epsilon}}$$</p>
<p>with $g_t=\nabla f_t(x_{t-1})+w_tx_{t-1}$</p>
<p>这里可以看到$g_t$被归一化了，同时$w_t$也带着被归一化了，这样$w_t$就被减弱了。</p>
<p>实验结果：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.38.45.png" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.38.53.png" alt=""></p>
<p>横纵坐标分别是不同的weight decay和learning rate的组合, 可以看到learning rate和weight decay的相关性很大，固定weight decay，去调整learning rate，那么效果会变化较大，从图中看到，明显作者提出的算法，最有区域较大，更利于找出最优的参数组合。</p>

	
	</div>
  <a type="button" href="/2017/11/21/FIXING-WEIGHT-DECAY-REGULARIZATION-IN-ADAM/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/14/Networks/" >Dual Path Networks</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-14  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>This paper propose a novel deep CNN architecture called <strong>Dual Path Networks(DPN)</strong>. This idea is  based on the fact that the ResNet enables feature re-usage while DenseNet enable new features exploration which are both important for learning good feature representation.  </p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/image.png" alt=""></p>
<blockquote>
<p>Basically, the ResNet and DenseNet differ in the way of “wiring”. ResNet provides a path with which a layer can get access to both the output and the input of the immediately previous layer. The DenseNet provides a path that can access the outputs of multiple previous layers. </p>
</blockquote>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-14%20%E4%B8%8B%E5%8D%886.25.45.png" alt="Architecture comparison of different networks"></p>
<p>The DPN balance ResNet and DenseNet in a tricky way, and can be formulated as:<br>$$<br>x^k = \sum_{t=1}^{k-1} f_t^k (h^t),\ y^k = \sum_{t=1}^{k-1} v_t(h^t) = y^{k-1} +\phi^{k-1}(y^{k-1}),\\r^k=x^k+y^k,\\h^k=g^k(r^k)<br>$$<br>where $x_k$ and $y_k$ denote the extracted information at k-th step from individual path, $v_t(\cdot)$is a feature learning function as $f_k^t(\cdot)$, $\phi_k(\cdot) = f_k(g_k(\cdot))$.  The dual path means the left side is ResNet, the right side is DenseNet. The block parameters are shared between them. The outputs of two sides will be concated as next block’s input.</p>
<p>This is the implementation of dual path block</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DualPathBlock</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, G, _type=<span class="string">'normal'</span>)</span>:</span></div><div class="line">        super(DualPathBlock, self).__init__()</div><div class="line">        self.num_1x1_c = num_1x1_c</div><div class="line"></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'proj'</span>:</div><div class="line">            key_stride = <span class="number">1</span></div><div class="line">            self.has_proj = <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'down'</span>:</div><div class="line">            key_stride = <span class="number">2</span></div><div class="line">            self.has_proj = <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'normal'</span>:</div><div class="line">            key_stride = <span class="number">1</span></div><div class="line">            self.has_proj = <span class="keyword">False</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.has_proj:</div><div class="line">            self.c1x1_w = self.BN_ReLU_Conv(in_chs=in_chs, out_chs=num_1x1_c+<span class="number">2</span>*inc, kernel_size=<span class="number">1</span>, stride=key_stride)</div><div class="line"></div><div class="line">        self.layers = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'c1x1_a'</span>, self.BN_ReLU_Conv(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)),</div><div class="line">            (<span class="string">'c3x3_b'</span>, self.BN_ReLU_Conv(in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=<span class="number">3</span>, stride=key_stride, padding=<span class="number">1</span>, groups=G)),</div><div class="line">            (<span class="string">'c1x1_c'</span>, self.BN_ReLU_Conv(in_chs=num_3x3_b, out_chs=num_1x1_c+inc, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BN_ReLU_Conv</span><span class="params">(self, in_chs, out_chs, kernel_size, stride, padding=<span class="number">0</span>, groups=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="keyword">return</span> nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'norm'</span>, nn.BatchNorm2d(in_chs)),</div><div class="line">            (<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">            (<span class="string">'conv'</span>, nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        data_in = torch.cat(x, dim=<span class="number">1</span>) <span class="keyword">if</span> isinstance(x, list) <span class="keyword">else</span> x</div><div class="line">        <span class="keyword">if</span> self.has_proj:</div><div class="line">            data_o = self.c1x1_w(data_in)</div><div class="line">            data_o1 = data_o[:,:self.num_1x1_c,:,:]</div><div class="line">            data_o2 = data_o[:,self.num_1x1_c:,:,:]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            data_o1 = x[<span class="number">0</span>]</div><div class="line">            data_o2 = x[<span class="number">1</span>]</div><div class="line"></div><div class="line">        out = self.layers(data_in)</div><div class="line"></div><div class="line">        summ = data_o1 + out[:,:self.num_1x1_c,:,:]</div><div class="line">        dense = torch.cat([data_o2, out[:,self.num_1x1_c:,:,:]], dim=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> [summ, dense]</div></pre></td></tr></table></figure>

	
	</div>
  <a type="button" href="/2017/11/14/Networks/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/12/meet-in-the-middle的一些实例/" >Meet in the middle的一些实例</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-12  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>Meet in the Middle是在搜索问题经常会用到的一个技巧，其核心思想就是解决一个A&lt;-&gt;B的问题，分别从A端和B端出发，向对方进发，当他们在中点相遇的时候，就找到了A&lt;-&gt;B的一个解。</p>
<h3 id="先看一道简单题："><a href="#先看一道简单题：" class="headerlink" title="先看一道简单题："></a>先看一道简单题：</h3><p><a href="http://codeforces.com/contest/888/problem/E" target="_blank" rel="external">CF888E Maximum Subsequence</a></p>
<p>You are given an array <em>a</em> consisting of <em>n</em> integers, and additionally an integer <em>m</em>. You have to choose some sequence of indices $b_1, b_2, …, b_k (1 \le b_1 \lt b_2 \lt … \lt b_k\le n)$ in such a way that the value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img"> is maximized. Chosen sequence can be empty.</p>
<p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p>
<p><strong>Input</strong></p>
<p>The first line contains two integers <em>n</em> and <em>m</em> ($1 \le n\le 35$, $1 \le m \le 10^9$).</p>
<p>The second line contains <em>n</em> integers $a_1, a_2, …, a_n$ ($1 \le a_i \le10^9$).</p>
<p><strong>Output</strong></p>
<p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p>
<p>题目意思很简单，就是从一个大小为n的数组中挑选k个，使他们的和对m求余最大。</p>
<p>如果直接枚举a的所有子集，大小为$2^{35}$， 明显会超时。</p>
<p>如果利用meet in the middle的思路： 先枚举左边17，右边17，再让他们meet in the middle，然后利用求余的性质，左边和右边的和都小于m，进行排序，二分即可： $L_i+R_i \lt m$   or  $ m \lt L_i + R_i \lt 2*m$</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">40</span>;</div><div class="line"></div><div class="line"><span class="keyword">int</span> a[maxn];</div><div class="line"></div><div class="line"><span class="keyword">int</span> L[<span class="number">1</span>&lt;&lt;<span class="number">18</span>], R[<span class="number">1</span>&lt;&lt;<span class="number">18</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="comment">//freopen("in", "r", stdin);</span></div><div class="line">	<span class="keyword">int</span> n, m;</div><div class="line">	<span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</div><div class="line"></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</div><div class="line">		<span class="built_in">cin</span> &gt;&gt; a[i];</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">int</span> ans = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;<span class="number">18</span>); mask++) &#123;</div><div class="line">		<span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">18</span>; j++) &#123;</div><div class="line">			<span class="keyword">if</span> ((mask &gt;&gt; j) &amp; <span class="number">1</span>) &#123;</div><div class="line">				res = (res+a[j]) % m;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		L[mask] = res;</div><div class="line">		ans = max(ans, res);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">if</span> (n &lt;= <span class="number">18</span>) &#123;</div><div class="line">		<span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">		<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>)); mask++) &#123;</div><div class="line">		<span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">18</span>; j &lt; n; j++) &#123;</div><div class="line">			<span class="keyword">if</span> ((mask &gt;&gt; (j<span class="number">-18</span>)) &amp; <span class="number">1</span>) &#123;</div><div class="line">				res = (res+a[j]) % m;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		R[mask] = res;</div><div class="line">		ans = max(ans, res);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">int</span> Lsz = (<span class="number">1</span>&lt;&lt;<span class="number">18</span>);</div><div class="line">	<span class="keyword">int</span> Rsz = (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>));</div><div class="line">	sort(R, R+Rsz);</div><div class="line"></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Lsz; i++) &#123;</div><div class="line">		<span class="keyword">int</span> res = L[i];</div><div class="line">		<span class="keyword">int</span> *t1 = upper_bound(R, R+Rsz, m<span class="number">-1</span>-res);</div><div class="line">		<span class="keyword">if</span> (t1 != R) &#123;</div><div class="line">			t1--;</div><div class="line">			ans = max(ans, (res+(*t1))%m);</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">int</span> *t2 = upper_bound(R, R+Rsz, <span class="number">2</span>*m<span class="number">-1</span>-res);</div><div class="line">		<span class="keyword">if</span> (t2 != R) &#123;</div><div class="line">			t2--;</div><div class="line">			ans = max(ans, (res+(*t2))%m);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="进阶一点"><a href="#进阶一点" class="headerlink" title="进阶一点"></a>进阶一点</h3><p><a href="https://community.topcoder.com/stat?c=problem_statement&amp;pm=11644&amp;rd=14548" target="_blank" rel="external">topcoder srm 523 AlphabetPath</a></p>
<p><strong>Problem Statement</strong></p>
<p>The original Latin alphabet contained the following 21 letters: </p>
<p>A B C D E F Z H I K L M N O P Q R S T V X</p>
<p>You are given a 2-dimensional matrix of characters represented by the String[] letterMaze. The i-th character of the j-th element of letterMaze will represent the character at row i and column j. The matrix will contain each of the 21 letters at least once. It may also contain empty cells marked as ‘.’ (quotes for clarity).</p>
<p>A path is a sequence of matrix elements such that the second element is (horizontally or vertically) adjacent to the first one, the third element is adjacent to the second one, and so on. No element may be repeated on a path. A Latin alphabet path is a path consisting of exactly 21 elements, each containing a different letter of the Latin alphabet. The letters are not required to be in any particular order.</p>
<p>Return the total number of Latin alphabet paths in the matrix described by letterMaze.</p>
<p>题目意思就是给定一个$R\times C$的矩阵，格子里要么是空，要么包含0~20的整数，长度为21的路径，每个整数恰出现一次， $R,C\le 21$</p>
<p>那么，我们枚举middle点，这样有$R\times C$种选择，假设Middle点为x，从Middle点出发DFS10步，令$S(P)$为不包含Middle点的10个格子的数值的集合。那么$S(P_1)\cup S(P_2)….\cup {x}$ = {0,1…20},那么整个时间复杂度就变成了$O(RC\times 4 \times 3^9)$</p>
<h3 id="密码学中的应用"><a href="#密码学中的应用" class="headerlink" title="密码学中的应用"></a>密码学中的应用</h3><h4 id="DES"><a href="#DES" class="headerlink" title="DES"></a>DES</h4><p>首先介绍一下DES（Data Encryption Standard），DES是一种分组的对称加密技术，具体见下图（coursera crypto stanford笔记）：</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/WechatIMG11.jpeg" alt="DES"></p>
<p>那么如何attack DES呢？</p>
<p>Lemma: Suppose that DES is an ideal cipher ($2^{56}$ random invertible functions, key是56位)</p>
<p>为什么不能用double DES呢？因为我们可以用meet in the middle attack来攻击：</p>
<p>对于double DES来说：</p>
<ol>
<li>我们需要找到这样的$k_1$和$k_2$：$E(k_1, E(k_2, M))=C$,这和$E(k_2, M) = D(k_1, C)$一个意思</li>
<li>首先，我们用表M记录$k_2$和$C^\prime=DES(k_2, M)$的所有值，时间复杂度为$O(2^{56})$</li>
<li>然后我们就可以暴力枚举$k_1$，计算$C^{\prime\prime} =DES^{-1}(k_1, C)$, 看是否有对应的值在表中</li>
<li>这样attack的时间复杂度就变成了$O(2^{56}+2^{56}) \lt O(2^{63})$ ,这比期望的$2^{112}$要小很多，以及空间复杂度为$O(2^{56})$。</li>
</ol>
<p>而换成3DES就没有这样的问题了！</p>
<p>$C = E(K_3, D(K_2, E(K_1,P) ) ) $</p>

	
	</div>
  <a type="button" href="/2017/11/12/meet-in-the-middle的一些实例/#more" class="btn btn-default more">Read More</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
    
     <a href="/page/2/" type="button" class="btn btn-default"><i class="fa fa-arrow-circle-o-left"></i> Prev</a>
      

        <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
 
       <a href="/page/4/" type="button" class="btn btn-default ">Next<i class="fa fa-arrow-circle-o-right"></i></a>     
        

  
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/algorithms/">algorithms<span>23</span></a></li>
		
			<li><a href="/categories/competition/">competition<span>1</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/classfication/">classfication<span>3</span></a></li>
		
			<li><a href="/tags/Deep-Learning/">Deep Learning<span>1</span></a></li>
		
			<li><a href="/tags/summary/">summary<span>2</span></a></li>
		
			<li><a href="/tags/algorithms/">algorithms<span>2</span></a></li>
		
			<li><a href="/tags/classification/">classification<span>2</span></a></li>
		
			<li><a href="/tags/paper-notes/">paper notes<span>4</span></a></li>
		
			<li><a href="/tags/杂/">杂<span>1</span></a></li>
		
			<li><a href="/tags/object-detection/">object detection<span>7</span></a></li>
		
			<li><a href="/tags/deep-learning/">deep learning<span>29</span></a></li>
		
			<li><a href="/tags/RNN/">RNN<span>1</span></a></li>
		
			<li><a href="/tags/machine-learning/">machine learning<span>1</span></a></li>
		
			<li><a href="/tags/notes/">notes<span>6</span></a></li>
		
			<li><a href="/tags/face-detection/">face detection<span>1</span></a></li>
		
			<li><a href="/tags/GAN/">GAN<span>1</span></a></li>
		
			<li><a href="/tags/paper/">paper<span>1</span></a></li>
		
			<li><a href="/tags/computer-vision/">computer vision<span>4</span></a></li>
		
			<li><a href="/tags/segmentation/">segmentation<span>2</span></a></li>
		
			<li><a href="/tags/math/">math<span>2</span></a></li>
		
			<li><a href="/tags/classifcation/">classifcation<span>8</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2018/04/18/OHEM算法笔记/" ><i class="fa fa-file-o"></i>OHEM算法笔记</a>
      </li>
    
      <li>
        <a href="/2018/04/11/SSD笔记/" ><i class="fa fa-file-o"></i>SSD笔记</a>
      </li>
    
      <li>
        <a href="/2018/04/07/YOLO&amp;YOLOv2笔记/" ><i class="fa fa-file-o"></i>YOLO&amp;YOLOv2笔记</a>
      </li>
    
      <li>
        <a href="/2018/04/02/Faster-R-CNN笔记/" ><i class="fa fa-file-o"></i>Faster R-CNN笔记</a>
      </li>
    
      <li>
        <a href="/2018/03/22/Fast-R-CNN笔记/" ><i class="fa fa-file-o"></i>Fast R-CNN笔记</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/mowayao" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.weibo.com/mowayao" title="My weibo account." target="_blank"]);">My Weibo</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 Mowayao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$'], ['\[','\]'] ], 
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
   </html>
