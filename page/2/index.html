<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Page 2 | Mowayao&#39;s Blog</title>
  <meta name="author" content="Mowayao">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Mowayao&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110229492-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Mowayao&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class="fa fa-user"></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>一往无前虎山行<span class="blink-fast">∎</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">
      <i class="fa fa-heart blink-slow"></i>
      一往无前虎山行
</div>    
		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/12/meet-in-the-middle的一些实例/" >Meet in the middle的一些实例</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-12  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>Meet in the Middle是在搜索问题经常会用到的一个技巧，其核心思想就是解决一个A&lt;-&gt;B的问题，分别从A端和B端出发，向对方进发，当他们在中点相遇的时候，就找到了A&lt;-&gt;B的一个解。</p>
<h3 id="先看一道简单题："><a href="#先看一道简单题：" class="headerlink" title="先看一道简单题："></a>先看一道简单题：</h3><p><a href="http://codeforces.com/contest/888/problem/E" target="_blank" rel="external">CF888E Maximum Subsequence</a></p>
<p>You are given an array <em>a</em> consisting of <em>n</em> integers, and additionally an integer <em>m</em>. You have to choose some sequence of indices $b_1, b_2, …, b_k (1 \le b_1 \lt b_2 \lt … \lt b_k\le n)$ in such a way that the value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img"> is maximized. Chosen sequence can be empty.</p>
<p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p>
<p><strong>Input</strong></p>
<p>The first line contains two integers <em>n</em> and <em>m</em> ($1 \le n\le 35$, $1 \le m \le 10^9$).</p>
<p>The second line contains <em>n</em> integers $a_1, a_2, …, a_n$ ($1 \le a_i \le10^9$).</p>
<p><strong>Output</strong></p>
<p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p>
<p>题目意思很简单，就是从一个大小为n的数组中挑选k个，使他们的和对m求余最大。</p>
<p>如果直接枚举a的所有子集，大小为$2^{35}$， 明显会超时。</p>
<p>如果利用meet in the middle的思路： 先枚举左边17，右边17，再让他们meet in the middle，然后利用求余的性质，左边和右边的和都小于m，进行排序，二分即可： $L_i+R_i \lt m$   or  $ m \lt L_i + R_i \lt 2*m$</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">40</span>;</div><div class="line"></div><div class="line"><span class="keyword">int</span> a[maxn];</div><div class="line"></div><div class="line"><span class="keyword">int</span> L[<span class="number">1</span>&lt;&lt;<span class="number">18</span>], R[<span class="number">1</span>&lt;&lt;<span class="number">18</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="comment">//freopen("in", "r", stdin);</span></div><div class="line">	<span class="keyword">int</span> n, m;</div><div class="line">	<span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</div><div class="line"></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</div><div class="line">		<span class="built_in">cin</span> &gt;&gt; a[i];</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">int</span> ans = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;<span class="number">18</span>); mask++) &#123;</div><div class="line">		<span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">18</span>; j++) &#123;</div><div class="line">			<span class="keyword">if</span> ((mask &gt;&gt; j) &amp; <span class="number">1</span>) &#123;</div><div class="line">				res = (res+a[j]) % m;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		L[mask] = res;</div><div class="line">		ans = max(ans, res);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">if</span> (n &lt;= <span class="number">18</span>) &#123;</div><div class="line">		<span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">		<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>)); mask++) &#123;</div><div class="line">		<span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">18</span>; j &lt; n; j++) &#123;</div><div class="line">			<span class="keyword">if</span> ((mask &gt;&gt; (j<span class="number">-18</span>)) &amp; <span class="number">1</span>) &#123;</div><div class="line">				res = (res+a[j]) % m;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		R[mask] = res;</div><div class="line">		ans = max(ans, res);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">int</span> Lsz = (<span class="number">1</span>&lt;&lt;<span class="number">18</span>);</div><div class="line">	<span class="keyword">int</span> Rsz = (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>));</div><div class="line">	sort(R, R+Rsz);</div><div class="line"></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Lsz; i++) &#123;</div><div class="line">		<span class="keyword">int</span> res = L[i];</div><div class="line">		<span class="keyword">int</span> *t1 = upper_bound(R, R+Rsz, m<span class="number">-1</span>-res);</div><div class="line">		<span class="keyword">if</span> (t1 != R) &#123;</div><div class="line">			t1--;</div><div class="line">			ans = max(ans, (res+(*t1))%m);</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">int</span> *t2 = upper_bound(R, R+Rsz, <span class="number">2</span>*m<span class="number">-1</span>-res);</div><div class="line">		<span class="keyword">if</span> (t2 != R) &#123;</div><div class="line">			t2--;</div><div class="line">			ans = max(ans, (res+(*t2))%m);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="进阶一点"><a href="#进阶一点" class="headerlink" title="进阶一点"></a>进阶一点</h3><p><a href="https://community.topcoder.com/stat?c=problem_statement&amp;pm=11644&amp;rd=14548" target="_blank" rel="external">topcoder srm 523 AlphabetPath</a></p>
<p><strong>Problem Statement</strong></p>
<p>The original Latin alphabet contained the following 21 letters: </p>
<p>A B C D E F Z H I K L M N O P Q R S T V X</p>
<p>You are given a 2-dimensional matrix of characters represented by the String[] letterMaze. The i-th character of the j-th element of letterMaze will represent the character at row i and column j. The matrix will contain each of the 21 letters at least once. It may also contain empty cells marked as ‘.’ (quotes for clarity).</p>
<p>A path is a sequence of matrix elements such that the second element is (horizontally or vertically) adjacent to the first one, the third element is adjacent to the second one, and so on. No element may be repeated on a path. A Latin alphabet path is a path consisting of exactly 21 elements, each containing a different letter of the Latin alphabet. The letters are not required to be in any particular order.</p>
<p>Return the total number of Latin alphabet paths in the matrix described by letterMaze.</p>
<p>题目意思就是给定一个$R\times C$的矩阵，格子里要么是空，要么包含0~20的整数，长度为21的路径，每个整数恰出现一次， $R,C\le 21$</p>
<p>那么，我们枚举middle点，这样有$R\times C$种选择，假设Middle点为x，从Middle点出发DFS10步，令$S(P)$为不包含Middle点的10个格子的数值的集合。那么$S(P_1)\cup S(P_2)….\cup {x}$ = {0,1…20},那么整个时间复杂度就变成了$O(RC\times 4 \times 3^9)$</p>
<h3 id="密码学中的应用"><a href="#密码学中的应用" class="headerlink" title="密码学中的应用"></a>密码学中的应用</h3><h4 id="DES"><a href="#DES" class="headerlink" title="DES"></a>DES</h4><p>首先介绍一下DES（Data Encryption Standard），DES是一种分组的对称加密技术，具体见下图（coursera crypto stanford笔记）：</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/WechatIMG11.jpeg" alt="DES"></p>
<p>那么如何attack DES呢？</p>
<p>Lemma: Suppose that DES is an ideal cipher ($2^{56}$ random invertible functions, key是56位)</p>
<p>为什么不能用double DES呢？因为我们可以用meet in the middle attack来攻击：</p>
<p>对于double DES来说：</p>
<ol>
<li>我们需要找到这样的$k_1$和$k_2$：$E(k_1, E(k_2, M))=C$,这和$E(k_2, M) = D(k_1, C)$一个意思</li>
<li>首先，我们用表M记录$k_2$和$C^\prime=DES(k_2, M)$的所有值，时间复杂度为$O(2^{56})$</li>
<li>然后我们就可以暴力枚举$k_1$，计算$C^{\prime\prime} =DES^{-1}(k_1, C)$, 看是否有对应的值在表中</li>
<li>这样attack的时间复杂度就变成了$O(2^{56}+2^{56}) \lt O(2^{63})$ ,这比期望的$2^{112}$要小很多，以及空间复杂度为$O(2^{56})$。</li>
</ol>
<p>而换成3DES就没有这样的问题了！</p>
<p>$C = E(K_3, D(K_2, E(K_1,P) ) ) $</p>

	
	</div>
  <a type="button" href="/2017/11/12/meet-in-the-middle的一些实例/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/12/GatedRNN/" >GatedRNN</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-12  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="Gated-RNN"><a href="#Gated-RNN" class="headerlink" title="Gated RNN"></a>Gated RNN</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>$$<br>h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime)<br>$$</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.11.44.png" alt="RNN"></p>
<p>下面是RNN的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></div><div class="line"><span class="string">  activation function.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for this timestep, of shape (N, D).</span></div><div class="line"><span class="string">  - prev_h: Hidden state from previous timestep, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h = np.tanh(x.dot(Wx)+prev_h.dot(Wh)+b)</div><div class="line">  cache = (next_h,x,prev_h,Wx,Wh)</div><div class="line">  <span class="keyword">return</span> next_h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for a single timestep of a vanilla RNN.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dnext_h: Gradient of loss with respect to next hidden state</span></div><div class="line"><span class="string">  - cache: Cache object from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradients of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></div><div class="line"><span class="string">  - db: Gradients of bias vector, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  next_h,x,prev_h,Wx,Wh = cache</div><div class="line">  dout = dnext_h*(<span class="number">1</span>-next_h**<span class="number">2</span>)</div><div class="line">  db = np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">  dx = dout.dot(Wx.T)</div><div class="line">  dprev_h = dout.dot(Wh.T)</div><div class="line">  dWx = np.dot(x.T,dout)</div><div class="line">  dWh = np.dot(prev_h.T,dout)</div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the RNN forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for the entire timeseries, of shape (N, T, D).</span></div><div class="line"><span class="string">  - h0: Initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></div><div class="line"><span class="string">  - cache: Values needed in the backward pass</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  N,T,D = x.shape</div><div class="line">  N,H = h0.shape</div><div class="line">  cache = []</div><div class="line">  prev_h = h0</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">    prev_h,cache_n = rnn_step_forward(x[:,t,:],prev_h,Wx,Wh,b)</div><div class="line">    cache.append(cache_n)</div><div class="line">    h[:,t,:] = prev_h</div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of all hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of inputs, of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  N,T,H = dh.shape</div><div class="line">  N,D = cache[<span class="number">0</span>][<span class="number">1</span>].shape</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dWx = np.zeros((D,H))</div><div class="line">  dWh = np.zeros((H,H))</div><div class="line">  db = np.zeros((H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">    dx[:,t,:],dprev_h, dWx_n, dWh_n, db_n = rnn_step_backward(dprev_h+dh[:,t,:],cache[t])</div><div class="line">    dWh += dWh_n</div><div class="line">    dWx += dWx_n</div><div class="line">    db += db_n</div><div class="line">  dh0 = dprev_h</div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure>
<h3 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h3><p>$$<br>h^\prime, y = f_1(h,x) \ b^\prime, c = f_2(b, y) \ ….<br>$$</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.13.12.png" alt="Deep RNN"></p>
<h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>$$<br>h^\prime, a = f_1(h, x), b^\prime, c= f_2(b, x), y = f_3(a, c)<br>$$</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.15.10.png" alt="双向RNN"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>$$<br>z^i = \tanh(W^ix^t+W^ih^{t-1})\\<br>z^f=\tanh(W^fx^t+W^fh^{t-1})\\<br>z^o=\tanh(W^0x^t+W^oh^{t-1})\\<br>$$</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.18.36.png" alt="LSTM"></p>
<p>对比分析：</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.23.59.png" alt="LSTM对比"></p>
<p>最左边的是标准的LSTM， 左边第二个是GRU， </p>
<p>可以看出： 没有output gate，forget gate, input gate, input activation function, output activation function都会对结果变差。forget gate和关于$c^t$的$\tanh$激活函数对性能影响较大。</p>
<p>下面是LSTM的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for a single timestep of an LSTM.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data, of shape (N, D)</span></div><div class="line"><span class="string">  - prev_h: Previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - prev_c: previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases, of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - next_c: Next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h, next_c, cache = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for a single timestep of an LSTM.        #</span></div><div class="line">  <span class="comment"># You may want to use the numerically stable sigmoid implementation above.  #</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  a = x.dot(Wx)+prev_h.dot(Wh)+b</div><div class="line">  N,H = prev_h.shape</div><div class="line">  i = sigmoid(a[:,:H])</div><div class="line">  f = sigmoid(a[:,H:<span class="number">2</span>*H])</div><div class="line">  o = sigmoid(a[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div><div class="line">  g = np.tanh(a[:,<span class="number">3</span>*H:])</div><div class="line"></div><div class="line">  next_c = f*prev_c + i*g</div><div class="line">  next_h = o*np.tanh(next_c)</div><div class="line">  cache = (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h)</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> next_h, next_c, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass foLSTM: forward</span></div><div class="line"><span class="string">  - dnext_h: Gradients of next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dnext_c: Gradients of next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dprev_c: Gradient of previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dprev_c, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h) = cache</div><div class="line">  (N,H) = dnext_h.shape</div><div class="line">  (N,D) = x.shape</div><div class="line"></div><div class="line"></div><div class="line">  dx = np.zeros(x.shape)</div><div class="line">  dprev_c = np.zeros(prev_c.shape)</div><div class="line">  dprev_h = np.zeros(prev_h.shape)</div><div class="line">  dWx = np.zeros(Wx.shape)</div><div class="line">  dWh = np.zeros(Wh.shape)</div><div class="line">  db = np.zeros(b.shape)</div><div class="line"></div><div class="line"></div><div class="line">  di = dnext_c*g</div><div class="line">  df = dnext_c*prev_c</div><div class="line">  do = dnext_h*np.tanh(next_c)</div><div class="line">  dg = dnext_c*i</div><div class="line"></div><div class="line">  da = np.zeros(a.shape)</div><div class="line"></div><div class="line">  da[:,:H] = di*i*(<span class="number">1</span>-i) <span class="comment">#i</span></div><div class="line">  da[:,H:<span class="number">2</span>*H] = df*f*(<span class="number">1</span>-f) <span class="comment">#f</span></div><div class="line">  da[:,<span class="number">2</span>*H:<span class="number">3</span>*H] = do*o*(<span class="number">1</span>-o) <span class="comment">#o</span></div><div class="line">  da[:,<span class="number">3</span>*H:] = dg*(<span class="number">1</span>-g**<span class="number">2</span>) <span class="comment">#g</span></div><div class="line"></div><div class="line">  dprev_h = np.dot(da,Wh.T)</div><div class="line">  dWx = np.dot(x.T,da)</div><div class="line">  dWh = np.dot(prev_h.T,da)</div><div class="line">  db = np.sum(da,axis=<span class="number">0</span>)</div><div class="line">  dprev_c = dnext_c*f</div><div class="line">  dx = np.dot(da,Wx.T)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for an LSTM over an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the LSTM forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Note that the initial cell state is passed as input, but the initial cell</span></div><div class="line"><span class="string">  state is set to zero. Also note that the cell state is not returned; it is</span></div><div class="line"><span class="string">  an internal variable to the LSTM and is not accessed from outside.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - h0: Initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,T,D) = x.shape</div><div class="line">  (N,H) = h0.shape</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  cache = []</div><div class="line">  prev_c = np.zeros((N,H))</div><div class="line">  prev_h = h0</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">      prev_h,prev_c,cache_n = lstm_step_forward(x[:,t,:],prev_h,prev_c,Wx,Wh,b)</div><div class="line">      cache.append(cache_n)</div><div class="line">      h[:,t,:] = prev_h</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for an LSTM over an entire sequence of data.]</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,D) = cache[<span class="number">0</span>][<span class="number">0</span>].shape</div><div class="line">  (N,T,H) = dh.shape</div><div class="line"></div><div class="line">  dprev_c = np.zeros((N,H))</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dh0 = np.zeros((N,H))</div><div class="line">  dWx = np.zeros((D,<span class="number">4</span>*H))</div><div class="line">  dWh = np.zeros((H,<span class="number">4</span>*H))</div><div class="line">  db= np.zeros((<span class="number">4</span>*H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line"></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">      dx_n, dprev_h, dprev_c, dWx_n, dWh_n, db_n = lstm_step_backward(dh[:,t,:]+dprev_h,dprev_c,cache[t])</div><div class="line">      dWx += dWx_n</div><div class="line">      dWh_n += dWh_n</div><div class="line">      db += db_n</div><div class="line">      dx[:,t,:] = dx_n</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure>
<p>###GRU</p>
<p>z在GRU充当的是LSTM里面forget gate和input gate一样的作用，将两者耦合在一起。</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.19.17.png" alt="GRU"></p>

	
	</div>
  <a type="button" href="/2017/11/12/GatedRNN/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/01/mixup/" >mixup-Beyond Empirical Risk Minimization</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-01  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p><strong>Gist</strong>: The authors propose a new training strategy  dubbed <strong>mixup</strong> that trains a neural network on convex combinations of pairs of examples and their labels and improves the generalization of state-of-the-art neural network architectures.    </p>
<p>​    </p>
<p><strong>Pytorch Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (x1, y1), (x2, y2) <span class="keyword">in</span> zip(loader1, loader2): </div><div class="line">  	lam = numpy.random.beta(alpha, alpha)</div><div class="line">	x = Variable(lam * x1 + (<span class="number">1.</span> - lam) * x2)</div><div class="line">	y = Variable(lam * y1 + (<span class="number">1.</span> - lam) * y2) optimizer.zero_grad()</div><div class="line">    loss(net(x), y).backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<p><strong>Empirical Risk Minimization</strong></p>
<p>We need to minimize the <strong>expected risk</strong>, that is the average of the loss function $l$ over the data distribution $P$</p>
<p>$$R(f ) = \int l(f (x), y)dP (x, y)$$</p>
<p>$l$ is the loss function, $P(x,y)$ is a joint data distribution, $f\in F$ is a function that describes the relationship between a random vector X and a random target vector Y .</p>
<p>Unsually, the distribution of P is unknown. In most pracitical situation, we may approximate $P$ by the <strong><em>empirical distribution</em></strong>, though it is easy to compute, it ofen leads to the undesirable behaviour of $f$ outside the training data.</p>
<p>$$P_\sigma(x,y)=\frac{1}{n}\sum_{i=1}^{n}\sigma(x=x_i, y=y_i)$$</p>
<p>where $\sigma(x = x_i, y = y_i)$ is a Dirac mass centered at $(x_i, y_i)$</p>
<p>$$R_\sigma(f) = \frac{1}{n}\sum_{i=1}^nl(f(x_i), y_i)$$</p>
<p><strong>Vicinal Risk Minimization</strong></p>
<p>$$P_v (\widetilde{x}, \widetilde{y})=\frac{1}{n}\sum_{i=1}^nv(\widetilde{x}, \widetilde{y}|x_i,y_i)$$<br>where $v(\widetilde{x}, \widetilde{y}|x_i,y_i)$ is  a vicinity distribution that measures the probability of finding the virtual feature-target pair $(\widetilde{x}, \widetilde{y})$ in the vicinity of the training feature-target pair $(x_i,y_i)$</p>
<p>This paper propose a generic vicinal distribution, <strong><em>mixup</em></strong>:</p>
<p>$$\mu(\widetilde{x}, \widetilde{y}|x_i,y_i)=\frac{1}{n}\sum_j^n\mathbb{E}_\lambda[\sigma(\widetilde{x}=\lambda \cdot x_i+(1-\lambda)\cdot x_j,\widetilde{y} =\lambda \cdot y_i + (1-\lambda) \cdot y_j)]$$<br>where $\lambda \sim Beta(\alpha, \alpha)$ , for $\alpha \in (0, \infty)$Sampling from the mixup vicinal distribution:<br>$$\widetilde{x} = \lambda \cdot x_i + (1 − \lambda)\cdot x_j$$<br>$$\widetilde{y} = \lambda \cdot y_i + (1 − \lambda)\cdot y_j$$</p>

	
	</div>
  <a type="button" href="/2017/11/01/mixup/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/" >Single Shot Scale-invariant Face Detector</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-11-01  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>The authors propose to tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besides, they try to improve the recall rate of small faces by a scale compensation anchor matching strategy. Max-out background label is used to reduce the false positive rate of small faces.</p>
<p>Key points:</p>
<ul>
<li>VGG net (throgh Pool5 layer) and some extra convolutional layers</li>
<li>Anchor  is 1:1 aspect ratio (face annotation)</li>
<li>two stages to improve the anchor matching strategy<ul>
<li>stage one: decrese the jaccord overlap threshold from 0.5 to 0.35</li>
<li>stage two: decrese the threshold to 0.1 and sort to select the top-N</li>
</ul>
</li>
<li>max-out operation is performed on the background label scores</li>
</ul>
<p>model architecture:</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-01%20%E4%B8%8B%E5%8D%887.46.53.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/10/20/A-List-of-Saliency-Detection-Papers/" >A List of Saliency Detection Papers</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-10-20  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<ol>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Deep%20Spatial%20Contextual%20Long-term%20Recurrent%20Convolutional%20Network%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Fast%20and%20Compact%20Saliency%20Score%20Regression%20Network%20Based%20on%20Fully%20Convolutional%20Network.pdf" target="_blank" rel="external">A Fast and Compact Saliency Score Regression Network Based on Fully Convolutional Network</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Amulet.pdf" target="_blank" rel="external">Amulet</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/DHSNet:%20Deep%20Hierarchical%20Saliency%20Network%20for%20Salient%20Object%20Detection%20.pdf" target="_blank" rel="external">DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Group-wise%20Deep%20Co-saliency%20Detection.pdf" target="_blank" rel="external">Group-wise Deep Co-saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Large-Scale%20Optimization%20of%20Hierarchical%20Features%20for%20Saliency%20Prediction%20in%20Natural%20Images.pdf" target="_blank" rel="external">Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Learning%20Uncertain%20Convolutional%20Features%20for%20Accurate%20Saliency%20Detection.pdf" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/PiCANet.pdf" target="_blank" rel="external">PiCANet</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Recurrent%20Attentional%20Networks%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">Recurrent Attentional Networks for Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/SalGAN:%20Visual%20Saliency%20Prediction%20with%20Generative%20Adversarial%20Networks.pdf" target="_blank" rel="external">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Forward%20and%20Backward%20Cues%20in%20Deep-CNNs.pdf" target="_blank" rel="external">Saliency Detection by Forward and Backward Cues in Deep-CNNs</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Multi-Context%20Deep%20Learning.pdf" target="_blank" rel="external">Saliency Detection by Multi-Context Deep Learning.pdf</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Shallow%20and%20Deep%20Convolutional%20Networks%20for%20Saliency%20Prediction.pdf" target="_blank" rel="external">Shallow and Deep Convolutional Networks for Saliency Prediction</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Supervised%20Adversarial%20Networks%20for%20Image%20Saliency%20Detection.pdf" target="_blank" rel="external">Supervised Adversarial Networks for Image Saliency Detection</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Two-Stream%20Convolutional%20Networks%20for%20Dynamic%20Saliency%20Prediction.pdf" target="_blank" rel="external">Two-Stream Convolutional Networks for Dynamic Saliency Prediction</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Detection%20Based%20on%20Multiscale%20Deep%20CNN%20Features.pdf" target="_blank" rel="external">Visual Saliency Detection Based on Multiscale Deep CNN Features</a></li>
<li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Prediction%20Using%20a%20Mixture%20of%20Deep%20Neural%20Networks.pdf" target="_blank" rel="external">Visual Saliency Prediction Using a Mixture of Deep Neural Networks</a></li>
</ol>

	
	</div>
  <a type="button" href="/2017/10/20/A-List-of-Saliency-Detection-Papers/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/09/05/图个新鲜/" >图个新鲜</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-09-05  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/2017-09-05%2017-15-19%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/09/05/图个新鲜/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/09/05/overview-of-object-detection/" >Overview of Object Detection</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-09-05  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>主要有三个步骤：</p>
<ol>
<li>用selective search提取可能的objects<ol>
<li>使用<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph Based Image Segmentation</a>中的方法来得到region</li>
<li>得到所有region之间两两的相似度</li>
<li>合并最像的两个region</li>
<li>重新计算新合并region与其他region的相似度</li>
<li>重复上述过程直到整张图片都聚合成一个大的region</li>
<li>使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果</li>
</ol>
</li>
<li>用CNN提取特征</li>
<li>用SVM对区域进行分类</li>
</ol>
<p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/rcnn.jpg" alt="[Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; 2014.](https://arxiv.org/abs/1311.2524)"></p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>在feature map加入RoI Pooling,然今后做分类和回归（位置），这样可以end-to-end的训练了，缺点是依然依赖于selective search</p>
<p>每一个RoI都有一个四元组$（r,c,h,w）$表示，其中$（r，c）$表示左上角，而$（h，w）$则代表高度和宽度。这一层使用最大池化（max pooling）来将RoI区域转化成固定大小的$H<em>W$的特征图。假设一个RoI的窗口大小为$h</em>w$,则转换成$H<em>W$之后，每一个网格都是一个$h/H </em> w/W$大小的子网，利用最大池化将这个子网中的值映射到$H*W$窗口即可。Pooling对每一个特征图通道都是独立的</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/ROI.png" alt=""></p>
<p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/fastrcnn.jpg" alt=""></p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>加入region proposal network，为了代替selective search使得模型能够完全的end-to-end的训练。</p>
<p>这样的话就存在４个loss:</p>
<ol>
<li>RPN分类：是否是object</li>
<li>RPN box坐标回归</li>
<li>object分类</li>
<li>最终的坐标回归</li>
</ol>
<p><img src="http://shartoo.github.io/images/blog/rcnn9.png" alt=""></p>
<p>Anchor:</p>
<p>Anchors是一组大小固定的参考窗口：三种尺度{ $128^2，256^2，512^2$ }×三种长宽比{1:1，1:2，2:1}，如下图所示，<strong>表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小</strong>，相当于模板，对任意图像任意滑窗位置都是这9种模板。<strong>继而根据图像大小计算滑窗中心点对应原图区域的中心点</strong>，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，由此原图位置并根据与Ground Truth重复率贴上正负标签，让RPN学习该Anchors是否有物体即可。对于每个滑窗位置，产生<strong>k=9</strong>个anchor对于一个大小为$W*H$的卷积feature map，总共会产生$WHk$个anchor。</p>
<p><img src="http://shartoo.github.io/images/blog/rcnn12.png" alt=""></p>
<p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/fasterrcnn.jpg" alt="[Ren, Shaoqing, et al. &quot;Faster R-CNN: Towards real-time object detection with region proposal networks.&quot; 2015.](https://arxiv.org/abs/1506.01497)"></p>
<p><img src="http://img.blog.csdn.net/20160414164536029" alt=""></p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><strong>同时采用lower和upper的feature map做检测</strong></p>
<p>假设每个feature map cell有k个default box，那么对于每个default box都需要预测c个类别score和4个offset，那么如果一个feature map的大小是$m\times n$，也就是有<strong>$m\times n$</strong>个feature map cell，那么这个feature map就一共有$（c+4)\times k\times m\times n$ 个输出。这些输出个数的含义是：采用$3\times3$的卷积核对该层的feature map卷积时卷积核的个数，包含两部分：数量$c\times k\times m\times n$是confidence输出，表示每个default box的confidence，也就是类别的概率；数量$4\times k\times m\times n$是localization输出，表示每个default box回归后的坐标）。训练中还有一个东西：<strong>prior box</strong>，是指实际中选择的default box（每一个feature map cell 不是k个default box都取）。</p>
<ul>
<li>feature map cell 就是将 feature map 切分成 8×8 或者 4×4 之后的一个个格子；</li>
<li>而 default box 就是每一个格子上，一系列固定大小的 box，即图中虚线所形成的一系列 boxes。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20160918092529925" alt=""></p>
<p><img src="http://img.blog.csdn.net/20160918092701558" alt=""></p>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p><img src="http://upload-images.jianshu.io/upload_images/75110-91ee171b49f3ea20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>YOLO首先将图像分为S×S的格子（grid cell）。如果一个目标的中心落入格子，该格子就负责检测该目标。每一个格子（grid cell）预测bounding boxes和该boxes的置信值（confidence score）。置信值代表box包含一个目标的置信度。然后，我们定义置信值为。如果没有目标，置信值为零。另外，我们希望预测的置信值和ground truth的intersection over union (IOU)相同。</p>
<p>每一个bounding box包含5个值：$x，y，w，h$和confidence。$（x，y）$代表与格子相关的box的中心。$（w，h）$为与全图信息相关的box的宽和高。confidence代表预测boxes的IOU和gound truth。</p>
<p>每个格子（grid cell）预测条件概率值C($Pr(Class_i|Object) $)。概率值C代表了格子包含一个目标的概率，每一格子只预测一类概率。在测试时，每个box通过类别概率和box置信度相乘来得到特定类别置信分数：<br>$$<br>Pr(Class_i|Object) \cdot Pr(Object)\cdot IOU_{pred}^{truth} = Pr(Class_i)\cdot IOU_{pred}^{truth}<br>$$<br>它将图片划分为S×S的网格，对于每个网格单元预测边界框(B)、边界框的置信度以及类别概率(C)，因此这些预测值可以表示为S×S×(B∗5+C)的张量。</p>

	
	</div>
  <a type="button" href="/2017/09/05/overview-of-object-detection/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/18/如何在圆内均匀采样/" >如何在圆内均匀采样</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>这个问题之前在面试网易游戏的时候碰到过，当时只想了一个很朴素(naive)的做法，现在有时间重新推导了一下：</p>
<p>假设圆是一个单元圆，半径为1, 面积为$\pi$。</p>
<p>当时面试的时候想的方法是用一个长度为1的外接正方形来代替采样，如果在圆内，则返回，不然继续采样，这样的采样方法明显是不稳定的，但是期望的采样步数很容易计算，是$\frac{4}{\pi}$。</p>
<p>后来想到说可以先采样角度，每个角度确定一个半径，再在半径上均匀采样，面试官有提示说在半径上是均匀的吗？ 明显在半径上采样是不均匀的，因为在半径上的每一个点对应的周长是不一样的！！</p>
<p>正确的姿势是这样的：</p>
<p>我们计算长度为$r(0\le r\le1)$的概率为$p(r)$:</p>
<p>首先我们需要计算落在$r\sim r+\Delta r$的概率，很直观，就是面积/$\pi$，然后因为均匀采样所以需要除以$\Delta r$求得$p(r)$也就是：<br>$$<br>p(r\sim r+\Delta r) = \lim_{\Delta r\rightarrow0}\frac{\pi(r+\Delta r)^2-\pi r^2}{\pi}<br>$$</p>
<p>$$<br>p(r) = \lim_{\Delta r\rightarrow0}\frac{\pi(r+\Delta r)^2-\pi r^2}{\pi \Delta r}=2r<br>$$</p>
<p>接下来就是求$p(r)$的CDF,$P(r)$，积分即可：<br>$$<br>P(r) = \int_0^r p(x) dx=r^2<br>$$<br>然后求得它的逆函数$P^{-1}(r)$:</p>
<p>$$P^{-1}(r) = \sqrt r$$</p>
<p>算到这里，答案呼之欲出，我们用一个随机变量$\zeta$ 在$[0,1]$均匀采样，然后在通过$r = \sqrt \zeta$求得r,为什么是均匀的呢，只要把$\sqrt \zeta$带入$P(r)$就可以发现$P(r)=\zeta$。</p>

	
	</div>
  <a type="button" href="/2017/05/18/如何在圆内均匀采样/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/17/一个离散概率分布中采样/" >一个离散概率分布中采样</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-17  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>先考虑一个简单的例子：</p>
<p>一个n-sided dice. 其每一面是均匀的，也就是每一面的概率都是$\frac{1}{n}$,我们可以把区间[0,1)分成n份，采样的过程就可以变成：从[0,1)采样得到x，然后返回$\lfloor n\times x\rfloor$。</p>
<blockquote>
<h4 id="Algorithm-Simulating-a-Fair-Die"><a href="#Algorithm-Simulating-a-Fair-Die" class="headerlink" title="Algorithm: Simulating a Fair Die"></a>Algorithm: Simulating a Fair Die</h4><ol>
<li>Generate a uniformly-random value xx in the range [0,1).</li>
<li>Return $⌊x\times n⌋$</li>
</ol>
</blockquote>
<h2 id="朴素的做法"><a href="#朴素的做法" class="headerlink" title="朴素的做法"></a>朴素的做法</h2><p>但如果不是均匀的呢？假设给定各个离散值的概率p(x)，先计算CDF，即 P(x)。然后从[0,1)采样，得到a，我们需要确定a所在的区间，朴素的做法就是二分搜索P（x）（单调性）。所以时间复杂度为$O(\log(n))$</p>
<h2 id="The-Alias-Method"><a href="#The-Alias-Method" class="headerlink" title="The Alias Method"></a>The Alias Method</h2><p>如果想要用$O(1)$的时间采样呢？</p>
<p>考虑如下的情况：</p>
<p>有四个概率：$\frac{1}{2}, \frac{1}{3}, \frac{1}{12},\frac{1}{12}$</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodInitialProbabilities.png" alt=""></p>
<p>首先，将他们归一化，用均值做归一化操作</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodScaled.png" alt=""></p>
<p>先画一个$1\times 4$的矩形：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup.png" alt=""></p>
<p>可以看到$\frac{1}{2}, \frac{1}{3}$并不是完全在矩形内，如果我们允许将自身的矩阵切除，然后补到其他区域内？例如将$\frac{1}{2}$切掉一部分补到最后那个区域：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup2.png" alt=""></p>
<p>到现在，还是在矩阵之外的块，接下来，把$\frac{1}{2}$切掉足够的部分补到第三个中：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup3.png" alt=""></p>
<p>最后：</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup4.png" alt=""></p>
<p>完美！</p>
<p>从上述可以看出有几条非常赞的性质：</p>
<ol>
<li>每个概率对应的面积都没有改变，随之对应的就是每个bar都是满的，这样保证了我每次采样都会命中！</li>
<li>每个bar最多有两种颜色。</li>
</ol>
<p>alias method主要依赖于两张表，一张概率表P还有一张alias表 Alias</p>
<p>构建完上述的表格以后，如何采样呢？</p>
<p><img src="http://www.keithschwarz.com/darts-dice-coins/images/completedAliasSetup.png" alt=""></p>
<p>首先对每列进行采样，列确定后，再采样，利用P和alias。过程非常简单，时间效率是$O(1)$</p>
<p>接下来就是证明这个alias表和P表是否一定存在！</p>
<blockquote>
<p><strong>Theorem:</strong> Given k width-one rectangles of heights $h_0,h_1,…,h_{k−1}$ such that $\sum_{i=0}^{k-1}h_i=k$, there is a way of cutting the rectangles and distributing them into k columns, each of which has height 1, such that each column contains at most two different rectangles and the $i$th column contains at least one piece of the $i$th rectangle.</p>
</blockquote>
<p>证明：</p>
<p>当k=1的时候，很明显是成立的。</p>
<p>假设当$k=x$的时候成立，那么我们就需要证明$k=x+1$时，是否满足。<br>考虑任一个宽度为$x+1$的矩形，高度分别是：$h_0, h_1, …, h_{k}$, 且满足$\sum_{i = 0}^{k}{h_i} =  x+ 1$,假设一些高度$h_l \le 1$ 还有一些$h_g\ge 1$。不可能同时大于0或者小于0。</p>
<p>接下来就是用$h_g$把$h_l$填满，这样我们就只剩下$x$个未解决的。所以。。成立！</p>
<p>具体的做法如下：</p>
<blockquote>
<h4 id="Algorithm-Naive-Alias-Method"><a href="#Algorithm-Naive-Alias-Method" class="headerlink" title="Algorithm: Naive Alias Method"></a>Algorithm: Naive Alias Method</h4><ul>
<li>Initialization:<ol>
<li>Multiply each probability $p_i$ by n.</li>
<li>Create arrays Alias and Prob, each of size n.</li>
<li>For j=1 to n−1:<ol>
<li>Find a probability pl satisfying $p_l\le1$.</li>
<li>Find a probability $p_g$ (with $l\ne g$) satisfying $p_g\ge1$</li>
<li>Set $Prob[l]=p_l$.</li>
<li>Set $Alias[l]=g$.</li>
<li>Remove $p_l$ from the list of initial probabilities.</li>
<li>Set $p_g:=p_g−(1−p_l)$.</li>
</ol>
</li>
<li>Let i be the last probability remaining, which must have weight 1.</li>
<li>Set $Prob[i]=1$.</li>
</ol>
</li>
<li>Generation:<ol>
<li>Generate a fair die roll from an n-sided die; call the side i.</li>
<li>Flip a biased coin that comes up heads with probability $Prob[i]$.</li>
<li>If the coin comes up “heads,” return i.</li>
<li>Otherwise, return $Alias[i]$.</li>
</ol>
</li>
</ul>
</blockquote>

	
	</div>
  <a type="button" href="/2017/05/17/一个离散概率分布中采样/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2017/05/12/cnn-case-study/" >CNN Case Stud</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2017-05-12  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>结构：</p>
<p>CONV1-&gt;MAX POOL1-&gt;NORM1-&gt;CONV2-&gt;MAX POOL2-&gt;NORM-&gt;CONV3-&gt;CONV4-&gt;CONV5-&gt;MAX POOL3-&gt;FC6-&gt;FC7-&gt;FC8</p>
<p>输入： $227\times 227\times 3$ 的图像</p>
<p>第一层（CONV1），96个$11\times11$的卷积和，stride为4，,因为(227-11)/4+1=55</p>
<p>参数的大小是$11\times11\times3\times96=35K$,输出为$55\times55\times96$</p>
<p>第二层（MAX POOL1）， $3\times 3$， 步数为2，因为(55-3)/2+1=27,所以，输出为$27\times27\times96$</p>
<p>第三层（NORM1）</p>
<p>第四层CONV2，256个$5\times5$的卷积和，stride为1，pad为2，因为（27-5+2*2）/1+1= 27，所以输出为$27\times27\times256$</p>
<p>第五层（MAX POOL2）， $3\times 3$， stride为2，因为(27-3)/2+1=13,所以，输出为$13\times13\times256$</p>
<p>第六层 （NORM2）</p>
<p>第七层（CONV3），384个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times384$</p>
<p>第八层（CONV4），384个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times384$</p>
<p>第九层（CONV5），256个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times256$</p>
<p>第十层（MAX POOL2）， $3\times 3$， 步数为2，因为(13-3)/2+1=6,所以，输出为$6\times6\times256$</p>
<p>第十一层（FC6），4096个neurons</p>
<p>第十二层（FC7）， 4096个neurons</p>
<p>第十三层（FC8）， 1000个neurons</p>
<h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.11.48.png" alt=""></p>
<p>为什么要使用小的卷积核（$3\times3$ conv）?</p>
<p>因为3个$3\times3$stride为1的conv堆起来的receptive field是和$7\times7$的conv layer是一样的，这样的话网络可以更深，同时非线性能力提高，而且前者参数数量为：$3*(3^2C^2)$ 后者为 $7^2C^2$,前者数量较少。</p>
<p>INPUT: $[224\times224\times3]$        memory:  $224<em>224</em>3$=150K   params: 0</p>
<p>CONV3-64:$ [224\times224\times64] $ memory:  $224<em>224</em>64$=3.2M   params:$ (3<em>3</em>3)*64 = 1,728$</p>
<p>CONV3-64:$ [224\times224\times64]$  memory:  $224<em>224</em>64$ =3.2M   params: $(3<em>3</em>64)*64 = 36,864$</p>
<p>POOL2: $[112\times112\times64]$  memory:  $112<em>112</em>64$=800K   params: 0</p>
<p>CONV3-128: $[112\times112\times128]$  memory:  $112<em>112</em>128$=1.6M   params: $(3<em>3</em>64)*128 = 73,728$</p>
<p>CONV3-128: $[112\times112\times128] $ memory: $112<em>112</em>128$=1.6M   params:$ (3<em>3</em>128)*128 = 147,456 $</p>
<p>POOL2:$ [56\times56\times128]$  memory:  $56<em>56</em>128=400K$   params: 0</p>
<p>CONV3-256:$ [56\times56\times256] $ memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>128)*256 = 294,912$</p>
<p>CONV3-256:$ [56\times56\times256]$  memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>256)*256 = 589,824 $</p>
<p>CONV3-256: $[56\times56\times256] $ memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>256)*256 = 589,824$</p>
<p>POOL2: $[28\times28\times256]$  memory:  $28<em>28</em>256=200K$   params: 0</p>
<p>CONV3-512:$ [28\times28\times512] $ memory:  $28<em>28</em>512=400K$   params:$ (3<em>3</em>256)*512 = 1,179,648$</p>
<p>CONV3-512: $[28\times28\times512]$  memory: $ 28<em>28</em>512=400K $  params:$ (3<em>3</em>512)*512 = 2,359,296$</p>
<p>CONV3-512:$ [28\times28\times512]$  memory: $ 28<em>28</em>512=400K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p>
<p>POOL2:$ [14\times14\times512]$  memory:  $14<em>14</em>512=100K$   params: 0 </p>
<p>CONV3-512:$ [14\times14\times512]$  memory: $ 14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p>
<p>CONV3-512:$ [14\times14\times512] $ memory: $ 14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296 $</p>
<p>CONV3-512: $[14\times14\times512]$  memory:  $14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p>
<p>POOL2: $[7\times7\times512] $ memory:  $7<em>7</em>512=25K$  params: 0</p>
<p>FC: $[1\times1\times4096]$  memory:  4096  params: $7<em>7</em>512*4096 = 102,760,448 $</p>
<p>FC: $[1\times1\times4096]$  memory:  4096  params: $4096*4096 = 16,777,216$</p>
<p>FC: $[1\times1\times1000]$  memory:  1000 params: $4096*1000 = 4,096,000 $</p>
<p>总结一下：对于一张图片来说，需要花费的内存是24M*4 bytes = 96MB，而总共的参数有138M </p>
<p>VGG的FC7的特征非常棒！通常用来提特征。</p>
<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>22层，有高效的inception module，没有FC层</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.15.37.png" alt=""></p>
<p>重点分析一下Inception module,下图是一个朴素的inception module</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.12.08.png" alt=""></p>
<p>采用的并行filter运算，有多个receptive field size的卷积，（$1\times1$,$3\times3$,$5\times5$）,从左向右第一个输出的size为$28\times28\times128$，第二个输出的size为$28\times28\times192$,第三个为$28\times28\times96$，第四个为$28\times28\times256$，concate以后的size为：$28\times28\times672$</p>
<p>缺点就是卷积运算过多：</p>
<p>$1\times1$ conv, 128=&gt;   $28\times28\times128\times1\times1\times256$</p>
<p> $3\times3$ conv, 192=&gt; $28\times28\times192\times3\times3\times256$</p>
<p> $5\times5$ conv, 96=&gt; $28\times28\times96\times5\times5\times256$</p>
<p>总共需要854M次运算</p>
<p>而且，最终的输出太大了！我们需要减少feature depth,可以用$1\times1$的卷积（$1\times1$ conv “bottleneck” layers）来解决，例如一个$56\times56\times64$的feature map经过32个$1\times1$以后，得到$56\times56\times32$,这样做就是将深度投影到较低的维度，（feature map的组合）</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.12.36.png" alt=""></p>
<p>卷积运算的数量：</p>
<p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$</p>
<p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$</p>
<p>$[1\times1 conv, 128] $ $28\times28\times128\times1\times1\times256$</p>
<p>$[3\times3 conv, 192]$  $28\times28\times192\times3\times3\times64$</p>
<p>$[5\times5 conv, 96]$  $28\times28\times96\times5\times5\times64$</p>
<p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$ </p>
<p>Total: 358M ops</p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><ul>
<li>每一层CONV层接BN</li>
<li>没有dropout</li>
</ul>
<p>具体见：</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.17.12.png" alt=""></p>

	
	</div>
  <a type="button" href="/2017/05/12/cnn-case-study/#more" class="btn btn-default more">Read More</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
    
     <a href="/" type="button" class="btn btn-default"><i class="fa fa-arrow-circle-o-left"></i> Prev</a>
      

        <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
 
       <a href="/page/3/" type="button" class="btn btn-default ">Next<i class="fa fa-arrow-circle-o-right"></i></a>     
        

  
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/algorithms/">algorithms<span>5</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/paper-notes/">paper notes<span>4</span></a></li>
		
			<li><a href="/tags/杂/">杂<span>1</span></a></li>
		
			<li><a href="/tags/paper/">paper<span>1</span></a></li>
		
			<li><a href="/tags/classifcation/">classifcation<span>1</span></a></li>
		
			<li><a href="/tags/GAN/">GAN<span>1</span></a></li>
		
			<li><a href="/tags/classfication/">classfication<span>1</span></a></li>
		
			<li><a href="/tags/deep-learning/">deep learning<span>9</span></a></li>
		
			<li><a href="/tags/computer-vision/">computer vision<span>3</span></a></li>
		
			<li><a href="/tags/machine-learning/">machine learning<span>1</span></a></li>
		
			<li><a href="/tags/face-detection/">face detection<span>1</span></a></li>
		
			<li><a href="/tags/notes/">notes<span>7</span></a></li>
		
			<li><a href="/tags/math/">math<span>2</span></a></li>
		
			<li><a href="/tags/algorithms/">algorithms<span>2</span></a></li>
		
			<li><a href="/tags/RNN/">RNN<span>1</span></a></li>
		
			<li><a href="/tags/summary/">summary<span>2</span></a></li>
		
			<li><a href="/tags/Deep-Learning/">Deep Learning<span>1</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2018/01/18/AlexNet算法笔记/" ><i class="fa fa-file-o"></i>AlexNet算法笔记</a>
      </li>
    
      <li>
        <a href="/2018/01/18/VGGNet算法笔记/" ><i class="fa fa-file-o"></i>VGGNet算法笔记</a>
      </li>
    
      <li>
        <a href="/2018/01/05/Comic-Generation/" ><i class="fa fa-file-o"></i>Comic Generation</a>
      </li>
    
      <li>
        <a href="/2017/12/20/CS224n-assignment2/" ><i class="fa fa-file-o"></i>CS224n-assignment2</a>
      </li>
    
      <li>
        <a href="/2017/12/18/CS224n-assignment1/" ><i class="fa fa-file-o"></i>CS224n assignment1</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/mowayao" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.weibo.com/mowayao" title="My weibo account." target="_blank"]);">My Weibo</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 Mowayao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$'], ['\[','\]'] ], 
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
   </html>
