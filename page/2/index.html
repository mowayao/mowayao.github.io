<!DOCTYPE html>
<html lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="stylesheet" href="/style/style.css">
<script>
    var NlviConfig = {
        title: "Mowayao's Blog",
        author: "Mowayao",
        theme: "banderole",
        lightbox: true,
        animate: true,
        baseUrl: "/",
        search: true,
        friends: false
    }
</script>



    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">









    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="browsermode" content="application">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Mowayao's Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name= "format-detection" content="telephone=no" />
<meta name="keywords" content="nlvi, Nlvi" />


  <meta name="subtitle" content="一往无前虎山行">




  <title>
  Mowayao's Blog
  
    | 一往无前虎山行
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="progress">
  <div class="progress-inner"></div>
</div>
  
  
    <div class="tagcloud-mask"></div>  
<div class="tagcloud" id="tagcloud">
  <div class="tagcloud-inner">
    <a href="/tags/DP/" style="font-size: 14px;">DP</a> <a href="/tags/Deep-Learning/" style="font-size: 14px;">Deep Learning</a> <a href="/tags/GAN/" style="font-size: 14px;">GAN</a> <a href="/tags/Leetcode/" style="font-size: 14px;">Leetcode</a> <a href="/tags/RNN/" style="font-size: 14px;">RNN</a> <a href="/tags/algorithms/" style="font-size: 14px;">algorithms</a> <a href="/tags/classfication/" style="font-size: 14px;">classfication</a> <a href="/tags/classifcation/" style="font-size: 14px;">classifcation</a> <a href="/tags/classification/" style="font-size: 14px;">classification</a> <a href="/tags/computer-vision/" style="font-size: 14px;">computer vision</a> <a href="/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/tags/face-detection/" style="font-size: 14px;">face detection</a> <a href="/tags/fine-grained-classification/" style="font-size: 14px;">fine-grained classification</a> <a href="/tags/leetcode/" style="font-size: 14px;">leetcode</a> <a href="/tags/machine-learning/" style="font-size: 14px;">machine learning</a> <a href="/tags/math/" style="font-size: 14px;">math</a> <a href="/tags/notes/" style="font-size: 14px;">notes</a> <a href="/tags/object-detection/" style="font-size: 14px;">object detection</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/paper-notes/" style="font-size: 14px;">paper notes</a> <a href="/tags/segmentation/" style="font-size: 14px;">segmentation</a> <a href="/tags/speech/" style="font-size: 14px;">speech</a> <a href="/tags/summary/" style="font-size: 14px;">summary</a> <a href="/tags/杂/" style="font-size: 14px;">杂</a>
  </div>
</div>  
  

  <div class="container">

    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <span><a href="/">Mowayao's Blog</a></span>
    
      <span id="subtitle">一往无前虎山行</span>
    
  </div>
</div>
    <nav class="main-nav">
        
  <ul class="main-nav-list syuanpi tvIn">
  
    <li class="menu-item">
      <a href="javascript:;" id="search">
        <span>Search</span>
        
          <span class="menu-item-label">search</span>
        
      </a>
    </li>
  
  
    
      
    
    <li class="menu-item">
      <a href="/" id="article">
        <span class="base-name">Article</span>
        
          <span class="menu-item-label">article</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">Archives</span>
        
          <span class="menu-item-label">archives</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">Tags</span>
        
          <span class="menu-item-label">tags</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">About</span>
        
          <span class="menu-item-label">about</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/atom.xml" id="RSS">
        <span class="base-name">RSS</span>
        
          <span class="menu-item-label">RSS</span>
        
      </a>
    </li>  
  
  </ul>
  
</nav>
    
    
  </div>
</header>
<div class="mobile-header">
  <div class="mobile-header-body">
    <div class="mobile-header-list">
      
        
            <div class="mobile-nav-item">
                <a href="/">
                    <span>Article</span>
                    
                    
                </a>
            </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/archives">
                    <span>Archives</span>
                    
                    
                </a>
            </div>
        
      
        
          <div class="mobile-nav-item inner-cloud">
            <div class="mobile-nav-tag">
              <a href="javascript:;" id="mobile-tags">
                <span>Tags</span>
                
                
              </a>
            </div>
            <div class="mobile-nav-tagcloud">
              <div class="mobile-tagcloud-inner">
                <a href="/tags/DP/" style="font-size: 14px;">DP</a> <a href="/tags/Deep-Learning/" style="font-size: 14px;">Deep Learning</a> <a href="/tags/GAN/" style="font-size: 14px;">GAN</a> <a href="/tags/Leetcode/" style="font-size: 14px;">Leetcode</a> <a href="/tags/RNN/" style="font-size: 14px;">RNN</a> <a href="/tags/algorithms/" style="font-size: 14px;">algorithms</a> <a href="/tags/classfication/" style="font-size: 14px;">classfication</a> <a href="/tags/classifcation/" style="font-size: 14px;">classifcation</a> <a href="/tags/classification/" style="font-size: 14px;">classification</a> <a href="/tags/computer-vision/" style="font-size: 14px;">computer vision</a> <a href="/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/tags/face-detection/" style="font-size: 14px;">face detection</a> <a href="/tags/fine-grained-classification/" style="font-size: 14px;">fine-grained classification</a> <a href="/tags/leetcode/" style="font-size: 14px;">leetcode</a> <a href="/tags/machine-learning/" style="font-size: 14px;">machine learning</a> <a href="/tags/math/" style="font-size: 14px;">math</a> <a href="/tags/notes/" style="font-size: 14px;">notes</a> <a href="/tags/object-detection/" style="font-size: 14px;">object detection</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/paper-notes/" style="font-size: 14px;">paper notes</a> <a href="/tags/segmentation/" style="font-size: 14px;">segmentation</a> <a href="/tags/speech/" style="font-size: 14px;">speech</a> <a href="/tags/summary/" style="font-size: 14px;">summary</a> <a href="/tags/杂/" style="font-size: 14px;">杂</a>
              </div>
            </div>
          </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/about">
                    <span>About</span>
                    
                    
                </a>
            </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/atom.xml">
                    <span>RSS</span>
                    
                    
                </a>
            </div>
        
      
    </div>
  </div>
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <span class="header-menu-line"></span>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">Mowayao's Blog</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
</div>
    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
  

<section class="posts">
  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-21</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/04/21/FPN论文笔记/">FPN论文笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：Feature Pyramid Networks for Object Detection，<strong>CVPR</strong> 2017</p>
<p>作者：Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie</p>
<p>链接：https://arxiv.org/pdf/1612.03144.pdf</p>
<p>代码(unofficial)：https://github.com/unsky/FPN</p>
<ul>
<li>feature pyramid在物体识别中比较常用，作者将其应用到object detection中</li>
<li>不同于SSD，作者使用top-down的结构将来自更高pyramid level的semantically stronger的feature map和higher resolution features相结合，使得detection的结构更加准确</li>
</ul>
<p>Figure 1是不同类型的feature map的使用方法，作者使用的是(d)的结构，SSD使用的是(c)的结构</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqnpdczxhuj30w20usgvn.jpg" alt=""></p>
<h5>Feature Pyramid Networks</h5>
<p>Figure 3是top-down pathway的示意结构，现将来自higher pyramid的feature maps升采样，再将该层的feature map做1x1的conv，减少channel dimension（固定为256d），再用element-wise addtion做merge，然后再用3x3的conv去消除上采样aliasing的影响。在C5上（最高层），先用1x1 conv产生粗糙的特征图。</p>
<p>${C2, C3, C4, C5}$层对应的融合特征层为${P2, P3, P4, P5}$</p>
<p>通过这样的操作来加强特征，即<strong>保留空间信息并增强语义信息</strong>。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqnpkyj02ej30tk0jcn00.jpg" alt=""></p>
<h5>Applications</h5>
<p><strong>Feature Pyramid Networks for RPN</strong></p>
<p>RPN的参数配置和Faster R-CNN类似，作者提到了一点就是在feature pyramid的detection head参数是可以共享的，结果和不共享接近，这说明了<strong>不同level的pyramid的semantic level是相似的</strong>！</p>
<p><strong>Feature Pyramid Networks for Fast R-CNN</strong></p>
<p>这里的重点是ROI的分配：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqnqq1o4irj30m2034dg3.jpg" alt=""></p>
<p>w,h是ROI的宽和高，$k_0$=4，表示的是大小为224x224的ROI的target level，利用这个式子进行转换，分配到相应的pyramid level。</p>
<h5>Experiments</h5>
<p><strong>Ablation Study</strong></p>
<p>Table 1，2，3是ablation study的表格</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqnqwou2qwj315g0uc7ef.jpg" alt=""></p>
<p>Table 4是单模型在COCO detection benchmark上的表现，FPN取得SOTA的表现</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqnr7se6qpj31kw0iatgv.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-21</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/04/21/RON论文笔记/">RON论文笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：RON: Reverse Connection with Objectness Prior Networks for Object Detection，<strong>CVPR</strong> 2017</p>
<p>作者：Tao Kong，Fuchun Sun，Anbang Yao，Huaping Liu，Ming Lu，Yurong Chen</p>
<p>链接：https://arxiv.org/pdf/1707.01691.pdf</p>
<p>代码：https://github.com/taokong/RON</p>
<p><strong>objective：</strong></p>
<ul>
<li>bridge the gap between the region-based and region-free methodologies</li>
</ul>
<p><strong>solution</strong>：</p>
<ul>
<li>Multi-scale object localization，利用多个scale的feature map做detection
<ul>
<li>建立reverse connection，为前面的层提供highly semantic的information</li>
</ul>
</li>
<li>Negative space mining
<ul>
<li>通过建立objectness prior减少objects的搜索空间</li>
</ul>
</li>
</ul>
<p>384x384的输入，PASCAL VOC 2007 81.3% mAP，PASCAL VOC 2012 80.7% mAP，COCO 27.4%</p>
<p>效率较高，1.5G的 GPU内存，forward的速度是15FPS</p>
<p>Figure 2是RON的网络模型，有4个scales feature map做detection。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqk5dydd50j30t20tcqde.jpg" alt=""></p>
<h5>Architecture</h5>
<p>网络以VGG 16为backbone，将FC6和FC7改造为conv layer，在FC7处降采样。</p>
<p>所以每个用于detection的feature map大小为：1/8 (conv 4 3), 1/16 (conv 5 3), 1/32 (conv 6) and 1/64 (conv 7)</p>
<p><strong>Reverse Connection</strong></p>
<p>reverse connection的结构见Figure 3，将后一层的输出经过deconv做upsampling，deconv的通道是512，当前层经过conv，再用summation的方式做merge。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqk5exvqeej30io0g6my5.jpg" alt=""></p>
<p><strong>Reference Boxes</strong></p>
<p>不同层的网络对应不同的receptive field，所以需要设计对应层的ref box的scale和ratio，下面是scale的设计：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqmjznn3cij30rc02umxf.jpg" alt=""></p>
<p>这里$s_{min}$设为$\frac{1}{10}$,不同的对应ratios是${\frac{1}{3},\frac{1}{2}, 1, 2, 3}$</p>
<p><strong>Objectness Prior</strong></p>
<p>针对正负样本比例严重失调， 这里使用 Objectness Prior 来过滤大部分负样本，具体的做法就是利用conv通道的设计，因为每个cell有10个default boxes，所以通道数是10，然后就是做二分类。这里需要设定阈值，将objectness score大于$o_p$的选为样本，这里$o_p$设为0.03，可以过滤掉大部分的样本。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqmmemlca0j30s00fwduh.jpg" alt=""></p>
<p><strong>Detection and Bounding Box Regression</strong></p>
<p>Figure 5是detection bbox reg head的结构，可以发现在分类的module中，作者加入了两个inception module来提高分类的精度</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqk5fg7sxbj30tg0dimz5.jpg" alt=""></p>
<h5>Training</h5>
<p><strong>Loss</strong></p>
<p>相似的，也是multi-task loss</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqmmnkgq8jj30so04igm3.jpg" alt=""></p>
<h5>Experiments</h5>
<p>Table 1-3是模型在PASCAL VOC 2007, 2012和COCO上的结果。</p>
<ul>
<li>可以发现RON对小物体的检测提升比较明显，例如boat和bottle。</li>
</ul>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqmmqfxmthj31ko0dqq9a.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqmmraplo7j31kw0g2gt8.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqmmvqkb0xj30tg0hydjk.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-18</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/04/18/OHEM算法笔记/">OHEM算法笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：Training Region-based Object Detectors with Online Hard Example Mining，<strong>CVPR</strong> 2016</p>
<p>作者：Abhinav Shrivastava, Abhinav Gupta, Ross Girshick</p>
<p>链接：https://arxiv.org/pdf/1604.03540.pdf</p>
<p>代码：https://github.com/abhi2610/ohem</p>
<p>Hard example mining是机器学习模型训练经常会用的trick，顾名思义，就是sample目前对于模型比较难的example进行“强化”学习。在CNN中对于patch选择根据策略的不同，主要有sliding window和proposal。大部分的情况是根据loss来判断是否是hard，只是作为训练的一个trick。文章针对Fast R-CNN，提出online hard example mining的算法对其进行优化。在VOC2007, 2012都取得了SOTA，mAP分别是78.9%，76.3%。</p>
<h5>Fast R-CNN</h5>
<p>FRCNN中，proposal的选择由它与gt的overlap决定，确定一个proposal为背景的阈值范围是：$[bg_{lo},0.5]$，这个范围的假设是这样的proposal是hard的可能性较大。作者认为这样得到的结果很可能是次优的，因为在其他位置可能存在更hard但是infrequent的样本，OHEM算法中移除了这个阈值</p>
<h5>OHEM</h5>
<p>idea很简单，就是在forward的时候根据loss排序，然后选择loss最大的，也就是最worst的样本进行backward更新模型的参数。但是这样会存在一个问题，就是当两个ROI位置相近的时候，在feature map上对应的是同一个位置，loss是相近的，所以作者提出了对hard examples做NMS，选择B/N个ROI最backward，这里NMS的阈值为0.7。</p>
<p>接下来就是工程上的优化，ROI进行backward的时候，空间和时间的消耗较大，如果直接做backward，那些没有选中的ROI还是会做backward，所以作者提出了Figure 2的网络结构，包含两个一样的的ROI network，其中一个是immutable的，用于计算forward的loss，只有在forward的时候分配内存，然后hard RoI sampling module使用刚才所说的方法进行采样，作为输入传到第二个ROI network，进行forward和backward，然后累积gradient，再backward。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqgutzexflj31kw0v41kx.jpg" alt=""></p>
<h5>Experiments</h5>
<p>Table 3和Table 4是模型在VOC 2007和VOC 2012的表现，都是SOTA，对FRCN提升明显。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqh3cawf16j31600radsm.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-11</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/04/11/SSD笔记/">SSD笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：SSD: Single Shot MultiBox Detector，<strong>ECCV</strong> 2016</p>
<p>作者：Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg</p>
<p>链接：https://arxiv.org/pdf/1512.02325.pdf</p>
<p>代码：https://github.com/weiliu89/caffe/tree/ssd</p>
<p>gluon实现：https://github.com/mowayao/gluon_SSD</p>
<ul>
<li>将bbox的输出空间离散化到一些默认的priors，也就是默认设定的anchors</li>
<li>不同Faster R-CNN的two stage的方式，SSD采用的是one shot的方法，精度和faster r-cnn接近，但是效率更高</li>
<li>将多尺度的feature maps应用到detection中，针对不同尺度的feature map，采用不同scale和ratio</li>
</ul>
<p>#####The Single Shot Detector (SSD)</p>
<p>将feature map分成nxn的cells，每个cell可以预测固定数量的box的conf和offset</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fqfue2u6o9j316a0taww0.jpg" alt=""></p>
<p><strong>Model</strong></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqgroawjivj315i0r0dnf.jpg" alt=""></p>
<p>模型是基于pretrained的VGG-16进行修改和fine-tune</p>
<ul>
<li>Multi-scale feature map for detection: 在base net后加了conv feature layers用来做detection，这些layer使得feature maps的size逐渐降低，这样就可以做多尺度的预测</li>
<li>Convolutional predictors for detection：在添加的conv feature layer后加入输出层，即conv predictor，生成固定大小的输出，对于mxn，p个channel的feature map，使用3x3xp的卷积核，预测conf和loc。</li>
<li>Default boxes and aspect ratios：每个cell预测k个boxes，以及类别的数量是c，那么最后的输出channel则是(c+4)*k</li>
</ul>
<p><strong>Training</strong></p>
<ul>
<li>Matching strategy: 需要将default boxes做划分，将与gt与box的jaccard overlap大于0.5的定为gt，其他定为background</li>
<li>Training objective:</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqgqb60zbdj30wo040q3a.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqgqb6a4gij30zg09wmyu.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqgqb74a9zj315c04mdgq.jpg" alt=""></p>
<ul>
<li>Choosing scales and aspect ratios for default boxes: 为了针对不同scale的object，scale被定义为：</li>
</ul>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqgrc1164dj30wa03q3yw.jpg" alt=""></p>
<p>$s_{min}=0.2, s_{max}=0.9,$ratios $a_r\in {1,2,3,\frac{1}{2}, \frac{1}{3}}$，$w_k=s_k\sqrt a_r, h_k=s_k/\sqrt a_r$。另外对于 ratio = 1 的情况，再指定 scale 为$，s_k=\sqrt {s_ks_{k+1}}$也就是总共有 6 种不同的 default box</p>
<ul>
<li>Hard negative mining: 在做完matching后，大部分的box都是背景，所以需要做采样，选择loss最高的几个，并且保持3:1的比例</li>
<li>Data augmentation：
<ul>
<li>使用全图作为输入，</li>
<li>使用IOU和目标物体为0.1, 0.3，0.5, 0.7, 0.9的patch （这些 patch 在原图的大小的 $[0.1,1]$ 之间， 相应的宽高比在$[1/2,2]$之间）</li>
<li>随机采取一个patch</li>
</ul>
</li>
</ul>
<h5>Experimental Results</h5>
<p>Table 1是SSD在PASCAL VOC2007上的表现，可以发现SSD512的表现优于Faster R-CNN，SSD300兼顾性能和效率</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqgqghdz3lj31600k2wo1.jpg" alt=""></p>
<p>Table 4是SSD在 PASCAL VOC2012上的表现，可以发现SSD512取得了SOTA的表现</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqgrepc7k2j316w0es7b4.jpg" alt=""></p>
<p>Table 5是在COCO上的表现，同样是SOTA</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqgreqfglbj316i0dwtd9.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-07</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/04/07/YOLO&YOLOv2笔记/">YOLO&YOLOv2笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <h3>YOLO</h3>
<p>论文：You Only Look Once: Unified, Real-Time Object Detection，<strong>CVPR</strong> 2016</p>
<p>作者：Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi</p>
<p>链接：https://arxiv.org/pdf/1506.02640.pdf</p>
<p>代码：https://pjreddie.com/darknet/yolo/</p>
<ul>
<li>将detection的分类问题转化为回归问题</li>
<li>在VOC 2007上，达到45FPS，mAP 63.4%，Fast YOLO可以达到155FPS，mAP 52.7%</li>
<li>将整张图划分为7x7的网格，每个格子预测置信度score和坐标位置，5个输出</li>
<li>没有利用proposal（不同于SSD和fast r-cnn），对小目标不是很友好</li>
<li>使用全局的context信息，背景错误较少</li>
</ul>
<h5>Unified Detection</h5>
<p>首先将输入图像分成SxS块，每个grid cell预测B个bounding box和confidence scores，以及预测C类。这样网络最后一层的输出是SxSx(Bx5+C)的tensor。如果一个object的中心落在某个grid，那么这个grid就需要负责预测这个object。文中的参数S=7,B=2,C=20。所以最后的输出的tensor大小为7x7x30。</p>
<p>confidence score的计算则是:$P(object)\times IOU_{pred}^{gt}$，第一项是表示是否落入grid，第二项则是预测的框和实际的框的IOU值。</p>
<p>loss function的设计为：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq7loi7sgqj30ro0hotao.jpg" alt=""></p>
<p>可以发现作者的一个trick，就是对于宽和长，做了开根号的处理，这样做的目的是当物体很小的时候相对于大物体有同等的差值，可以获得更大的loss。</p>
<p>网络结构Figure 3所示，启发于GoogLeNet，但是没有使用inception，而是使用1x1的conv，共有24个卷积层加2个全连接层，先在ImageNet上预训练。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq7l4n5bd5j31kw0pjtfl.jpg" alt=""></p>
<p><strong>Limitations of YOLO</strong></p>
<ol>
<li>对bounding box的预测做了较强的空间假设，每个grid只能预测两个boxes，并且只属于一类。这样就对小物体的预测非常不友好，特别是当一些小物体成群出现的时候。</li>
<li>在测试的时候，如果objects有不同的或者不寻常的比例的时候，泛化性会比较差</li>
<li>loss function的设计还需要再优化</li>
</ol>
<h5>Experiments</h5>
<p>Table 1是各个object detection实时系统的比较，可以发现Fast YOLO的速度非常高，同时保持较高的mAP。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fq7mi5up9aj30v20su0yx.jpg" alt=""></p>
<p>Figure 4是Fast R-CNN和YOLO的错误组成对比，可以发现YOLO对于背景的预测较好，因为利用了全局的context，但是loc的错误较大，这和loss function的设计不无关系。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq7mknid1jj30t60mun1s.jpg" alt=""></p>
<h3>YOLO v2</h3>
<p>论文：YOLO9000: Better, Faster, Stronger Joseph，<strong>CVPR</strong> 2017</p>
<p>作者：Joseph Redmon, Ali Farhadi</p>
<p>链接：http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf</p>
<p>代码：http://pjreddie.com/yolo9000/</p>
<h5>Better</h5>
<ul>
<li>
<p>Batch Normalization:在每个conv后加入bn，取消dropout，mAP提高了2%</p>
</li>
<li>
<p>High Resolution Classifie: pretrain的时候就把图像分辨率调高，从224x224变成448x448，提高了4%</p>
</li>
<li>
<p>Convolutional With Anchor Boxes: 吸收Faster R-CNN中RPN的思想，去掉全连接层，加入anchor boxes，这样导致acc略有下降，但是召回率提高较多</p>
</li>
<li>
<p>Dimension Clusters: 通过k-means选择较好的priors，也就是anchor，选择k=5作为复杂度和高召回率的折中，将distance metric定义为：</p>
</li>
<li>
<p>$$
d(box, centroid) = 1-IOU(box, centroid)
$$</p>
<p>在k=5的前提下，召回率和使用9个anchor box的相近</p>
</li>
<li>
<p>Direct location prediction: 对坐标进行转换，和fast r-cnn等类似，提升5%，见Figure 3</p>
</li>
<li>
<p>Fine-Grained Features: 为了得到奇数大小的feature maps，将图像分辨率调整到416，得到奇数大小的好处是改点就是这个区域的中心，然后调整降采样的步长为32，这样就是416/32=13，参考SSD和faster r-cnn利用多尺度的feature maps，作者提出passthrough layer，将high resolution features和low resolution features结合，对于high resolution的feature maps，将相邻的特征分配到不同的channels，例如26x26x512变换成13x13x2048，再和原来的特征图相连接，相当于把feature maps做深度的扩充，这样有1%的提升</p>
</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq7upwxukoj30t60teadt.jpg" alt=""></p>
<ul>
<li>Multi-Scale Training: 每隔10个batches，随机选择一个scale，sclales有：320，352…,608。Table 3是不同尺度的结果，可以发现 544的最优！mAP达到78.6。在288x288的scale上，可以得到90FPS，同时mAP和Fast R-CNN一样</li>
</ul>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fq7vdy6vauj30ty0skjya.jpg" alt=""></p>
<p>Table 2是ablation studies的结果</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fq7vx9xa1aj31kw0nwjy5.jpg" alt=""></p>
<h5>Faster</h5>
<p>提出darknet-19，效率相较于VGG 16有了较大的提升。</p>
<p>VGG-16对于224x224的图像做一次forward，卷积层的浮点数计算为30.69 billion。而darknet-19的卷积层浮点数计算为8.52 billion，但是在分类的准确性上略有降低。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fq7x1hlp1vj30jm0pujve.jpg" alt=""></p>
<h5>Stronger</h5>
<p>这里作者提出了一个非常fancy的idea，WordNet，解决不同数据集之间label mutually exclusive的问题。WordNet通过构建hierarchical tree来简化问题，并且选择最短的到达root的路径，Figure 6为图示。利用有向图对条件概率计算进行优化，分层地计算概率，然后做出预测。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq7wy9z9kxj30pc0wkqaw.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-04-02</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/04/02/Faster-R-CNN笔记/">Faster R-CNN笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks，TPAMI 2016</p>
<p>作者：Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun</p>
<p>链接：https://arxiv.org/pdf/1506.01497.pdf</p>
<p>代码：https://github.com/rbgirshick/py-faster-rcnn</p>
<ul>
<li>在Fast R-CNN的基础上进行优化，将Region Proposal Network（RPN）取代Selective Search，可以更加有效地检测object candidates，与Fast R-CNN共享参数，RPN的任务是预测物体的bbox和objectness score</li>
<li>在RPN的基础上，提出anchor的概念，每个anchor可以代表不同的scale和ratio，从而达到translation-invariant的效果</li>
</ul>
<p>Figure 2的Faster R-CNN的结构图，主要要经过3个步骤：</p>
<ol>
<li>输入图像进入CNNs，得到相应的feature maps</li>
<li>将feature maps输入到RPN中，得到region proposals</li>
<li>将region proposals输入到Fast R-CNN中，得到detection的结果</li>
</ol>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fpyjflds95j30ny0tcq81.jpg" alt=""></p>
<h5>Architecture</h5>
<p><strong>RPN</strong></p>
<p>RPN的输入为任意形状的图像，输出是一堆proposals的坐标，以及对应的confidence score。为了生成一堆region proposals，对于nxn的feature maps来说，作者通过sliding window（其实也就是3x3的conv）将其映射到固定维度的特征空间，然后再对得到的特征向量做分类和回归（将全连接改造成1x1的卷积，可以接受任何大小的输入）。</p>
<p><strong>anchors</strong></p>
<p>对于每个滑动窗口来说，假设该窗口有k个对应的proposals。这样reg layer就有4k的输出，cls layer有2k个输出。Figure 3就是anchor的示例。anchor可以解释为sliding window的中心点，通过假设这个中心点来自不同原始区域池化得到，所以可以根据这个中心点(anchor)逆推得到这些区域坐标和种类。假设现在有3中scales和3种ratios，以及feature maps的大小是WxH，那么总共有9xWxH个anchors，也可以将其理解成先验的bounding box。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fpynk5f034j31d40k8nle.jpg" alt=""></p>
<h5>Experiments</h5>
<p>首先是坐标空间的转换，将其转到相对空间，这在R-CNN中已经介绍过。其实是Multi-task的训练：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fpyo96peu1j30ke064t97.jpg" alt=""></p>
<p><strong>RPN训练</strong></p>
<p>RPN就直接用end-to-end的方法进行训练，对于每张图像，采样256个anchors，正负比例为1：1，如果正样本不够则用负样本补足。</p>
<p><strong>Fast R-CNN训练</strong></p>
<p>两个网络的训练需要比较多的trick，作者采用4-Step Alternating Training:</p>
<ol>
<li>对网络初始化（pretrained model），end-to-end地训练RPN</li>
<li>用RPN生成propals训练Fast R-CNN</li>
<li>将共享的卷积层fix住，训练RPN相关的层</li>
<li>将共享的卷积层fix住，训练Fast R-CNN相关的层</li>
</ol>
<p>Table 6和Table 7是Faster R-CNN在PASCAL VOC 2007和PASCAL VOC 2012两个数据集上的结果们也是RPN和SS的比较，可以看出RPN要好于SS</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq0p3j4jrnj31580ictfx.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-03-22</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/03/22/Fast-R-CNN笔记/">Fast R-CNN笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：Fast R-CNN，<strong>ICCV</strong> 2015</p>
<p>作者：Ross Girshick</p>
<p>链接：https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf</p>
<p>代码：https://github.com/rbgirshick/fast-rcnn</p>
<ul>
<li>
<p>简化训练过程，实现end-to-end的训练方式</p>
<ul>
<li>将SVM分类器换成softmax，整合到网络中，鼓励类间竞争</li>
<li>将bounding box regression整合到网络中</li>
</ul>
</li>
<li>
<p>在Fast-RCNN中，region proposal的计算都是share的，避免重复计算</p>
</li>
<li>
<p>提出ROI pooling layer使得每个region proposal可以得到确定长度的特征向量</p>
</li>
<li>
<p>探索了一些训练的tricks，可以参考！</p>
<p>​</p>
<p>Figure 1是Fast R-CNN的图示</p>
</li>
</ul>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/FuPCSWv0EUA2laIjMRDcJTU9oA0F" alt=""></p>
<h5>Architecture</h5>
<p>下图是R-CNN和Fast R-CNN的比较</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/FurXilgjM6ixsxoU1HRAgilULtUt" alt=""></p>
<p>回顾一下R-CNN：</p>
<ol>
<li>用selective search提取region proposals</li>
<li>将region warp到固定大小，用CNN提取特征，得到一定长度的特征向量</li>
<li>对于提取到的特征，放到若干个SVM，得到各个类别的概率，再用简单的线性回归模型做 bounding box regression（就是对候选区域进行微调）</li>
<li>利用NMS优化结果</li>
</ol>
<p>可以发现：</p>
<ol>
<li>R-CNN存在着大量的重复计算，即对每个region propasal使用CNN提取特征，如果有n个patch就需要网络n次的forward</li>
<li>需要将region warp到固定大小，造成物体变形分辨率降低影响分类结果</li>
<li>没有end-to-end。。。</li>
</ol>
<p><strong>改进</strong></p>
<p>首先，R-CNN重复大量计算，需要针对这点进行优化，将卷积计算的结果重复利用，减少不必要的重复计算，先用CNN对全图提取特征，得到feature maps，然后用selective search对原图提取region proposals，根据缩放比例，在feature maps上找到相应的区域，再对这个区域进行分类和bounding box的回归。这样我们只需要做一次卷积就可以了。但是这又面临一个问题，这些区域的特征长度是不固定的，所以，收到SPP-Net的启发，提出ROI Pooling layer，使得feature maps输入能够得到固定长度的特征向量。</p>
<p>其次，Fast R-CNN将softmax分类器替换SVM，同时将bounding box regression融合到网络中，实现end-to-end的训练，其实就是通过multi-task的思想，实现对CNN的fine tune。</p>
<p><strong>ROI POOLING</strong></p>
<p>为了能够使得每个region得到相同固定大小的特征，我们需要调整它的维度使得它能够适应全连接层。所以论文提出了ROI POOLING这个方法，下面介绍一下ROI POOLING的工作原理：</p>
<p>假设输入的feature maps的维度是CxHxW，C,H,W分别表示feature maps的深度，高，宽。因为C是固定的而H和W是不固定的，所以论文采用了一种和SPPNet近似的方法。加入我们想要得到固定大小Cxhxw的输出，采用动态pooling的方法，将pooling的kernel size设置成(H/h, W/w)，步长也是一样。这样就可以得到固定长度的feature maps了。</p>
<p><strong>Scale Invariance</strong></p>
<p>为了实现object detector的scale invariance，作者探索了两者方法：</p>
<ol>
<li>直接暴力地resize</li>
<li>使用图像金字塔</li>
</ol>
<p><strong>Truncated SVD</strong></p>
<p>在训练的时候，因为ROIs数量不多，所以大部分的时间花在卷积运算上，而在测试的时候，需要计算每个ROIs的后验概率，有一半的时间花在全连接上，所以文章提出truncated SVD优化计算，将两个全连接取代一个全连接。
$$
W \approx U\Sigma_tV^T
$$
U是一个$u\times t$的矩阵，$\Sigma_t$是$t\times t$的矩阵，V是$v\times t$的矩阵，所以参数从$uv$减少到了$t(v+u)$，如果t要远小于$\min(u,v)$。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/Frc5IQiqhfIQW8Z39bNmc8BF3-vM" alt=""></p>
<p><strong>Mini-batch sampling</strong></p>
<p>在训练的时候，每次mini-batch中，从中采样两张图像，每张图像采样64个ROIs，这样每个batch的大小就是128，正负样本比例为1：3。</p>
<h5>Experiments</h5>
<p>Table 1-3是Fast R-CNN在VOC 2007, 2010, 2012三个数据集上的表现，都是SOAT。</p>
<p>Table 4是Fast-RCNN,RCNN以及SPPnet在training和testing效率上的比较，可以发现效率提升非常明显。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/FozRMPEXdJimmQmIloMArAEK4bjz" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/FkHrR24XRqVg4etaxtFh-t2IF5G3" alt=""></p>
<h5>Design evaluation</h5>
<ul>
<li>Multi-task训练可以提高分类结果</li>
<li>Multi-scale detection可以帮助提高结果，但是提升不大，折中之下，选了single scale</li>
<li>training data越多越好，数据增强，将VOC2012的数据加到VOC2007中，提升明显！</li>
<li>在FRCN中，softmax要好于SVM，鼓励类间竞争</li>
<li>并不是proposals越多越好，需要取个合适的数量</li>
</ul>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-03-20</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/03/20/SPPNet笔记/">SPPNet笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition，<strong>TPAMI</strong> 2015</p>
<p>作者：Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun</p>
<p>链接：https://arxiv.org/pdf/1406.4729.pdf</p>
<p>代码：https://github.com/ShaoqingRen/SPP_net</p>
<ul>
<li>提出spatial pyramid pooling layer，使得网络能够接受任意大小的输入，同时利用multi-level的spatial bins做hierarchy information aggregation，提高模型的鲁棒性</li>
<li>对image classification和object detection（针对RCNN的优化）都可以提升模型的表现</li>
<li>在ILSVRC 2014 object detection中排名第2，image classification中排名第3</li>
</ul>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/56717690.jpg" alt=""></p>
<p>在RCNN中，我们提取各个region proposals，然后将其warp到指定的大小，再用CNN提特征，这样的问题是图像object经过warp以后，会变形严重并且影响分辨率，从而影响分类的结果，所以作者提出SPP layer，使得对于任何大小的输入图像，神经网络都可以输出固定长度的特征，同时提高分类的鲁棒性。</p>
<h5>Architecture</h5>
<p>见Figure 3，将卷积层的输出经过SPP layer以后，利用max pooling分成3种的spatial bins，第一种是4x4，第二种是2x2，第三种是1x1(global pooling)。在做完pooling以后，我们可以得到16x256,4x256,1x256这三种维度的特征，256是输入的feature map的深度，拼接以后得到21x256长度的特征，其实可以将SPP layer看成是特征层面的re-scaled。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/71347165.jpg" alt=""></p>
<p>例如输入的feature map的大小是axa，需要产生3x3，2x2，1x1的特征向量，那么pooling的window size是ceil(a/n)，步长是floor(a/n)。将这些特征拼接得到固定长度的特征向量，再连接全连接层进行分类。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/44112090.jpg" alt=""></p>
<h5>Training Strategy</h5>
<p><strong>Multi-size Training</strong></p>
<p>在分类任务的模型训练过程中，采用多个size的输入可以提高模型的结果。对于有两个不同大小输入的训练策略，使用两个固定大小（框架所限）的相同参数的网络进行交替训练。</p>
<p><strong>Full-image Representation</strong></p>
<p>在分类任务的模型训练过程中，将图像resize到min(w,h)=256，保持长宽比不变。</p>
<h5>Experiments</h5>
<p><strong>Image Classifications</strong></p>
<p>Table 1是三种base network的结构。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/11136760.jpg" alt=""></p>
<p>在分类任务中，作者通过实验证明，SPP layer，multi-size training，能够提高分类的精度（见Table 2）。SPP layer能够提高精度是因为利用spatial pyramid pooling可以提高鲁棒性。multi-size training中，有三种策略，一种是单个大小，第二种是180和224，还有一种是从[180,224]随机选择其中的一个size，实验证明，第二种精度最高。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/84456005.jpg" alt=""></p>
<p>从Table 3看，full-image representation相比较于central crop，可以提高准确率。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/81983088.jpg" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/64521005.jpg" alt=""></p>
<p><strong>Object Detection</strong></p>
<p>下图是R-CNN和SPP-Net的对比，R-CNN的缺点是计算量大，包括了大量的重复计算。</p>
<p><img src="http://img.blog.csdn.net/20170617102150673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdjFfdml2aWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p>其具体步骤和R-CNN类似：</p>
<ol>
<li>通过selective search得到2000个候选窗口</li>
<li>将整张图像输入到CNN中，完成特征的提取，然后在feature map上找到候选框的区域，在对候选框进行spaital pyramid pooling，得到定长的特征向量,这样做可以大大提高效率。</li>
<li>采用SVM模型，对物体进行分类。</li>
</ol>
<p>从Table 9看出，SPPNet相较于R-CNN计算效率有了较高的提升，同时准确率也有提高。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/35453761.jpg" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/27949731.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-03-13</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/03/13/RCNN笔记/">RCNN笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文：Rich feature hierarchies for accurate object detection and semantic segmentation，CVPR 2014</p>
<p>作者：Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik</p>
<p>链接：https://arxiv.org/pdf/1311.2524.pdf</p>
<p>代码：https://github.com/rbgirshick/rcnn</p>
<h5>Idea</h5>
<ul>
<li>之前的object detection算法都是用一些手工特征，例如SIFT或者HOG等，R-CNN使用pretrained的CNN提取region的特征，然后再用SVM分类做finetune（bridging the gap between image classification and object detection）</li>
<li>尝试不同层的feature作为分类的特征</li>
</ul>
<h5>Architecture</h5>
<p>R-CNN的模型见Figure 1，主要包括3个部分：</p>
<ol>
<li>提取region proposals</li>
<li>用CNN提取特征，得到一定长度的特征向量</li>
<li>对于提取到的特征，喂到若干个SVM，得到各个类别的概率</li>
<li>利用NMS优化结果</li>
</ol>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fpbdwp017oj30tq0mq0wc.jpg" alt=""></p>
<p><strong>Region proposals</strong></p>
<p>对于region proposals的提取，主要利用selective search这个启发式搜索方法。</p>
<p>selective search的步骤为：</p>
<ul>
<li>使用<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph Based Image Segmentation</a>中的方法来得到region</li>
<li>得到所有region之间两两的相似度</li>
<li>合并最像的两个region</li>
<li>重新计算新合并region与其他region的相似度</li>
<li>重复上述过程直到整张图片都聚合成一个大的region</li>
<li>使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果</li>
</ul>
<hr>
<p><strong>Feature extraction</strong></p>
<p>使用AlexNet提取特征，AlexNet现在ILSVRC 2012(IMAGENET)上做pre train。将图像的region patch warp到227x227，然后喂到网络，得到特征。</p>
<p><strong>Object category classifier</strong></p>
<p>对于每个类别，训练各自的SVM，这样就可以得到每个类的score，训练的时候将gt视为正样本，与gt的IoU &lt; 0.3视为负样本。因为负样本往往远远大于正样本，所以需要做hard negative mining，控制合理的比例，一般设置为1：3。</p>
<p><strong>Bounding box regression</strong></p>
<p>输入是n对${(P^i,G^i)}i=1,…,N$，其中$P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$，对应bounding box的中心，长宽，$G^i=(G_x,G_y,G_w,G_h)$。将P进行映射（前两者是平移，后两个是缩放）：
$$
\hat{G}_x = P_wd_x(P)+P_x \ \hat{G}_y = P_hd_y(P)+P_y \ \hat{G}<em>x = P_w\exp (d_w(P))\ \hat{G}<em>h = P_h\exp(d_h(P)) \d</em>\star(P)=w</em>\star^T\phi_5(P)
$$
训练的时候对gt的四个坐标进行转换（上面的逆运算，使得$t_x,t_y$）：
$$
t_x = (G_x-P_x)/P_w \ t_y =(G_y −P_y)/P_h \ t_w = \log(G_w/P_w)\t_h = \log(G_h/P_h).
$$
经过这样的变换，$t_x,t_y$为需要学习的平移量，$t_w, t_h$为需要学习的缩放量。</p>
<p>其中，$G$表示的是ground truth，P表示的训练样本，我们的任务是求解$W_\star$，其实就是通过梯度下降或者最小二乘法求解ridge regression问题，:
$$
w_\star =  \arg\min_{\hat{w}<em>\star}(t^i</em>\star − \hat{w}^T_\star \phi_5(P^i))^2+\lambda ||\hat{w}_\star||^2
$$
其中$\phi_5(P^i)$表示的是第5个pooling的输出。需要注意的是，正则系数$\lambda=1000$，而且当G和P相差很大的时候，效果会不好，所以需要设置IoU阈值，将其设为0.6。</p>
<h5>Experiments</h5>
<p>Table 1是模型在VOC 2012 test上的表现，可以看出R-CNN BB远超其他baseline，证明了模型的性能。</p>
<p>Table 2是模型在VOC 2007 test上的表现，可以在上面做一些ablation study：</p>
<ul>
<li>对于不finetune的模型，第一行到第三行，可以发现fc6的结果最好，这样就可以移除fc7的参数，简化模型。</li>
<li>对于finetune的模型，第四行到第六行，可以发现fc7的结果最好，经过网络的同层比较，可以发现finetune可以大大提升模型的performance。</li>
<li>通过第七行与第六行比较，可以看出bounding box regression可以较为显著地提升模型性能。</li>
<li>跟其他手工特征相比，CNN提取的特征具有更强的表达能力。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fpbidj661zj31kw0eeq6d.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fpbi5y3ykqj31kw0k9wjg.jpg" alt=""></p>
<p>​</p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
    
  
  <article class="
  post
  
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2018-03-06</span>
          
            
              <span class="post-category"><a href="/categories/algorithms/">algorithms</a></span>
            
          
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          <a href="/2018/03/06/ResNeXt笔记/">ResNeXt笔记</a>
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        
          <p>论文 :Aggregated Residual Transformations for Deep Neural Networks, CVPR 2017</p>
<p>链接：https://arxiv.org/pdf/1611.05431.pdf</p>
<p>作者:  Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He</p>
<p>源码：https://github.com/facebookresearch/ResNeXt</p>
<h5>Idea</h5>
<p>提出ResNeXt网络，基于ResNet和GoogleNet。</p>
<p>提出cardinality的概念，对于每个conv block，对输入做多种不同的（结构相同，参数不同）transformations，然后再aggregation，而transformations的数量就是cardinality。</p>
<p>结构启发于inception module，同样是split-transform-merge的策略，这里的merge和transform都和inception不一样。split-transform-merge的策略主要是<strong><em>通过在一个尽可能小的计算复杂度的前提下，提高模型的表达能力</em></strong>（approach the representational power of large and dense layers, but at a considerably lower computational complexity）！ Inception的问题在于每个block都需要定制，导致了模型的灵活性较差，这些block的定制化设计相当于引入了一堆的超参，大量的超参对模型来讲无疑是不利的。而这篇文章就是基于这点进行改进！</p>
<ul>
<li>模块化</li>
<li>超参少</li>
<li>表达能力强</li>
</ul>
<h5>Architecture</h5>
<p>首先，总结了两点block设计的原则：</p>
<ol>
<li>对于相同大小的feature map，block的超参要相同，也就是conv的filter参数要一样。【if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes)】</li>
<li>如果空间维度减半，那么feature maps的数量要加倍。【each time when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2】</li>
</ol>
<p>Figure 1是ResNet和ResNeXt的block结构比较，重点看ResNeXt，可以看到这里将256维的输入做32个不同的transformations，每个transformation先通过1x1的conv做降维(information embedding)，然后再经过几个conv，最后将这32个输出做aggregation。</p>
<p>将其公式化:
$$
F(x) = \sum_{i=1}^C \tau_i(x)
$$
$\tau_i$的作用就是将x投影到低维空间（embeeding），然后做transform。C就是做transformation的数量，也就是cardinaty。aggregation后，再用short connection做identity mapping。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp3f3tppr3j30te0i0jur.jpg" alt=""></p>
<p>Figure 3是异构的三种结构，（a）是初始版本，（b）concat，（c）group conv，这三者可以说是等价的。那么在实现上，可以利用group conv，更加方便！</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fp4d6cqzx8j31cq0i2q8m.jpg" alt=""></p>
<p>​	Table 1是ResNet-50和ResNeXt-50的参数对比</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fp4fhovd49j30nw0uaq8x.jpg" alt=""></p>
<h5>Experiment</h5>
<p>Table 3是各个版本的ResNeXt以及ResNet在ImageNet-1K数据集上的表现，可以发现随着Cardinaty增加，模型总体上是变好的，体现在top-1 error逐渐在降低。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fp4fegkbv6j30oc0jqn0j.jpg" alt=""></p>
<p>Table 6是ResNet和ResNeXt在ImageNet-5K上的比较，可以可发现后者都要好于前者。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fp4fkycmlxj30pi0eotbv.jpg" alt=""></p>
<p>Table 5则是各个state-of-the-art的模型在ImageNet-1K数据集上表现，可以发现模型达到了最好的结果。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fp4fmo7mobj30ow0gaacv.jpg" alt=""></p>
<p>从Table 7可以发现，相同的参数，ResNeXt比Wide ResNet表现更好。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fp4fojev6aj30pc09sgn0.jpg" alt=""></p>

        
      
    
    </div>
    
  </article>
  
   
    
  

  
</section>
  
  <nav class="pagination">
      <a class="extend prev" rel="prev" href="/">< Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/">Next ></a>
  </nav>


        </div>
      </main>

      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
        
        
        
        
            <a href="https://www.zhihu.com/people/https://www.zhihu.com/people/yao-ze-ping" class="iconfont icon-zhihu" title="zhihu"></a>
        
        
        
        
    
        
            <a href="https://github.com/https://github.com/mowayao" class="iconfont icon-github" title="github"></a>
        
        
        
        
        
        
        
    
</div>
    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2018 ~ 2018</span>
        <span>❤</span>
        <span>Mowayao</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
    <div class="visit_count">
        <i class="iconfont icon-visit"></i>
        <span id="busuanzi_value_site_uv"></span>
        <i class="iconfont icon-people"></i>
        <span id="busuanzi_value_site_pv"></span>
    </div>
    
</div>
    </div>
  </div>
</footer>
    </div>
  </div>
  <script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


    <script src="/script/lib/lightbox/js/lightbox.min.js"></script>



    <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>



    
        <script src="/h.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/x.js"></script>
    
        <script src="/o.js"></script>
    
        <script src="/-.js"></script>
    
        <script src="/g.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/n.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/r.js"></script>
    
        <script src="/a.js"></script>
    
        <script src="/t.js"></script>
    
        <script src="/o.js"></script>
    
        <script src="/r.js"></script>
    
        <script src="/-.js"></script>
    
        <script src="/f.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/d.js"></script>
    


<script src="/script/src/nlvi.js"></script>
<script src="/script/src/utils.js"></script>
<script src="/script/scheme/balance.js"></script>
<script src="/script/src/plugins.js"></script>
<script src="/script/bootstarp.js"></script>


<div class="backtop syuanpi dead toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


  <div class="search" id="search">
    <div class="mask" id="mask"></div>
    <div class="search-wrapper syuanpi">
      <h2 id="search-header" class="syuanpi">搜索一下？</h2>
      <div class="input">
        <input type="text" id="local-search-input" results="0" name="">
      </div>
      <div id="local-search-result"></div>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("MathJax config");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
