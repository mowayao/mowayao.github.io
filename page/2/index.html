<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Page 2 | Mowayao&#39;s Blog</title>
  <meta name="author" content="Mowayao">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Mowayao&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110229492-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Mowayao&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class="fa fa-user"></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>一往无前虎山行<span class="blink-fast">∎</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">
      <i class="fa fa-heart blink-slow"></i>
      一往无前虎山行
</div>    
		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/03/20/SPPNet笔记/" >SPPNet笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-03-20  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition，<strong>TPAMI</strong> 2015</p>
<p>作者：Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun</p>
<p>链接：<a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1406.4729.pdf</a></p>
<p>代码：<a href="https://github.com/ShaoqingRen/SPP_net" target="_blank" rel="external">https://github.com/ShaoqingRen/SPP_net</a></p>
<ul>
<li>提出spatial pyramid pooling layer，使得网络能够接受任意大小的输入，同时利用multi-level的spatial bins做hierarchy information aggregation，提高模型的鲁棒性</li>
<li>对image classification和object detection（针对RCNN的优化）都可以提升模型的表现</li>
<li>在ILSVRC 2014 object detection中排名第2，image classification中排名第3</li>
</ul>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/56717690.jpg" alt=""></p>
<p>在RCNN中，我们提取各个region proposals，然后将其warp到指定的大小，再用CNN提特征，这样的问题是图像object经过warp以后，会变形严重并且影响分辨率，从而影响分类的结果，所以作者提出SPP layer，使得对于任何大小的输入图像，神经网络都可以输出固定长度的特征，同时提高分类的鲁棒性。</p>
<h5 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h5><p>见Figure 3，将卷积层的输出经过SPP layer以后，利用max pooling分成3种的spatial bins，第一种是4x4，第二种是2x2，第三种是1x1(global pooling)。在做完pooling以后，我们可以得到16x256,4x256,1x256这三种维度的特征，256是输入的feature map的深度，拼接以后得到21x256长度的特征，其实可以将SPP layer看成是特征层面的re-scaled。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/71347165.jpg" alt=""></p>
<p>例如输入的feature map的大小是axa，需要产生3x3，2x2，1x1的特征向量，那么pooling的window size是ceil(a/n)，步长是floor(a/n)。将这些特征拼接得到固定长度的特征向量，再连接全连接层进行分类。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/44112090.jpg" alt=""></p>
<h5 id="Training-Strategy"><a href="#Training-Strategy" class="headerlink" title="Training Strategy"></a>Training Strategy</h5><p><strong>Multi-size Training</strong></p>
<p>在分类任务的模型训练过程中，采用多个size的输入可以提高模型的结果。对于有两个不同大小输入的训练策略，使用两个固定大小（框架所限）的相同参数的网络进行交替训练。</p>
<p><strong>Full-image Representation</strong></p>
<p>在分类任务的模型训练过程中，将图像resize到min(w,h)=256，保持长宽比不变。</p>
<h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p><strong>Image Classifications</strong></p>
<p>Table 1是三种base network的结构。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/11136760.jpg" alt=""></p>
<p>在分类任务中，作者通过实验证明，SPP layer，multi-size training，能够提高分类的精度（见Table 2）。SPP layer能够提高精度是因为利用spatial pyramid pooling可以提高鲁棒性。multi-size training中，有三种策略，一种是单个大小，第二种是180和224，还有一种是从[180,224]随机选择其中的一个size，实验证明，第二种精度最高。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/84456005.jpg" alt=""></p>
<p>从Table 3看，full-image representation相比较于central crop，可以提高准确率。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/81983088.jpg" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/64521005.jpg" alt=""></p>
<p><strong>Object Detection</strong></p>
<p>下图是R-CNN和SPP-Net的对比，R-CNN的缺点是计算量大，包括了大量的重复计算。</p>
<p><img src="http://img.blog.csdn.net/20170617102150673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdjFfdml2aWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p>其具体步骤和R-CNN类似：</p>
<ol>
<li>通过selective search得到2000个候选窗口</li>
<li>将整张图像输入到CNN中，完成特征的提取，然后在feature map上找到候选框的区域，在对候选框进行spaital pyramid pooling，得到定长的特征向量,这样做可以大大提高效率。</li>
<li>采用SVM模型，对物体进行分类。</li>
</ol>
<p>从Table 9看出，SPPNet相较于R-CNN计算效率有了较高的提升，同时准确率也有提高。</p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/35453761.jpg" alt=""></p>
<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/18-3-21/27949731.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/03/20/SPPNet笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/03/13/RCNN笔记/" >RCNN笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-03-13  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：Rich feature hierarchies for accurate object detection and semantic segmentation，CVPR 2014</p>
<p>作者：Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik</p>
<p>链接：<a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1311.2524.pdf</a></p>
<p>代码：<a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="external">https://github.com/rbgirshick/rcnn</a></p>
<h5 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h5><ul>
<li>之前的object detection算法都是用一些手工特征，例如SIFT或者HOG等，R-CNN使用pretrained的CNN提取region的特征，然后再用SVM分类做finetune（bridging the gap between image classification and object detection）</li>
<li>尝试不同层的feature作为分类的特征</li>
</ul>
<h5 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h5><p>R-CNN的模型见Figure 1，主要包括3个部分：</p>
<ol>
<li>提取region proposals</li>
<li>用CNN提取特征，得到一定长度的特征向量</li>
<li>对于提取到的特征，喂到若干个SVM，得到各个类别的概率</li>
<li>利用NMS优化结果</li>
</ol>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fpbdwp017oj30tq0mq0wc.jpg" alt=""></p>
<p><strong>Region proposals</strong></p>
<p>对于region proposals的提取，主要利用selective search这个启发式搜索方法。</p>
<p>selective search的步骤为：</p>
<ul>
<li>使用<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph Based Image Segmentation</a>中的方法来得到region</li>
<li>得到所有region之间两两的相似度</li>
<li>合并最像的两个region</li>
<li>重新计算新合并region与其他region的相似度</li>
<li>重复上述过程直到整张图片都聚合成一个大的region</li>
<li>使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果</li>
</ul>
<hr>
<p><strong>Feature extraction</strong></p>
<p>使用AlexNet提取特征，AlexNet现在ILSVRC 2012(IMAGENET)上做pre train。将图像的region patch warp到227x227，然后喂到网络，得到特征。</p>
<p><strong>Object category classifier</strong></p>
<p>对于每个类别，训练各自的SVM，这样就可以得到每个类的score，训练的时候将gt视为正样本，与gt的IoU &lt; 0.3视为负样本。因为负样本往往远远大于正样本，所以需要做hard negative mining，控制合理的比例，一般设置为1：3。</p>
<p><strong>Bounding box regression</strong></p>
<p>输入是n对${(P^i,G^i)}i=1,…,N$，其中$P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$，对应bounding box的中心，长宽，$G^i=(G_x,G_y,G_w,G_h)$。将P进行映射（前两者是平移，后两个是缩放）：<br>$$<br>\hat{G}_x = P_wd_x(P)+P_x \ \hat{G}_y = P_hd_y(P)+P_y \ \hat{G}_x = P_w\exp (d_w(P))\ \hat{G}_h = P_h\exp(d_h(P)) \\d_\star(P)=w_\star^T\phi_5(P)<br>$$<br>训练的时候对gt的四个坐标进行转换（上面的逆运算，使得$t_x,t_y$）：<br>$$<br>t_x = (G_x-P_x)/P_w \ t_y =(G_y −P_y)/P_h \ t_w = \log(G_w/P_w)\\t_h = \log(G_h/P_h).<br>$$<br>经过这样的变换，$t_x,t_y$为需要学习的平移量，$t_w, t_h$为需要学习的缩放量。</p>
<p>其中，$G$表示的是ground truth，P表示的训练样本，我们的任务是求解$W_\star$，其实就是通过梯度下降或者最小二乘法求解ridge regression问题，:<br>$$<br>w_\star =  \arg\min_{\hat{w}_\star}(t^i_\star − \hat{w}^T_\star \phi_5(P^i))^2+\lambda ||\hat{w}_\star||^2<br>$$<br>其中$\phi_5(P^i)$表示的是第5个pooling的输出。需要注意的是，正则系数$\lambda=1000$，而且当G和P相差很大的时候，效果会不好，所以需要设置IoU阈值，将其设为0.6。</p>
<h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>Table 1是模型在VOC 2012 test上的表现，可以看出R-CNN BB远超其他baseline，证明了模型的性能。</p>
<p>Table 2是模型在VOC 2007 test上的表现，可以在上面做一些ablation study：</p>
<ul>
<li>对于不finetune的模型，第一行到第三行，可以发现fc6的结果最好，这样就可以移除fc7的参数，简化模型。</li>
<li>对于finetune的模型，第四行到第六行，可以发现fc7的结果最好，经过网络的同层比较，可以发现finetune可以大大提升模型的performance。</li>
<li>通过第七行与第六行比较，可以看出bounding box regression可以较为显著地提升模型性能。</li>
<li>跟其他手工特征相比，CNN提取的特征具有更强的表达能力。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fpbidj661zj31kw0eeq6d.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fpbi5y3ykqj31kw0k9wjg.jpg" alt=""></p>
<p>​    </p>

	
	</div>
  <a type="button" href="/2018/03/13/RCNN笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/03/06/ResNeXt笔记/" >ResNeXt笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-03-06  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 :Aggregated Residual Transformations for Deep Neural Networks, CVPR 2017</p>
<p>链接：<a href="https://arxiv.org/pdf/1611.05431.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1611.05431.pdf</a></p>
<p>作者:  Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He</p>
<p>源码：<a href="https://github.com/facebookresearch/ResNeXt" target="_blank" rel="external">https://github.com/facebookresearch/ResNeXt</a></p>
<h5 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h5><p>提出ResNeXt网络，基于ResNet和GoogleNet。</p>
<p>提出cardinality的概念，对于每个conv block，对输入做多种不同的（结构相同，参数不同）transformations，然后再aggregation，而transformations的数量就是cardinality。</p>
<p>结构启发于inception module，同样是split-transform-merge的策略，这里的merge和transform都和inception不一样。split-transform-merge的策略主要是<strong><em>通过在一个尽可能小的计算复杂度的前提下，提高模型的表达能力</em></strong>（approach the representational power of large and dense layers, but at a considerably lower computational complexity）！ Inception的问题在于每个block都需要定制，导致了模型的灵活性较差，这些block的定制化设计相当于引入了一堆的超参，大量的超参对模型来讲无疑是不利的。而这篇文章就是基于这点进行改进！</p>
<ul>
<li>模块化</li>
<li>超参少</li>
<li>表达能力强</li>
</ul>
<h5 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h5><p>首先，总结了两点block设计的原则：</p>
<ol>
<li>对于相同大小的feature map，block的超参要相同，也就是conv的filter参数要一样。【if producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes)】</li>
<li>如果空间维度减半，那么feature maps的数量要加倍。【each time when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor of 2】</li>
</ol>
<p>Figure 1是ResNet和ResNeXt的block结构比较，重点看ResNeXt，可以看到这里将256维的输入做32个不同的transformations，每个transformation先通过1x1的conv做降维(information embedding)，然后再经过几个conv，最后将这32个输出做aggregation。</p>
<p>将其公式化:<br>$$<br>F(x) = \sum_{i=1}^C \tau_i(x)<br>$$<br>$\tau_i$的作用就是将x投影到低维空间（embeeding），然后做transform。C就是做transformation的数量，也就是cardinaty。aggregation后，再用short connection做identity mapping。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp3f3tppr3j30te0i0jur.jpg" alt=""></p>
<p>Figure 3是异构的三种结构，（a）是初始版本，（b）concat，（c）group conv，这三者可以说是等价的。那么在实现上，可以利用group conv，更加方便！</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fp4d6cqzx8j31cq0i2q8m.jpg" alt=""></p>
<p>​    Table 1是ResNet-50和ResNeXt-50的参数对比</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fp4fhovd49j30nw0uaq8x.jpg" alt=""></p>
<h5 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h5><p>Table 3是各个版本的ResNeXt以及ResNet在ImageNet-1K数据集上的表现，可以发现随着Cardinaty增加，模型总体上是变好的，体现在top-1 error逐渐在降低。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fp4fegkbv6j30oc0jqn0j.jpg" alt=""></p>
<p>Table 6是ResNet和ResNeXt在ImageNet-5K上的比较，可以可发现后者都要好于前者。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fp4fkycmlxj30pi0eotbv.jpg" alt=""></p>
<p>Table 5则是各个state-of-the-art的模型在ImageNet-1K数据集上表现，可以发现模型达到了最好的结果。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fp4fmo7mobj30ow0gaacv.jpg" alt=""></p>
<p>从Table 7可以发现，相同的参数，ResNeXt比Wide ResNet表现更好。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fp4fojev6aj30pc09sgn0.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/03/06/ResNeXt笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/03/06/DenseNet笔记/" >DenseNet笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-03-06  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Densely Connected Convolutional Networks, CVPR 2017</p>
<p>链接：<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf" target="_blank" rel="external">http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf</a></p>
<p>作者: Huang Gao, Liu Zhuang, Weinberger, K. Q., &amp; van der Maaten, L. </p>
<p>源码：<a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="external">https://github.com/liuzhuang13/DenseNet</a>.</p>
<h5 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h5><p>DenseNet的idea和ResNet有点相近，既然ResNet可以通过一个skip connection就提高模型的性能，那为什么不多几个呢？</p>
<p>这样的话就可以使得梯度信息的回传更加有效（ResNet通过identity mapping进行另一条路径的梯度回传,而DenseNet的每一层梯度都可以直接回传回它前面的层），可以充分利用各个level的特征，也可以减少参数等。</p>
<ul>
<li>alleviate the vanishing-gradient problem，利用dense connection有效回传梯度</li>
<li>strengthen feature propagation，利用dense connection融合(concat)不同level的特征，使得浅层的特征更容易传播到高层，同时也可以让浅层的conv学一些discriminative features。</li>
<li>encourage feature reuse</li>
<li>substantially reduce the number of parameters，通过控制growth rate减少参数量</li>
</ul>
<p>假设有L层网络，那么如果将网络视为DAG，那么如果实行全连接，并且保证网络前向传播，共有$\frac{L\times(L+1)}{2}$条边。</p>
<h5 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h5><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fp33t32d1mj31kw0bt0xv.jpg" alt=""></p>
<p><strong>Dense Block</strong></p>
<p>首先，将其公式化，我们考虑第l层网络的输入和输出，$x_l$表示为第l层的输出，那么有:<br>$$<br>x_l = H_l([x_0,x_1,…,x_{l-1}])<br>$$<br>这里，$[x_0,x_1,…,x_{l-1}]$表示feature map的按通道拼接，形成新的tensor。</p>
<p>$H_l$是一个组合函数，BN-ReLU-Conv(3x3)。</p>
<p>为了将模型简化，将预想中的DenseNet分拆成若干个dense block，见Figure 2。</p>
<p><strong>Growth Rate</strong></p>
<p>$H_l$的输出是k个feature maps，也被叫做是growth rate。</p>
<p>对于每个dense block来说，假设输入的深度是$k_0$，对于第l个层的输出，则有$k\times(l-1)+k_0$的深度，从Figure 2就可以简单得出。作者将这种dense connection解释为collective knowledge，获取前面层的输出，growth rate则是起到限制knowledge容量的作用，使得knowledge精简化!</p>
<p><strong>Transition Layer</strong></p>
<p>每个block之间用transition layer，包括1x1的conv和pooling层。transition layer起到压缩网络模型的作用，如果每个block包括m个feature maps，那么经过压缩后，就变成$\theta m, 0\le \theta \le 1$，同时利用pooling降采样。我们把$\theta =0.5$的结构（将feature maps的数量降为原来的一般）记作DenseNet-C。</p>
<p><strong>Bottleneck Layer</strong></p>
<p>因为每个block的输出都是k，但是它的输入往往比k更大，所以引入bottleneck layer起到降维的作用，同时保留尽可能多的信息。这样$H_l$就变成了BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)，我们将使用bottleneck layer的版本记作DenseNet-B。</p>
<p>Table 1是模型的参数结构。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fp3b35pbwqj31kw0qqair.jpg" alt=""></p>
<h5 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h5><p><strong>训练细节</strong></p>
<p>对于CIFAR和SVHN,初始lr设为0.1，在训练50%和75%的epochs时，除以10。</p>
<p>对于IMAGENET,初始lr设为0.1，在30和60个epochs后，除以10。</p>
<p>SGD，weight decay $10^{-4}$</p>
<p>因为CIFAR和SVHN数据集较小，所以在conv后（除第一个）都加了dropout，ratio为0.2。</p>
<p>Tabel 2是各个模型在CIFAR-10，CIFAR-100，SVHN三个数据集上的结果，+号表示做数据增强，将用了Bottleneck layer和feature maps compression的记做DenseNet-BC。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp3bafnkxmj314u0pwgtx.jpg" alt=""></p>
<p>总的来说，数据增强可以提升表现！</p>
<ul>
<li>可以发现，和ResNet相比，参数更少，错误率更低，模型参数利用率高！</li>
<li>在C10和C100上，DenseNet-BC(L=190,k=40)达到最优的结果。</li>
<li>观察原始的DenseNet（没有BC），可以发现，随着L和k的增大，模型性能在提升（参数量上升，模型容量提升，表达能力提升），说明dense block有较强的抗过拟合的能力。</li>
<li>在SVHN上，DenseNet-BC结果要比没有原始的DenseNet，这可能是数据集过小，模型过拟合了。</li>
</ul>

	
	</div>
  <a type="button" href="/2018/03/06/DenseNet笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/26/SegNet笔记/" >SegNet笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-26  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：SegNet: Identity Mappings in Deep Residual Networks, ECCV 2016</p>
<p>链接：<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7803544" target="_blank" rel="external">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7803544</a></p>
<p>作者: Vijay Badrinarayanan, Alex Kendall , and Roberto Cipolla</p>
<p><strong>Idea</strong></p>
<ul>
<li>SegNet基于FCN，包含encoder和decoder两个部分，encoder使用VGGNet，decoder则使用skip architecture将相同分辨率的feature map结合再做后续的预测。</li>
<li>上采样的方法不同于transposed convolution和双线性插值等方法，使用maxpooling的逆运算的方法做上采样，得到稀疏的feature map，再做卷积得到稠密的feature map。</li>
</ul>
<p><strong>Architecture</strong></p>
<p>具体的框架结构件Fig.2，主要由两部分组成，encoder和decoder。</p>
<p>encoder的部分包含VGGNet在全连接层前的所有卷积层，这样共有13个卷积层，并且参数是直接从在imagenet预训练过的VGGNet直接拷贝过来，进行初始化。通过移除全连接层，可以大大地减少参数量和提高运算效率，参数量从134M减少到了14.7M。</p>
<p>文章的重点部分在于decoder的设计，decoder部分包含和encoder一样的卷积层数量，并且参数配置一一对应。在VGGNet中，max pooling的stride是2，kernel size是2，所以它的下采样是没有重叠的。在升采样上，和其他类似U-Net等不同的是，SegNet使用的是maxpooling逆运算，也就是利用保存的max pooling的元素下标，将feature map恢复到原来的大小，再利用可训练的卷积层得到稠密的feature map。具体的过程见Fig.3。</p>
<p>因为使用max pooling可以提高特征的translation invariance，同时提高分类的鲁棒性，但是同时也损失了很多边缘信息，因为我们把边缘的位置信息丢弃了。通常来说，响应比较强的都是一些纹理边缘，而max pooling本质上是一个滤波器，将局部最大值保留下来，舍去其他值。通过将max pooling的下标保留下来，可以帮助我们尽可能的恢复边缘位置信息。</p>
<p>使用上述上采样的方法有以下的好处：</p>
<ul>
<li>可以得到更加细致的轮廓</li>
<li>减少训练参数，同时能够end-to-end的训练</li>
<li>可以将其融入到任何encoder-decoder的结构中，具有普适性。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fotu08o3s4j312u0e4ajm.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fotuej8g3gj313a0eo429.jpg" alt=""></p>
<p>Pytorch代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">segnet</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_classes=<span class="number">21</span>, in_channels=<span class="number">3</span>, is_unpooling=True)</span>:</span></div><div class="line">        super(segnet, self).__init__()</div><div class="line"></div><div class="line">        self.in_channels = in_channels</div><div class="line">        self.is_unpooling = is_unpooling</div><div class="line"></div><div class="line">        self.down1 = segnetDown2(self.in_channels, <span class="number">64</span>)</div><div class="line">        self.down2 = segnetDown2(<span class="number">64</span>, <span class="number">128</span>)</div><div class="line">        self.down3 = segnetDown3(<span class="number">128</span>, <span class="number">256</span>)</div><div class="line">        self.down4 = segnetDown3(<span class="number">256</span>, <span class="number">512</span>)</div><div class="line">        self.down5 = segnetDown3(<span class="number">512</span>, <span class="number">512</span>)</div><div class="line"></div><div class="line">        self.up5 = segnetUp3(<span class="number">512</span>, <span class="number">512</span>)</div><div class="line">        self.up4 = segnetUp3(<span class="number">512</span>, <span class="number">256</span>)</div><div class="line">        self.up3 = segnetUp3(<span class="number">256</span>, <span class="number">128</span>)</div><div class="line">        self.up2 = segnetUp2(<span class="number">128</span>, <span class="number">64</span>)</div><div class="line">        self.up1 = segnetUp2(<span class="number">64</span>, n_classes)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></div><div class="line"></div><div class="line">        down1, indices_1, unpool_shape1 = self.down1(inputs)</div><div class="line">        down2, indices_2, unpool_shape2 = self.down2(down1)</div><div class="line">        down3, indices_3, unpool_shape3 = self.down3(down2)</div><div class="line">        down4, indices_4, unpool_shape4 = self.down4(down3)</div><div class="line">        down5, indices_5, unpool_shape5 = self.down5(down4)</div><div class="line"></div><div class="line">        up5 = self.up5(down5, indices_5, unpool_shape5)</div><div class="line">        up4 = self.up4(up5, indices_4, unpool_shape4)</div><div class="line">        up3 = self.up3(up4, indices_3, unpool_shape3)</div><div class="line">        up2 = self.up2(up3, indices_2, unpool_shape2)</div><div class="line">        up1 = self.up1(up2, indices_1, unpool_shape1)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> up1</div></pre></td></tr></table></figure>
<p><strong>实验</strong></p>
<p>首先针对deocder做了以下几种变形：</p>
<ul>
<li>SegNet-basic: 4个编码块和4个解码块，编码块由conv,bn,relu组成，然后加max pooling，解码器由conv，bn组成，并且没有bias。所有的卷积核的大小都是7x7。</li>
<li>SegNet-Basic-SingleChannelDecoder：卷积是单通道的，分别和对应的feature map层做卷积，这样可以减少训练参数，同时提高inference的速度。</li>
<li>FCN-basic：和SegNet-basic类似，不同的是使用FCN的上采样方法。</li>
<li>FCN-basic-NoAddition：不进行feature map相加，只通过deconv上采样。</li>
<li>Bilinear-Interpolation：使用双线性插值上采样。</li>
<li>SegNet-Basic-EncoderAddition：和FCN的方法类似，比较消耗显存，将几个feature map相加得到最终的feature map</li>
<li>FCN-Basic-NoDimReduction：再输出最终的结果之间不进行降维，FCN-basic会将维度降低到64，然后在做1x1的卷积。</li>
</ul>
<p>比较结果见TABLE 1。参数数量上，FCN-Basic和FCN-Basic-NoAddition最少，inference time则是FCN-Basic-NoAddition最少。</p>
<ul>
<li>在性能上，bilinear-interpolation表现最差，说明了decoder需要学习，而不是简单粗暴的直接上采样！</li>
<li>SegNet-Basic和FCN-Basic相比，精度相近，区别在于后者的显存消耗更大因为后者需要存储多个feature map，而前者只需要存储max pooling的下标。除此之外，后者的forward时间更短，因为它在做预测前对feature map做了降维。</li>
<li>FCN-Basic-NoAddition和SegNet-Basic：两者的decoder最为相似，因为都是直接学上采样，没有feature map的相加，在精度上，SegNet-Basic较好，说明了利用低层次的feature map的重要性，也就是将高层次语义信息和低层次位置信息结合的重要性。</li>
<li>FCN-Basic-NoAddition-NoDimReduction和SegNet-Basic：前者的模型要大于后者，因为没有做降维。在性能上，前者也不如后者。说明并不是模型越大，表现越好，重要的是需要capture更多的边缘信息。</li>
<li>FCN-Basic-NoAddition 和SegNet-Basic-SingleChannelDecoder：证明了当面临存储消耗，精度和inference时间的妥协的时候，我们可以选择SegNet!</li>
<li>当内存和inference时间不受限的时候，模型越大，表现越好。因为FCN-Basic-NoDimReduction和SegNet-EncoderAddition比其他变种要好。</li>
<li>class balance的影响：在class average accuracy和mIoU指标上，可以发现没有class weighted的时候比经过class weighted的要差，而在global accuracy上，情况则相反，这是因为绝大部分的像素都是天空，道路和建筑。</li>
</ul>
<p>总结一下：</p>
<ul>
<li>内存受限的时候，可以用过降维，存储max pooling下标来提升表现。</li>
<li>编码器一定，解码越大，性能越好</li>
<li>编码器的feature map被完整保留下来时，效果最好，毕竟是空间换性能！</li>
</ul>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fotvqp2uqsj313y0g2agk.jpg" alt=""></p>
<p>模型在CamVid数据集上的比较结果见TABLE 3，CamVid是一个用于自动驾驶的室外场景，可以发现在迭代次数较少的时候，SegNet要好于其他方法其他方法，但是当迭代次数较高的时候，整体上SegNet还是表现最优，但是在BF指标上不如DeconvNet。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fotwygav2aj313c0a8tc7.jpg" alt=""></p>
<p>模型在SUNRGB-D数据集上的比较结果见TABLE 4，可以发现，所有方法的表现都比较差，在G，C，BF指标上，SegNet好于其他模型，但是mIoU比DeepLab-LargeFOV要差。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fotx37mf6nj31360c642m.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/26/SegNet笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/26/Fully-Convolutional-Networks-for-Semantic-Segmentation-FCN-笔记/" >Fully Convolutional Networks for Semantic Segmentation(FCN)笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-26  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：Fully Convolutional Networks for Semantic Segmentation，CVPR 2015</p>
<p>链接：<a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="external">https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf</a></p>
<p>作者：Jonathan Long, Evan Shelhamer, Trevor Darrell</p>
<p><strong>Idea</strong></p>
<p>传统的图像分割算法主要依赖于：</p>
<ol>
<li>图像patch的训练，使得效率和准确率都不够。</li>
<li>后处理</li>
<li>multi-scale pyramid的预处理</li>
<li>ensemble</li>
</ol>
<p>…</p>
<p>这篇文章的主要贡献就是建立构造了全卷积神经网络，在显存足够的前提下可以接受任意大小的输入，然后得到对应大小的输出。作者将VGGNet,GoogleNet用于物体分类并预训练好的网络改造成全卷积神经网络，能够实现end-to-end的训练，并将其finetune，用来做图像的语义分割。除此之外，在升采样的过程中，由于之前的降采样丢失了大量的位置信息，所以作者采用了skip architecture将低层的输出传给高层，实现浅层的位置信息和高层的语义信息的融合。</p>
<p><strong>网络结构</strong></p>
<p>以VGGNet为例，见Figure3，最后的全连接层的维度是4096，为了将其改造成全卷积，将全连接层改成1x1的卷积，这样它的输出变成了4096x1x1。然后，通过deconvotion的操作对feature map进行升采样，恢复到和原图一样的大小，所以最终的输出的大小为WxHxC，其中W,H是原图的大小，而C是像素的种类数，每个空间位置表示的是各个类别的概率。降采样会提高感受野，同时也会包含越来越多的语义信息，但是也会丢失位置信息，所以，为了做出更好的像素分类结果，需要将高层的语义信息(what)和底层的位置信息(where)结合起来，使用skip architecture的结构，用低层信息对结果进行修正，提高模型的性能。除此之外，作者还提供了3个变种，即FCN-32s，FCN-16s，FCN-8s。顾名思义，FCN-32s就是通过deconvolution进行32倍的升采样直接输出。FCN-16s则是联合pool4和2倍升采样的conv7，再做16倍的升采样。FCN-8s是pool3，2倍升采样的pool4和4倍升采样的conv7，结合以后再做8倍的升采样。</p>
<p>升采样的方法有多种，可以采用双线性插值等非学习性方法，也可以采用transposed convolution，通过设定步长等参数进行升采样。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fotrxkpssoj31j20tw113.jpg" alt=""></p>
<p>下面是FCN-8的pytorch实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">fcn_8s</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></div><div class="line">        super(fcn, self).__init__()</div><div class="line">        self.stage1 = nn.Sequential(*list(pretrained_net.children())[:<span class="number">-4</span>])</div><div class="line">        self.stage2 = list(pretrained_net.children())[<span class="number">-4</span>]</div><div class="line">        self.stage3 = list(pretrained_net.children())[<span class="number">-3</span>] </div><div class="line">        </div><div class="line">        self.scores1 = nn.Conv2d(<span class="number">512</span>, num_classes, <span class="number">1</span>)</div><div class="line">        self.scores2 = nn.Conv2d(<span class="number">256</span>, num_classes, <span class="number">1</span>)</div><div class="line">        self.scores3 = nn.Conv2d(<span class="number">128</span>, num_classes, <span class="number">1</span>)</div><div class="line">        </div><div class="line">        self.upsample_8x = nn.ConvTranspose2d(num_classes, num_classes, <span class="number">16</span>, <span class="number">8</span>, <span class="number">4</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.upsample_4x = nn.ConvTranspose2d(num_classes, num_classes, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.upsample_2x = nn.ConvTranspose2d(num_classes, num_classes, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>)   </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.stage1(x)</div><div class="line">        s1 = x <span class="comment"># 1/8</span></div><div class="line">        </div><div class="line">        x = self.stage2(x)</div><div class="line">        s2 = x <span class="comment"># 1/16</span></div><div class="line">    </div><div class="line">        x = self.stage3(x)</div><div class="line">        s3 = x <span class="comment"># 1/32</span></div><div class="line">        </div><div class="line">        s3 = self.scores1(s3)</div><div class="line">        s3 = self.upsample_2x(s3)</div><div class="line">        s2 = self.scores2(s2)</div><div class="line">        s2 = s2 + s3</div><div class="line">        </div><div class="line">        s1 = self.scores3(s1)</div><div class="line">        s2 = self.upsample_4x(s2)</div><div class="line">        s = s1 + s2</div><div class="line">        s = self.upsample_8x(s2)</div><div class="line">        <span class="keyword">return</span> s</div></pre></td></tr></table></figure>
<p><strong>实验</strong></p>
<p>3个变种的效果在PASCAL VOS数据集上的比较，见Table 2和Figure 4，可以发现都是FCN-8s更胜一筹。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fotswcnw1kj30q60dw76m.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fotsvv8zbuj30rw0h4win.jpg" alt=""></p>
<p>3种分类模型在PASCAL VOC 2011数据集的比较：AlexNet，VGGNet，GoogleNet，见Table1，可以发现VGGNet超出其他两个模型很多。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fotsyeuufij30q60me42k.jpg" alt=""></p>
<p>同时，FCN在PASCAL-S，NYUDv2，SIFT FLOW数据集也取得了state-of-the-art的结果。</p>

	
	</div>
  <a type="button" href="/2018/02/26/Fully-Convolutional-Networks-for-Semantic-Segmentation-FCN-笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/09/Kaggle比赛小结-Camera-Model-Identification/" >Kaggle比赛小结-Camera Model Identification</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-09  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p><a href="https://www.kaggle.com/c/sp-society-camera-model-identification" target="_blank" rel="external">https://www.kaggle.com/c/sp-society-camera-model-identification</a></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1foa5fw8h19j31k20483yy.jpg" alt=""></p>
<p>参加完比赛，做个简单的总结：</p>
<ul>
<li>数据增强：JPEG压缩，resizing，gamma修正</li>
<li>数据的不平衡需要设置采样权重</li>
<li>Random Crop，尽可能大，取512的结果好于256，先crop 1024，做完数据增强后再crop 512</li>
<li>网络模型，Dense201&gt;Dense161…..</li>
<li>Focal Loss可以提高结果</li>
<li>learning rate: 1e-4，optimizer: Adam，batch size: 16……</li>
<li>ensemble</li>
</ul>
<p>可以考虑的点：</p>
<ul>
<li>因为验证集的结果还不错，可以将测试集用训练得到的模型做分类，再训练，做数据扩充</li>
<li>可以人工地下载更多的数据。。。</li>
<li>结合手工提取的特征做结果的修正，例如noise pattern:<a href="https://www.kaggle.com/zeemeen/i-have-a-clue-what-i-am-doing-noise-patterns" target="_blank" rel="external">https://www.kaggle.com/zeemeen/i-have-a-clue-what-i-am-doing-noise-patterns</a></li>
</ul>

	
	</div>
  <a type="button" href="/2018/02/09/Kaggle比赛小结-Camera-Model-Identification/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/07/Inception-v4-Inception-ResNet-and-Inception-v4-笔记/" >Inception-v4, Inception-ResNet and(Inception v4)笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-07  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, AAAI 2017</p>
<p>链接：<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">https://arxiv.org/abs/1602.07261</a></p>
<p>作者: Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi</p>
<p>呃。。。这篇文章的主要贡献是了尝试了ResNet和Inception结合的多种可能性。。。。对Inception block进行修改和优化，提出了Inception v4（Figure 9），Inception-ResNet-v1和Inception-ResNet-v2（Figure 15），将ImageNet classification task的top-5 error刷到了3.08%。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobembpr0rj30ee0rcwhc.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdqhuy1oj30eo0de3zr.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobdr5dh4pj30ek0c6myf.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobdrmnbfwj30es0fitab.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdroxsyfj30fi0h2jt6.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobdsgyknpj30eg0m8wg5.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdss6xhkj30f40femyk.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobdsvxatrj30fg0j6dhi.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdt1ggqpj30ew0cc75o.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdtqs6ylj30g00jsdhj.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdu21ayyj30cw0kkgnb.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobdvixvgxj30ek0lewgk.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdvwhgcdj30ek0fe3zx.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdvyzlfvj30ek0j0myu.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobdw3cijkj30fi0c20u5.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdw6e7fqj30f20iiwg6.jpg" alt=""></p>
<p>实验发现：如果feature map的数量超过1000，网络会不work，训练的时候会不稳定，而且在早期就会“die”。所以在residual加个scaling，使得数值偏小，见Figure 20。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobe052il4j30s00o4q62.jpg" alt=""></p>
<h5 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h5><p>single crop和single model的比较结果见Table 2，可以发现Inception-ResNet-v2略胜一筹，但是和Inception-v4差距不大。</p>
<p>小数量的crop和single model的比较结果见Table 3，和Table 2的结果类似。</p>
<p>dense crop和single model的比较结果见Table 4，结果依然类似。</p>
<p>以及，crop的数量对结果影响还是很大的！</p>
<p>144 crop的模型ensemble后的比较结果见Table 5。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobe29be8mj30t60e6q5l.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobe24s4gmj30sq0e80vm.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobe25u8vpj30tq0dmju6.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobe291a2tj30uw0gu0wt.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/07/Inception-v4-Inception-ResNet-and-Inception-v4-笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/07/Rethinking-the-Inception-Architecture-for-Computer-Vision-Inception-V3-笔记/" >Rethinking the Inception Architecture for Computer Vision(Inception v3)笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-07  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Rethinking the Inception Architecture for Computer Vision, CVPR 2016</p>
<p>链接：<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1512.00567.pdf</a></p>
<p>作者: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna</p>
<ul>
<li>优化了Inception的结构</li>
<li>讨论了一些设计原则和可以优化的方向</li>
</ul>
<ul>
<li><p>在ILSVRC 2012 classification验证集上取得state-of-the-art的结果：21.2% top-1 error和5.6% top-5 error，ensemble后，取得3.5% top-5 error和17.3% top-1 error。</p>
<p>​</p>
</li>
</ul>
<h5 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h5><ul>
<li>为了避免表示瓶颈(representational bottlenecks)，feature map应该缓慢减小！</li>
<li>高维度的feature map能够更容易地处理局部信息（有更多的feature maps），在CNN中提高响应（融合不同感受野的卷积提取的特征）可以解耦更多特征，网络训练也更快</li>
<li>用1x1卷积做embeeding，在没有大量或者一些表现能力损失的基础上降低维度(局部空间的高相关性)。在做完spatial aggregation后用1x1卷积降低维度</li>
<li>网络的深度和宽度应该同时增加或者减少。这两者之间存在某种平衡</li>
</ul>
<h5 id="Factorizing-Convolutions-with-Large-Filter-Size"><a href="#Factorizing-Convolutions-with-Large-Filter-Size" class="headerlink" title="Factorizing Convolutions with Large Filter Size"></a>Factorizing Convolutions with Large Filter Size</h5><p>为了提高计算效率，可以将大的卷积核分解。</p>
<p><strong>Factorization into smaller convolutions</strong></p>
<p>例如5x5的卷积和3x3的卷积在其他条件相同的情况下，前者的计算量是后者的$\frac{25}{9}$。</p>
<p>用两个3x3的卷积代替5x5，两者具有相同的感受野，可以降低28%的计算量，而且经过实验证明，中间还是要有ReLU，而不是线性激活，见Figure 4和Figure 5</p>
<p><strong>Spatial Factorization into Asymmetric Convo- lutions</strong></p>
<p>分解成非对称的卷积，nx1的卷积。例如用3x1加1x3的两个卷积来代替3x3的卷积，这样的话可以降低33%的计算量。扩展一下，我们可以将nxn的卷积分解成1xn和nx1两个卷积，n越大，降低的计算量越大。</p>
<p>实际上，这样的分解在浅层效果并不好，只有在中层的时候效果不错，对于nxn的feature map来说，n一般从12到20，对于这些尺寸，7x1和1x7的小贵最好，见Figure 6。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo7ytt7rq5j30p60i2tab.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo7ytqofwjj30tc0mkjtw.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, pool_features)</span>:</span></div><div class="line">        super(InceptionA, self).__init__()</div><div class="line">        self.branch1x1 = BasicConv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">        self.branch5x5_1 = BasicConv2d(in_channels, <span class="number">48</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch5x5_2 = BasicConv2d(<span class="number">48</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</div><div class="line"></div><div class="line">        self.branch3x3dbl_1 = BasicConv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_2 = BasicConv2d(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_3 = BasicConv2d(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line"></div><div class="line">        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        branch1x1 = self.branch1x1(x)</div><div class="line"></div><div class="line">        branch5x5 = self.branch5x5_1(x)</div><div class="line">        branch5x5 = self.branch5x5_2(branch5x5)</div><div class="line"></div><div class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</div><div class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</div><div class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</div><div class="line"></div><div class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        branch_pool = self.branch_pool(branch_pool)</div><div class="line"></div><div class="line">        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]</div><div class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo7zsanjwbj30sq0t8dj2.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionC</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, channels_7x7)</span>:</span></div><div class="line">        super(InceptionC, self).__init__()</div><div class="line">        self.branch1x1 = BasicConv2d(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">        c7 = channels_7x7</div><div class="line">        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</div><div class="line">        self.branch7x7_3 = BasicConv2d(c7, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</div><div class="line">        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</div><div class="line">        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</div><div class="line">        self.branch7x7dbl_5 = BasicConv2d(c7, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</div><div class="line"></div><div class="line">        self.branch_pool = BasicConv2d(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        branch1x1 = self.branch1x1(x)</div><div class="line"></div><div class="line">        branch7x7 = self.branch7x7_1(x)</div><div class="line">        branch7x7 = self.branch7x7_2(branch7x7)</div><div class="line">        branch7x7 = self.branch7x7_3(branch7x7)</div><div class="line"></div><div class="line">        branch7x7dbl = self.branch7x7dbl_1(x)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)</div><div class="line"></div><div class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        branch_pool = self.branch_pool(branch_pool)</div><div class="line"></div><div class="line">        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]</div><div class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fo80dcq1fhj30ri0q8dk2.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionE</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels)</span>:</span></div><div class="line">        super(InceptionE, self).__init__()</div><div class="line">        self.branch1x1 = BasicConv2d(in_channels, <span class="number">320</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">        self.branch3x3_1 = BasicConv2d(in_channels, <span class="number">384</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch3x3_2a = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">        self.branch3x3_2b = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        self.branch3x3dbl_1 = BasicConv2d(in_channels, <span class="number">448</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_2 = BasicConv2d(<span class="number">448</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_3a = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">        self.branch3x3dbl_3b = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        self.branch_pool = BasicConv2d(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        branch1x1 = self.branch1x1(x)</div><div class="line"></div><div class="line">        branch3x3 = self.branch3x3_1(x)</div><div class="line">        branch3x3 = [</div><div class="line">            self.branch3x3_2a(branch3x3),</div><div class="line">            self.branch3x3_2b(branch3x3),</div><div class="line">        ]</div><div class="line">        branch3x3 = torch.cat(branch3x3, <span class="number">1</span>)</div><div class="line"></div><div class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</div><div class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</div><div class="line">        branch3x3dbl = [</div><div class="line">            self.branch3x3dbl_3a(branch3x3dbl),</div><div class="line">            self.branch3x3dbl_3b(branch3x3dbl),</div><div class="line">        ]</div><div class="line">        branch3x3dbl = torch.cat(branch3x3dbl, <span class="number">1</span>)</div><div class="line"></div><div class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        branch_pool = self.branch_pool(branch_pool)</div><div class="line"></div><div class="line">        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]</div><div class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h5 id="Auxiliary-Classifiers"><a href="#Auxiliary-Classifiers" class="headerlink" title="Auxiliary Classifiers"></a>Auxiliary Classifiers</h5><p>目的是为了让梯度回传更加有效，从而加快训练，而事实是辅助分类器在训练初期的时候对结果并没有特别大的帮助，在训练后期的时候会超上没有使用辅助分类器的模型。额外辅助器可以作为一个regularizer。</p>
<h5 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h5><p>在feature map深度加倍的时候，空间维度需要减半。对于一个k个dxd的feature map，主要有两种选项：</p>
<ol>
<li>首先进行步长为1的卷积，将深度加倍，然后加个pooling，这样的复杂度是$2d^2k^2$，见Figure 9。</li>
<li>扔掉pooling，也就是用一个卷积直接搞定，那么这样的复杂度是$2(\frac{d}{2})^2k^2$，变成了原来的四分之一，但是这样会有表达瓶颈。</li>
<li>用并行的步长为2卷积和pooling，见Figure 10。</li>
</ol>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo809hnvetj30sg0k6tbp.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo809udzjvj30sc0mg42c.jpg" alt=""></p>
<h5 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception-v3"></a>Inception-v3</h5><p>网络的配置见Table 1。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo80da87wrj30ra0vcagv.jpg" alt=""></p>
<p><strong>Label Smoothing</strong></p>
<p>为了防止模型预测的时候over-confident，用label smoothing来做一个正则化。</p>
<p>将label distribution从$q(k|x)=\delta_{k,y}$替换为$q(k|y)=(1-\epsilon)\delta_{k,y}+\epsilon\mu$</p>
<h5 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h5><p>首先针对不同的感受野做了实验，见Table2，299x299在保准计算效率的同时，有较高的准确率。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobcl3nkrlj30ra0a0dhh.jpg" alt=""></p>
<p>单模型比较结果见Table 3。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobckztjttj30ra0sq7b3.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/07/Rethinking-the-Inception-Architecture-for-Computer-Vision-Inception-V3-笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/01/Identity-Mappings-in-Deep-Residual-Networks笔记/" >Identity Mappings in Deep Residual Networks笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-01  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Identity Mappings in Deep Residual Networks, ECCV 2016</p>
<p>链接：<a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1603.05027.pdf</a></p>
<p>作者: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</p>
<h5 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h5><p>ResNet的每个unit可以表示为：<br>$$<br>y_l = h(x_l)+F(x_l,W_l) \ x_{l+1} = f(y_l)<br>$$<br>$x_l$是第l个unit的输入，$x_{l+1}$是l个单元的输出。在ResNet v1中，$h(x_l)=x_l$，表示identity mapping(做了3种尝试，选择了这种)，f是ReLU函数，F包含2-3个卷积层，中间还有BN和ReLU（见Figure 1(a))。如果h和f都是identity mapping的时候，<strong>信号可以直接从一个unit到另一个unit，无论是forward还是backward，这样可以使训练更加容易</strong>。</p>
<p>Forward:<br>$$<br>x_{l+1}=x_l+F(x_l,W_l)… \ x_L = x_l + \sum_{i=l}^{L+1}F(x_i,W_i)<br>$$</p>
<ul>
<li>第L层的特征$x_L$可以表示成$x_l+\sum_{i=l}^{L-1}F(x_i,W_i)$</li>
<li>$x_L=x_o+\sum_{i=0}^{L-1}F(x_i,W_i)$，可以由所有的残差函数相加得到，再加上$x_o$</li>
</ul>
<p>Backward:</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo5tv0mjxdj30my03mjrs.jpg" alt=""></p>
<p>从梯度计算来看，$x_l$的梯度与l+1到L的layer都有关，$\frac{\partial \epsilon}{\partial x_L}$保证梯度会从L穿回到l，而且梯度不会消失，因为$\frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}F(x_i, W_i)$很少为-1。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo5n89qljvj318e0v4tjb.jpg" alt=""></p>
<p><strong>Identity Skip Connection</strong></p>
<p>主要是印证当shortcut不取identity mapping时，效果为什么不好</p>
<ul>
<li>Scaling, $h(x)=\lambda x$</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo5yn3n5gsj30eo03o0sw.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fo5ynmkwmyj30k803uwev.jpg" alt=""></p>
<p>和原来的梯度相比，从1变成了$\prod_{i=l}^{L-1}\lambda_i$，分类讨论，如果$\lambda_i$都大于1，那么如果层数深，梯度爆炸！如果$\lambda_i$都小于1，那么那项就会很小，梯度弥散！堵塞了shortcut。结果见Table 1，容易发现，结果反而变差了！</p>
<ul>
<li><p>Exclusive gating</p>
<p>gating函数， $g(x)=\sigma(W_gx+b_g)$, $(1-g(x)) \times x + g(x)\times F(x)$,在卷积网络中，g(x)是1x1的卷积。</p>
<p>结果依然不如baseline，结果见Table 1。当g(x)=0的时候，近似于identity，但是又抑制了F(x)</p>
</li>
</ul>
<ul>
<li>Shortcut-only gating</li>
</ul>
<p>$(1-g(x))\times x+F(x)$，当$b_g$负的特别多的时候，例如-6，g(x)非常接近0的时候，近似于identity mapping，结果也比exclusive gating更好，更接近baseline，见Table 1。</p>
<ul>
<li>1x1 conv shortcut</li>
</ul>
<p>用1x1的conv代替identity，当深度加深的时候，效果会变差，见Table 1。</p>
<ul>
<li>Dropout shortcut</li>
</ul>
<p>对identity做dropout，类似于做scale，结果也不理想。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fo5yt8ozenj30ye0iy0x5.jpg" alt=""></p>
<p>总结一下：</p>
<p>上述4种对shortcuts的修改，在实验结果上看，都不如identity mapping，可能的原因是影响了梯度的回传！</p>
<p><strong>The impact of f</strong></p>
<p>在原始的ResNet是ReLU，作者探讨了激活函数可能放置的各种方案，见Figure 4</p>
<ol>
<li>f = ReLU</li>
<li>f = BN + ReLU，在CIFAR-10上的训练曲线见Figure 6，加了BN反而结果变差了！。</li>
<li>ReLU放在加前，会导致F(x)的输入都大于等于0，而残差应该是在负无穷和正无穷之间</li>
<li>非对称化，$x_{l+1}=x_l+F(\hat{f}(x_l),W_l)$，训练曲线见Figure 6，体现在训练时，收敛率更高，测试的错误率也更低。</li>
</ol>
<p>在CIFAR-10数据集上的对比结果见Table 2，可以发现，Figure 4(e)的方法错误率最低！</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo76e6t07mj30ys0kawic.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo76u1vr05j30yi0i20yf.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo776l9s7hj31kw0hytdr.jpg" alt=""></p>
<h5 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h5><p>在ImageNet,CIFAR-10,CIFAR-100这3个数据集上做了对比试验，都取得了state-of-the-art的结果，见Table 4,5。</p>
<p>在深层网络中，体现出了pre-act的作用！</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo77epwjbgj31900ss10h.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fo76t0q108j30ym0gagqg.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/01/Identity-Mappings-in-Deep-Residual-Networks笔记/#more" class="btn btn-default more">Read More</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
    
     <a href="/" type="button" class="btn btn-default"><i class="fa fa-arrow-circle-o-left"></i> Prev</a>
      

        <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
 
       <a href="/page/3/" type="button" class="btn btn-default ">Next<i class="fa fa-arrow-circle-o-right"></i></a>     
        

  
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/algorithms/">algorithms<span>26</span></a></li>
		
			<li><a href="/categories/competition/">competition<span>1</span></a></li>
		
			<li><a href="/categories/summary/">summary<span>1</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/RNN/">RNN<span>1</span></a></li>
		
			<li><a href="/tags/杂/">杂<span>1</span></a></li>
		
			<li><a href="/tags/classification/">classification<span>2</span></a></li>
		
			<li><a href="/tags/notes/">notes<span>7</span></a></li>
		
			<li><a href="/tags/GAN/">GAN<span>1</span></a></li>
		
			<li><a href="/tags/machine-learning/">machine learning<span>2</span></a></li>
		
			<li><a href="/tags/math/">math<span>2</span></a></li>
		
			<li><a href="/tags/summary/">summary<span>2</span></a></li>
		
			<li><a href="/tags/segmentation/">segmentation<span>2</span></a></li>
		
			<li><a href="/tags/paper/">paper<span>1</span></a></li>
		
			<li><a href="/tags/object-detection/">object detection<span>11</span></a></li>
		
			<li><a href="/tags/face-detection/">face detection<span>1</span></a></li>
		
			<li><a href="/tags/classifcation/">classifcation<span>8</span></a></li>
		
			<li><a href="/tags/computer-vision/">computer vision<span>4</span></a></li>
		
			<li><a href="/tags/deep-learning/">deep learning<span>33</span></a></li>
		
			<li><a href="/tags/paper-notes/">paper notes<span>4</span></a></li>
		
			<li><a href="/tags/Deep-Learning/">Deep Learning<span>1</span></a></li>
		
			<li><a href="/tags/classfication/">classfication<span>3</span></a></li>
		
			<li><a href="/tags/algorithms/">algorithms<span>2</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2018/05/07/detection-algorithms-overview/" ><i class="fa fa-file-o"></i>detection algorithms overview</a>
      </li>
    
      <li>
        <a href="/2018/05/07/正则化的理解/" ><i class="fa fa-file-o"></i>正则化的理解</a>
      </li>
    
      <li>
        <a href="/2018/05/02/R-FCN笔记/" ><i class="fa fa-file-o"></i>R-FCN笔记</a>
      </li>
    
      <li>
        <a href="/2018/04/21/FPN论文笔记/" ><i class="fa fa-file-o"></i>FPN论文笔记</a>
      </li>
    
      <li>
        <a href="/2018/04/21/RON论文笔记/" ><i class="fa fa-file-o"></i>RON论文笔记</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/mowayao" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.weibo.com/mowayao" title="My weibo account." target="_blank"]);">My Weibo</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 Mowayao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$'], ['\[','\]'] ], 
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
   </html>
