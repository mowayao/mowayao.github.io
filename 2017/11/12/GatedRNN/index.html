
<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>GatedRNN | Mowayao&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Gated RNNRNN h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime) 下面是RNN的实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515">
<meta name="keywords" content="RNN,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="GatedRNN">
<meta property="og:url" content="http://wulimengmeng.top/2017/11/12/GatedRNN/index.html">
<meta property="og:site_name" content="Mowayao&#39;s Blog">
<meta property="og:description" content="Gated RNNRNN h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime) 下面是RNN的实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515">
<meta property="og:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.11.44.png">
<meta property="og:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.13.12.png">
<meta property="og:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.15.10.png">
<meta property="og:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.18.36.png">
<meta property="og:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.23.59.png">
<meta property="og:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.19.17.png">
<meta property="og:updated_time" content="2017-11-12T05:41:58.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GatedRNN">
<meta name="twitter:description" content="Gated RNNRNN h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime) 下面是RNN的实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515">
<meta name="twitter:image" content="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.11.44.png">
  
    <link rel="alternative" href="/atom.xml" title="Mowayao&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>
<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Mowayao&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一往无前虎山行</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="wulimengmeng.top">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main"><article id="blog-GatedRNN" class="article article-type-blog" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/11/12/GatedRNN/" class="article-date">
  <time datetime="2017-11-12T05:35:24.000Z" itemprop="datePublished">2017-11-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      GatedRNN
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Gated-RNN"><a href="#Gated-RNN" class="headerlink" title="Gated RNN"></a>Gated RNN</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><script type="math/tex; mode=display">
h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime)</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.11.44.png" alt="RNN"></p>
<p>下面是RNN的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></div><div class="line"><span class="string">  activation function.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for this timestep, of shape (N, D).</span></div><div class="line"><span class="string">  - prev_h: Hidden state from previous timestep, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h = np.tanh(x.dot(Wx)+prev_h.dot(Wh)+b)</div><div class="line">  cache = (next_h,x,prev_h,Wx,Wh)</div><div class="line">  <span class="keyword">return</span> next_h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for a single timestep of a vanilla RNN.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dnext_h: Gradient of loss with respect to next hidden state</span></div><div class="line"><span class="string">  - cache: Cache object from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradients of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></div><div class="line"><span class="string">  - db: Gradients of bias vector, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  next_h,x,prev_h,Wx,Wh = cache</div><div class="line">  dout = dnext_h*(<span class="number">1</span>-next_h**<span class="number">2</span>)</div><div class="line">  db = np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">  dx = dout.dot(Wx.T)</div><div class="line">  dprev_h = dout.dot(Wh.T)</div><div class="line">  dWx = np.dot(x.T,dout)</div><div class="line">  dWh = np.dot(prev_h.T,dout)</div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the RNN forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for the entire timeseries, of shape (N, T, D).</span></div><div class="line"><span class="string">  - h0: Initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></div><div class="line"><span class="string">  - cache: Values needed in the backward pass</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  N,T,D = x.shape</div><div class="line">  N,H = h0.shape</div><div class="line">  cache = []</div><div class="line">  prev_h = h0</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">    prev_h,cache_n = rnn_step_forward(x[:,t,:],prev_h,Wx,Wh,b)</div><div class="line">    cache.append(cache_n)</div><div class="line">    h[:,t,:] = prev_h</div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of all hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of inputs, of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  N,T,H = dh.shape</div><div class="line">  N,D = cache[<span class="number">0</span>][<span class="number">1</span>].shape</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dWx = np.zeros((D,H))</div><div class="line">  dWh = np.zeros((H,H))</div><div class="line">  db = np.zeros((H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">    dx[:,t,:],dprev_h, dWx_n, dWh_n, db_n = rnn_step_backward(dprev_h+dh[:,t,:],cache[t])</div><div class="line">    dWh += dWh_n</div><div class="line">    dWx += dWx_n</div><div class="line">    db += db_n</div><div class="line">  dh0 = dprev_h</div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure>
<h3 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h3><script type="math/tex; mode=display">
h^\prime, y = f_1(h,x)\\b^\prime, c = f_2(b, y)\\....</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.13.12.png" alt="Deep RNN"></p>
<h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><script type="math/tex; mode=display">
h^\prime, a = f_1(h, x), b^\prime, c= f_2(b, x), y = f_3(a, c)</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.15.10.png" alt="双向RNN"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><script type="math/tex; mode=display">
z^i = \tanh(W^ix^t+W^ih^{t-1})\\
z^f=\tanh(W^fx^t+W^fh^{t-1})\\
z^o=\tanh(W^0x^t+W^oh^{t-1})\\</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.18.36.png" alt="LSTM"></p>
<p>对比分析：</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.23.59.png" alt="LSTM对比"></p>
<p>最左边的是标准的LSTM， 左边第二个是GRU， </p>
<p>可以看出： 没有output gate，forget gate, input gate, input activation function, output activation function都会对结果变差。forget gate和关于$c^t$的$\tanh$激活函数对性能影响较大。</p>
<p>下面是LSTM的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for a single timestep of an LSTM.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data, of shape (N, D)</span></div><div class="line"><span class="string">  - prev_h: Previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - prev_c: previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases, of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - next_c: Next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h, next_c, cache = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for a single timestep of an LSTM.        #</span></div><div class="line">  <span class="comment"># You may want to use the numerically stable sigmoid implementation above.  #</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  a = x.dot(Wx)+prev_h.dot(Wh)+b</div><div class="line">  N,H = prev_h.shape</div><div class="line">  i = sigmoid(a[:,:H])</div><div class="line">  f = sigmoid(a[:,H:<span class="number">2</span>*H])</div><div class="line">  o = sigmoid(a[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div><div class="line">  g = np.tanh(a[:,<span class="number">3</span>*H:])</div><div class="line"></div><div class="line">  next_c = f*prev_c + i*g</div><div class="line">  next_h = o*np.tanh(next_c)</div><div class="line">  cache = (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h)</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> next_h, next_c, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass foLSTM: forward</span></div><div class="line"><span class="string">  - dnext_h: Gradients of next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dnext_c: Gradients of next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dprev_c: Gradient of previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dprev_c, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h) = cache</div><div class="line">  (N,H) = dnext_h.shape</div><div class="line">  (N,D) = x.shape</div><div class="line"></div><div class="line"></div><div class="line">  dx = np.zeros(x.shape)</div><div class="line">  dprev_c = np.zeros(prev_c.shape)</div><div class="line">  dprev_h = np.zeros(prev_h.shape)</div><div class="line">  dWx = np.zeros(Wx.shape)</div><div class="line">  dWh = np.zeros(Wh.shape)</div><div class="line">  db = np.zeros(b.shape)</div><div class="line"></div><div class="line"></div><div class="line">  di = dnext_c*g</div><div class="line">  df = dnext_c*prev_c</div><div class="line">  do = dnext_h*np.tanh(next_c)</div><div class="line">  dg = dnext_c*i</div><div class="line"></div><div class="line">  da = np.zeros(a.shape)</div><div class="line"></div><div class="line">  da[:,:H] = di*i*(<span class="number">1</span>-i) <span class="comment">#i</span></div><div class="line">  da[:,H:<span class="number">2</span>*H] = df*f*(<span class="number">1</span>-f) <span class="comment">#f</span></div><div class="line">  da[:,<span class="number">2</span>*H:<span class="number">3</span>*H] = do*o*(<span class="number">1</span>-o) <span class="comment">#o</span></div><div class="line">  da[:,<span class="number">3</span>*H:] = dg*(<span class="number">1</span>-g**<span class="number">2</span>) <span class="comment">#g</span></div><div class="line"></div><div class="line">  dprev_h = np.dot(da,Wh.T)</div><div class="line">  dWx = np.dot(x.T,da)</div><div class="line">  dWh = np.dot(prev_h.T,da)</div><div class="line">  db = np.sum(da,axis=<span class="number">0</span>)</div><div class="line">  dprev_c = dnext_c*f</div><div class="line">  dx = np.dot(da,Wx.T)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for an LSTM over an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the LSTM forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Note that the initial cell state is passed as input, but the initial cell</span></div><div class="line"><span class="string">  state is set to zero. Also note that the cell state is not returned; it is</span></div><div class="line"><span class="string">  an internal variable to the LSTM and is not accessed from outside.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - h0: Initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,T,D) = x.shape</div><div class="line">  (N,H) = h0.shape</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  cache = []</div><div class="line">  prev_c = np.zeros((N,H))</div><div class="line">  prev_h = h0</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">      prev_h,prev_c,cache_n = lstm_step_forward(x[:,t,:],prev_h,prev_c,Wx,Wh,b)</div><div class="line">      cache.append(cache_n)</div><div class="line">      h[:,t,:] = prev_h</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for an LSTM over an entire sequence of data.]</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,D) = cache[<span class="number">0</span>][<span class="number">0</span>].shape</div><div class="line">  (N,T,H) = dh.shape</div><div class="line"></div><div class="line">  dprev_c = np.zeros((N,H))</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dh0 = np.zeros((N,H))</div><div class="line">  dWx = np.zeros((D,<span class="number">4</span>*H))</div><div class="line">  dWh = np.zeros((H,<span class="number">4</span>*H))</div><div class="line">  db= np.zeros((<span class="number">4</span>*H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line"></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">      dx_n, dprev_h, dprev_c, dWx_n, dWh_n, db_n = lstm_step_backward(dh[:,t,:]+dprev_h,dprev_c,cache[t])</div><div class="line">      dWx += dWx_n</div><div class="line">      dWh_n += dWh_n</div><div class="line">      db += db_n</div><div class="line">      dx[:,t,:] = dx_n</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>z在GRU充当的是LSTM里面forget gate和input gate一样的作用，将两者耦合在一起。</p>
<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.19.17.png" alt="GRU"></p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://wulimengmeng.top/2017/11/12/GatedRNN/" data-id="cjadm77la0004vr0n1exr9i1f" class="article-share-link" data-share="baidu" data-title="GatedRNN">Share</a>
      

      
        <a href="http://wulimengmeng.top/2017/11/12/GatedRNN/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/11/12/meet-in-the-middle的一些实例/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Meet in the middle的一些实例
        
      </div>
    </a>
  
  
    <a href="/2017/11/01/mixup/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">mixup-Beyond Empirical Risk Minimization</div>
    </a>
  
</nav>

  
</article>


  <section id="comments">
    <div id="ds-thread" class="ds-thread" data-thread-key="2017/11/12/GatedRNN/" data-title="GatedRNN" data-url="http://wulimengmeng.top/2017/11/12/GatedRNN/"></div>
  </section>
</section>
      
      <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithms/">algorithms</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/">deep learning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/face-detection/">face detection</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notes/">notes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper/">paper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper-notes/">paper notes</a><span class="tag-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/algorithms/" style="font-size: 10px;">algorithms</a> <a href="/tags/deep-learning/" style="font-size: 20px;">deep learning</a> <a href="/tags/face-detection/" style="font-size: 10px;">face detection</a> <a href="/tags/notes/" style="font-size: 10px;">notes</a> <a href="/tags/paper/" style="font-size: 10px;">paper</a> <a href="/tags/paper-notes/" style="font-size: 20px;">paper notes</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/11/24/capsule/">Introduction to Capsule Network</a>
          </li>
        
          <li>
            <a href="/2017/11/21/FIXING-WEIGHT-DECAY-REGULARIZATION-IN-ADAM/">Fixing  Weight Decay Regularization in Adam</a>
          </li>
        
          <li>
            <a href="/2017/11/14/Networks/">Dual Path Networks</a>
          </li>
        
          <li>
            <a href="/2017/11/12/meet-in-the-middle的一些实例/">Meet in the middle的一些实例</a>
          </li>
        
          <li>
            <a href="/2017/11/12/GatedRNN/">GatedRNN</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mowayao<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script src="/js/script.js"></script>

</div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
