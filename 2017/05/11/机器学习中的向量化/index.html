<!DOCTYPE html>
<html lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="stylesheet" href="/style/style.css">
<script>
    var NlviConfig = {
        title: "Mowayao's Blog",
        author: "Mowayao",
        theme: "banderole",
        lightbox: true,
        animate: true,
        baseUrl: "/",
        search: true,
        friends: false
    }
</script>



    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">









    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="browsermode" content="application">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Mowayao's Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name= "format-detection" content="telephone=no" />
<meta name="keywords" content="nlvi, Nlvi" />


  <meta name="subtitle" content="一往无前虎山行">




  <meta name="keywords" content="notes, machine learning, Nlvi" />

  <title> 机器学习中的向量化 · Mowayao's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="progress">
  <div class="progress-inner"></div>
</div>
  
  
    <div class="tagcloud-mask"></div>  
<div class="tagcloud" id="tagcloud">
  <div class="tagcloud-inner">
    <a href="/tags/DP/" style="font-size: 14px;">DP</a> <a href="/tags/Deep-Learning/" style="font-size: 14px;">Deep Learning</a> <a href="/tags/GAN/" style="font-size: 14px;">GAN</a> <a href="/tags/Leetcode/" style="font-size: 14px;">Leetcode</a> <a href="/tags/RNN/" style="font-size: 14px;">RNN</a> <a href="/tags/algorithms/" style="font-size: 14px;">algorithms</a> <a href="/tags/classfication/" style="font-size: 14px;">classfication</a> <a href="/tags/classifcation/" style="font-size: 14px;">classifcation</a> <a href="/tags/classification/" style="font-size: 14px;">classification</a> <a href="/tags/computer-vision/" style="font-size: 14px;">computer vision</a> <a href="/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/tags/detection/" style="font-size: 14px;">detection</a> <a href="/tags/face-detection/" style="font-size: 14px;">face detection</a> <a href="/tags/fine-grained-classification/" style="font-size: 14px;">fine-grained classification</a> <a href="/tags/leetcode/" style="font-size: 14px;">leetcode</a> <a href="/tags/machine-learning/" style="font-size: 14px;">machine learning</a> <a href="/tags/math/" style="font-size: 14px;">math</a> <a href="/tags/notes/" style="font-size: 14px;">notes</a> <a href="/tags/object-detection/" style="font-size: 14px;">object detection</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/paper-notes/" style="font-size: 14px;">paper notes</a> <a href="/tags/segmentation/" style="font-size: 14px;">segmentation</a> <a href="/tags/speech/" style="font-size: 14px;">speech</a> <a href="/tags/summary/" style="font-size: 14px;">summary</a> <a href="/tags/杂/" style="font-size: 14px;">杂</a>
  </div>
</div>  
  

  <div class="container">

    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <span><a href="/">Mowayao's Blog</a></span>
    
      <span id="subtitle">一往无前虎山行</span>
    
  </div>
</div>
    <nav class="main-nav">
        
  <ul class="main-nav-list syuanpi tvIn">
  
    <li class="menu-item">
      <a href="javascript:;" id="search">
        <span>Search</span>
        
          <span class="menu-item-label">search</span>
        
      </a>
    </li>
  
  
    
      
    
    <li class="menu-item">
      <a href="/" id="article">
        <span class="base-name">Article</span>
        
          <span class="menu-item-label">article</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">Archives</span>
        
          <span class="menu-item-label">archives</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">Tags</span>
        
          <span class="menu-item-label">tags</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">About</span>
        
          <span class="menu-item-label">about</span>
        
      </a>
    </li>  
  
    
      
    
    <li class="menu-item">
      <a href="/atom.xml" id="RSS">
        <span class="base-name">RSS</span>
        
          <span class="menu-item-label">RSS</span>
        
      </a>
    </li>  
  
  </ul>
  
</nav>
    
    
  </div>
</header>
<div class="mobile-header">
  <div class="mobile-header-body">
    <div class="mobile-header-list">
      
        
            <div class="mobile-nav-item">
                <a href="/">
                    <span>Article</span>
                    
                    
                </a>
            </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/archives">
                    <span>Archives</span>
                    
                    
                </a>
            </div>
        
      
        
          <div class="mobile-nav-item inner-cloud">
            <div class="mobile-nav-tag">
              <a href="javascript:;" id="mobile-tags">
                <span>Tags</span>
                
                
              </a>
            </div>
            <div class="mobile-nav-tagcloud">
              <div class="mobile-tagcloud-inner">
                <a href="/tags/DP/" style="font-size: 14px;">DP</a> <a href="/tags/Deep-Learning/" style="font-size: 14px;">Deep Learning</a> <a href="/tags/GAN/" style="font-size: 14px;">GAN</a> <a href="/tags/Leetcode/" style="font-size: 14px;">Leetcode</a> <a href="/tags/RNN/" style="font-size: 14px;">RNN</a> <a href="/tags/algorithms/" style="font-size: 14px;">algorithms</a> <a href="/tags/classfication/" style="font-size: 14px;">classfication</a> <a href="/tags/classifcation/" style="font-size: 14px;">classifcation</a> <a href="/tags/classification/" style="font-size: 14px;">classification</a> <a href="/tags/computer-vision/" style="font-size: 14px;">computer vision</a> <a href="/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/tags/detection/" style="font-size: 14px;">detection</a> <a href="/tags/face-detection/" style="font-size: 14px;">face detection</a> <a href="/tags/fine-grained-classification/" style="font-size: 14px;">fine-grained classification</a> <a href="/tags/leetcode/" style="font-size: 14px;">leetcode</a> <a href="/tags/machine-learning/" style="font-size: 14px;">machine learning</a> <a href="/tags/math/" style="font-size: 14px;">math</a> <a href="/tags/notes/" style="font-size: 14px;">notes</a> <a href="/tags/object-detection/" style="font-size: 14px;">object detection</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/paper-notes/" style="font-size: 14px;">paper notes</a> <a href="/tags/segmentation/" style="font-size: 14px;">segmentation</a> <a href="/tags/speech/" style="font-size: 14px;">speech</a> <a href="/tags/summary/" style="font-size: 14px;">summary</a> <a href="/tags/杂/" style="font-size: 14px;">杂</a>
              </div>
            </div>
          </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/about">
                    <span>About</span>
                    
                    
                </a>
            </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/atom.xml">
                    <span>RSS</span>
                    
                    
                </a>
            </div>
        
      
    </div>
  </div>
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <span class="header-menu-line"></span>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">Mowayao's Blog</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
</div>
    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  
    <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>
  
  <article class="
  post
   is_post 
  ">
    <header class="post-header">
      <div class="post-time syuanpi riseIn-light back-1">
        <div class="post-time-wrapper">
          <span>2017-05-11</span>
          
          
            
              <aside class="post-tags syuanpi riseIn-light back-3">
              
                <a href="/tags/notes/">notes</a>
              
                <a href="/tags/machine-learning/">machine learning</a>
              
              </aside>
            
          
        </div>
      </div>
      <h1 class="post-title syuanpi riseIn-light back-2">
        
          机器学习中的向量化
        
      </h1>
    </header>
    <div class="post-content syuanpi riseIn-light back-3">
      
        <h2>KNN</h2>
<p>以KNN算法为例，在训练过程中，也就是计算$X_{training}$和$X_{test}$的之间的距离的时候，可以用向量化，也就是矩阵运算加速，这里我们假设 $X_{train}\in R^{n\times d}$ ,$X_{test}\in R^{m\times d}$,那么我们就先需要计算一个$n\times m$ 的矩阵，最朴素的做法就是：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">num_test = X.shape[<span class="number">0</span>]</div><div class="line">num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">dists = np.zeros((num_test, num_train))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_train):</div><div class="line">        dists[i, j] = np.sqrt(np.sum(np.square(X[i]-self.X_train[j])))</div><div class="line">        <span class="keyword">return</span> dists</div></pre></td></tr></table></figure></p>
<p>如果用向量化呢？</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">num_test = X.shape[<span class="number">0</span>]</div><div class="line">num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">dists = np.sqrt(<span class="number">-2</span>*np.dot(X, self.X_train.T) + np.sum(np.square(self.X_train), axis=<span class="number">1</span>) + np.sum(np.square(X), axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<p>推导的过程是这样的：</p>
<p>对于第i个$X_{test}$和第j个$X_{train}$的距离来说，可以先将平方展开，可以发现由三部分组成，分别是$X_{test}$的平方，$X_{train}$的平方，两者的乘积。对于第三部分的分析比较简单就是$X_{train}X_{test}^T$, 而对于前面两部分的分析其实就是对于第二个维度求和，然后加到目标矩阵相应的维度即可。</p>
<p>看一下时间的对比：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Let's compare how fast the implementations are</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_function</span><span class="params">(f, *args)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Call a function f with args and return the time (in seconds) that it took to execute.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">import</span> time</div><div class="line">    tic = time.time()</div><div class="line">    f(*args)</div><div class="line">    toc = time.time()</div><div class="line">    <span class="keyword">return</span> toc - tic</div><div class="line"></div><div class="line">two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)</div><div class="line">print(<span class="string">'Two loop version took %f seconds'</span> % two_loop_time)</div><div class="line">no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)</div><div class="line">print(<span class="string">'No loop version took %f seconds'</span> % no_loop_time)</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Two loop version took 21.228456 seconds</div><div class="line">No loop version took 0.182820 seconds</div></pre></td></tr></table></figure></p>
<p>提升非常明显</p>
<h2>Multi-class Support Vector Machine</h2>
<p>那么它的loss function是：
$$
Li=\sum_{j≠yi}\max(0,s_j−s_{yi}+\Delta)
$$
$s_j = f(x_i, W)_j$, 这个loss function(hinge loss)其实保证的是label的score是最大的，否则不存在loss，还有梯度。</p>
<p>再转换一下：
$$
L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$
<img src="http://cs231n.github.io/assets/margin.jpg" alt=""></p>
<p>那么对于$X_{train}\in R^{N\times D}$, $W\in R^{D\times M}$，它的loss和梯度计算过程(朴素方法)如下：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Structured SVM loss function, naive implementation (with loops).</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></div><div class="line"><span class="string">  of N examples.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></div><div class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></div><div class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></div><div class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">  - reg: (float) regularization strength</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - loss as single float</span></div><div class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></div><div class="line"></div><div class="line">  <span class="comment"># compute the loss and the gradient</span></div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">    scores = X[i].dot(W)</div><div class="line">    correct_class_score = scores[y[i]]</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">      <span class="keyword">if</span> j == y[i]:</div><div class="line">        <span class="keyword">continue</span></div><div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div><div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">        loss += margin</div><div class="line"></div><div class="line">  <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></div><div class="line">  <span class="comment"># to be an average instead so we divide by num_train.</span></div><div class="line">  loss /= num_train</div><div class="line"></div><div class="line">  <span class="comment"># Add regularization to the loss.</span></div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">    scores = X[i].dot(W)</div><div class="line">    correct_class_score = scores[y[i]]</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">      <span class="keyword">if</span> j == y[i]:</div><div class="line">        <span class="keyword">continue</span></div><div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div><div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">        dW[:, j] += X[i]</div><div class="line">        dW[:, y[i]] -= X[i]</div><div class="line">  dW /= num_train</div><div class="line">  dW += reg * W</div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure></p>
<p>向量化之后就可以这样：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Structured SVM loss function, vectorized implementation.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs and outputs are the same as svm_loss_naive.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  scores = (X.dot(W)).T</div><div class="line">  margins = np.maximum(<span class="number">0</span>, scores-scores[y, range(num_train)]+<span class="number">1</span>)</div><div class="line">  margins[y, range(num_train)] = <span class="number">0</span></div><div class="line">  loss += np.sum(margins) / num_train</div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</div><div class="line"></div><div class="line">  D = np.zeros_like(margins)</div><div class="line">  D[margins&gt;<span class="number">0</span>] = <span class="number">1</span></div><div class="line">  D[y, range(num_train)] = -np.sum(margins&gt;<span class="number">0</span>, axis=<span class="number">0</span>)</div><div class="line">  dW += np.dot(D, X).T</div><div class="line">  dW /= num_train</div><div class="line">  dW += reg  * W</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure></p>
<p>比较一下效率</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">tic = time.time()</div><div class="line">_, grad_naive = svm_loss_naive(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'Naive loss and gradient: computed in %fs'</span> % (toc - tic))</div><div class="line"></div><div class="line">tic = time.time()</div><div class="line">_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'Vectorized loss and gradient: computed in %fs'</span> % (toc - tic))</div><div class="line"></div><div class="line"><span class="comment"># The loss is a single number, so it is easy to compare the values computed</span></div><div class="line"><span class="comment"># by the two implementations. The gradient on the other hand is a matrix, so</span></div><div class="line"><span class="comment"># we use the Frobenius norm to compare them.</span></div><div class="line">difference = np.linalg.norm(grad_naive - grad_vectorized, ord=<span class="string">'fro'</span>)</div><div class="line">print(<span class="string">'difference: %f'</span> % difference)</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Naive loss and gradient: computed in 0.141285s</div><div class="line">Vectorized loss and gradient: computed in 0.007416s</div><div class="line">difference: 0.000000</div></pre></td></tr></table></figure></p>
<p>提升非常大</p>
<h2>Softmax</h2>
<p>softmax分类器用的是交叉熵（cross entropy）,有以下的形式：
$$
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}
$$
对这个这个loss函数，可以有两种解释：</p>
<p>信息论的视角，也就是真的分布p和预测分布q之间的交叉熵：
$$
H(p,q) = - \sum_x p(x) \log q(x)
$$
p是这样一个向量：只有一个元素是1（$y_i$的位置），其他全是0。q就是模型输出的分布，所以两者相乘，得到上面的结果</p>
<p>还有就是概率的解释，例如下面的表达式：
$$
P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }
$$
因此，我们就可以用最大似然估计（MLE）来求解，也就是最小化负的正确标签的log似然。</p>
<p>softmax函数定义了每个类别的概率估计。</p>
<p>朴素的求法：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></div><div class="line"><span class="string">  of N examples.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></div><div class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></div><div class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></div><div class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">  - reg: (float) regularization strength</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - loss as single float</span></div><div class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></div><div class="line"><span class="string">  """</span></div><div class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  dW = np.zeros_like(W)</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">    score = X[i].dot(W)</div><div class="line">    score -= np.max(score)</div><div class="line">    exp_score = np.exp(score)</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">      dW[:, j] += X[i] * exp_score[j] / np.sum(exp_score)</div><div class="line">    dW[:, y[i]] -= X[i]</div><div class="line">    loss += -score[y[i]] + np.log(np.sum(exp_score))</div><div class="line">  loss /= num_train</div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W*W)</div><div class="line"></div><div class="line">  dW /= num_train</div><div class="line">  dW += reg * W</div><div class="line"></div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure></p>
<p>向量化：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Softmax loss function, vectorized version.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></div><div class="line">  loss = <span class="number">0.0</span></div><div class="line">  dW = np.zeros_like(W)</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  scores = X.dot(W).T</div><div class="line">  scores -= np.max(scores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">  exp_scores = np.exp(scores)</div><div class="line">  loss += np.sum(-scores[y, range(num_train)]) + np.sum(np.log(np.sum(exp_scores, axis=<span class="number">0</span>)))</div><div class="line">  loss /= num_train</div><div class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W*W)</div><div class="line">  D = exp_scores / np.sum(exp_scores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">  D[y, range(num_train)] -= <span class="number">1.0</span></div><div class="line">  dW += D.dot(X).T</div><div class="line">  dW /= num_train</div><div class="line">  dW += reg * W</div><div class="line">  <span class="keyword">return</span> loss, dW</div></pre></td></tr></table></figure></p>
<p>对比一下：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Now that we have a naive implementation of the softmax loss function and its gradient,</span></div><div class="line"><span class="comment"># implement a vectorized version in softmax_loss_vectorized.</span></div><div class="line"><span class="comment"># The two versions should compute the same results, but the vectorized version should be</span></div><div class="line"><span class="comment"># much faster.</span></div><div class="line">tic = time.time()</div><div class="line">loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'naive loss: %e computed in %fs'</span> % (loss_naive, toc - tic))</div><div class="line"></div><div class="line"><span class="keyword">from</span> cs231n.classifiers.softmax <span class="keyword">import</span> softmax_loss_vectorized</div><div class="line">tic = time.time()</div><div class="line">loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, <span class="number">0.000005</span>)</div><div class="line">toc = time.time()</div><div class="line">print(<span class="string">'vectorized loss: %e computed in %fs'</span> % (loss_vectorized, toc - tic))</div><div class="line"></div><div class="line"><span class="comment"># As we did for the SVM, we use the Frobenius norm to compare the two versions</span></div><div class="line"><span class="comment"># of the gradient.</span></div><div class="line">grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord=<span class="string">'fro'</span>)</div><div class="line">print(<span class="string">'Loss difference: %f'</span> % np.abs(loss_naive - loss_vectorized))</div><div class="line">print(<span class="string">'Gradient difference: %f'</span> % grad_difference)</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">naive loss: 2.332690e+00 computed in 0.397665s</div><div class="line">vectorized loss: 2.332690e+00 computed in 0.007957s</div><div class="line">Loss difference: 0.000000</div><div class="line">Gradient difference: 0.000000</div></pre></td></tr></table></figure></p>
<p>效果依然显著</p>
<h2>Softmax和SVM的对比</h2>
<p><img src="http://cs231n.github.io/assets/svmvssoftmax.png" alt=""></p>
<p>其实两者性能上的差异非常小，对于SVM来说，如果正确的类的分数已经比其他类高了，那么它的loss为0，同时也没有梯度。而softmax则一直会有梯度，除非概率分布变成one-hot的形式且预测和标签相同。</p>
<blockquote>
<p>the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better.</p>
</blockquote>

      
    
    </div>
    
      
      
  <hr class="copy-line">
  <div class="post-copyright">
    <div class="copy-author">
      <span>Author :</span>
      <span>Mowayao</span>
    </div>
    <div class="copy-url">
      <span>Url :</span>
      <a href="http://wulimengmeng.top/2017/05/11/机器学习中的向量化/">http://wulimengmeng.top/2017/05/11/机器学习中的向量化/</a>
    </div>
    <div class="copy-origin">
      <span>Origin :</span>
      <a href="http://wulimengmeng.top">http://wulimengmeng.top</a>
    </div>
    <div class="copy-license">
      
      著作权归作者所有，转载请联系作者获得授权。
    </div>
  </div>

    
  </article>
  
    
  <nav class="article-page">
    
      <a href="/2017/05/12/cnn-case-study/" id="art-left" class="art-left">
        <span class="next-title">
          <i class="iconfont icon-left"></i>CNN Case Study
        </span>
      </a>
    
    
      <a href="/2017/03/18/C-测试cache大小/" id="art-right" class="art-right">
        <span class="prev-title"> 
          C++测试cache大小<i class="iconfont icon-right"></i>  
        </span>
      </a>
    
  </nav>

    
  <i id="com-switch" class="iconfont icon-more jumping-in long infinite" style="font-size:24px;display:block;text-align:center;transform:rotate(180deg);"></i>
  <div class="post-comments" id="post-comments" style="display: block;margin: auto 16px;">
    
<!-- Gitment Comments -->
<div id="gitment"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  owner: "mowayao",
  repo: 'mowayao.github.io',
  oauth: {
    client_id: '7855335a0336c8808ae8',
    client_secret: '5de12a86e607d102099f0c51b6ad7af573d65fff',
  },
})
gitment.render('gitment')
</script>

    
    

  </div>


  
   
    
  
  <aside class="post-toc">
    <span class="title" id="toc-switch"><span>Index</span></span>
    <div class="toc-inner syuanpi back-1 fallIn-light">
      <li class="title-link"><a href="javascript:;" class="toTop">机器学习中的向量化</a></li>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">KNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">Multi-class Support Vector Machine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">Softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">Softmax和SVM的对比</span></a></li></ol>
    </div>
  </aside>


  


        </div>
      </main>

      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
        
        
        
        
            <a href="https://www.zhihu.com/people/https://www.zhihu.com/people/yao-ze-ping" class="iconfont icon-zhihu" title="zhihu"></a>
        
        
        
        
    
        
            <a href="https://github.com/https://github.com/mowayao" class="iconfont icon-github" title="github"></a>
        
        
        
        
        
        
        
    
</div>
    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2018 ~ 2018</span>
        <span>❤</span>
        <span>Mowayao</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
    <div class="visit_count">
        <i class="iconfont icon-visit"></i>
        <span id="busuanzi_value_site_uv"></span>
        <i class="iconfont icon-people"></i>
        <span id="busuanzi_value_site_pv"></span>
    </div>
    
</div>
    </div>
  </div>
</footer>
    </div>
  </div>
  <script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


    <script src="/script/lib/lightbox/js/lightbox.min.js"></script>



    <script src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ["\\(","\\)"]]}});</script>



    
        <script src="/h.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/x.js"></script>
    
        <script src="/o.js"></script>
    
        <script src="/-.js"></script>
    
        <script src="/g.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/n.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/r.js"></script>
    
        <script src="/a.js"></script>
    
        <script src="/t.js"></script>
    
        <script src="/o.js"></script>
    
        <script src="/r.js"></script>
    
        <script src="/-.js"></script>
    
        <script src="/f.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/e.js"></script>
    
        <script src="/d.js"></script>
    


<script src="/script/src/nlvi.js"></script>
<script src="/script/src/utils.js"></script>
<script src="/script/scheme/balance.js"></script>
<script src="/script/src/plugins.js"></script>
<script src="/script/bootstarp.js"></script>


<div class="backtop syuanpi dead toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


  <div class="search" id="search">
    <div class="mask" id="mask"></div>
    <div class="search-wrapper syuanpi">
      <h2 id="search-header" class="syuanpi">搜索一下？</h2>
      <div class="input">
        <input type="text" id="local-search-input" results="0" name="">
      </div>
      <div id="local-search-result"></div>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
