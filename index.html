<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Mowayao&#39;s Blog</title>
  <meta name="author" content="Mowayao">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Mowayao&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110229492-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Mowayao&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class="fa fa-user"></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>一往无前虎山行<span class="blink-fast">∎</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">
      <i class="fa fa-heart blink-slow"></i>
      一往无前虎山行
</div>    
		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/09/Kaggle比赛小结-Camera-Model-Identification/" >Kaggle比赛小结-Camera Model Identification</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-09  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>https://www.kaggle.com/c/sp-society-camera-model-identification</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1foa5fw8h19j31k20483yy.jpg" alt=""></p>
<p>参加完比赛，做个简单的总结：</p>
<ul>
<li>数据增强：JPEG压缩，resizing，gamma修正</li>
<li>数据的不平衡需要设置采样权重</li>
<li>Random Crop，尽可能大，取512的结果好于256，先crop 1024，做完数据增强后再crop 512</li>
<li>网络模型，Dense201&gt;Dense161…..</li>
<li>Focal Loss可以提高结果</li>
<li>learning rate: 1e-4，optimizer: Adam，batch size: 16…...</li>
<li>ensemble</li>
</ul>
<p>可以考虑的点：</p>
<ul>
<li>因为验证集的结果还不错，可以将测试集用训练得到的模型做分类，再训练，做数据扩充</li>
<li>可以人工地下载更多的数据。。。</li>
<li>结合手工提取的特征做结果的修正，例如noise pattern:https://www.kaggle.com/zeemeen/i-have-a-clue-what-i-am-doing-noise-patterns</li>
</ul>

	
	</div>
  <a type="button" href="/2018/02/09/Kaggle比赛小结-Camera-Model-Identification/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/07/Inception-v4-Inception-ResNet-and-Inception-v4-笔记/" >Inception-v4, Inception-ResNet and(Inception v4)笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-07  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, AAAI 2017</p>
<p>链接：<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">https://arxiv.org/abs/1602.07261</a></p>
<p>作者: Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi</p>
<p>呃。。。这篇文章的主要贡献是了尝试了ResNet和Inception结合的多种可能性。。。。对Inception block进行修改和优化，提出了Inception v4（Figure 9），Inception-ResNet-v1和Inception-ResNet-v2（Figure 15），将ImageNet classification task的top-5 error刷到了3.08%。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobembpr0rj30ee0rcwhc.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdqhuy1oj30eo0de3zr.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobdr5dh4pj30ek0c6myf.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobdrmnbfwj30es0fitab.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdroxsyfj30fi0h2jt6.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobdsgyknpj30eg0m8wg5.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdss6xhkj30f40femyk.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobdsvxatrj30fg0j6dhi.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdt1ggqpj30ew0cc75o.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdtqs6ylj30g00jsdhj.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdu21ayyj30cw0kkgnb.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobdvixvgxj30ek0lewgk.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdvwhgcdj30ek0fe3zx.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdvyzlfvj30ek0j0myu.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobdw3cijkj30fi0c20u5.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fobdw6e7fqj30f20iiwg6.jpg" alt=""></p>
<p>实验发现：如果feature map的数量超过1000，网络会不work，训练的时候会不稳定，而且在早期就会“die”。所以在residual加个scaling，使得数值偏小，见Figure 20。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobe052il4j30s00o4q62.jpg" alt=""></p>
<h5 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h5><p>single crop和single model的比较结果见Table 2，可以发现Inception-ResNet-v2略胜一筹，但是和Inception-v4差距不大。</p>
<p>小数量的crop和single model的比较结果见Table 3，和Table 2的结果类似。</p>
<p>dense crop和single model的比较结果见Table 4，结果依然类似。</p>
<p>以及，crop的数量对结果影响还是很大的！</p>
<p>144 crop的模型ensemble后的比较结果见Table 5。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobe29be8mj30t60e6q5l.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobe24s4gmj30sq0e80vm.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fobe25u8vpj30tq0dmju6.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fobe291a2tj30uw0gu0wt.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/07/Inception-v4-Inception-ResNet-and-Inception-v4-笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/07/Rethinking-the-Inception-Architecture-for-Computer-Vision-Inception-V3-笔记/" >Rethinking the Inception Architecture for Computer Vision(Inception v3)笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-07  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Rethinking the Inception Architecture for Computer Vision, CVPR 2016</p>
<p>链接：<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1512.00567.pdf</a></p>
<p>作者: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna</p>
<ul>
<li>优化了Inception的结构</li>
<li>讨论了一些设计原则和可以优化的方向</li>
</ul>
<ul>
<li><p>在ILSVRC 2012 classification验证集上取得state-of-the-art的结果：21.2% top-1 error和5.6% top-5 error，ensemble后，取得3.5% top-5 error和17.3% top-1 error。</p>
<p>​</p>
</li>
</ul>
<h5 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h5><ul>
<li>为了避免表示瓶颈(representational bottlenecks)，feature map应该缓慢减小！</li>
<li>高维度的feature map能够更容易地处理局部信息（有更多的feature maps），在CNN中提高响应（融合不同感受野的卷积提取的特征）可以解耦更多特征，网络训练也更快</li>
<li>用1x1卷积做embeeding，在没有大量或者一些表现能力损失的基础上降低维度(局部空间的高相关性)。在做完spatial aggregation后用1x1卷积降低维度</li>
<li>网络的深度和宽度应该同时增加或者减少。这两者之间存在某种平衡</li>
</ul>
<h5 id="Factorizing-Convolutions-with-Large-Filter-Size"><a href="#Factorizing-Convolutions-with-Large-Filter-Size" class="headerlink" title="Factorizing Convolutions with Large Filter Size"></a>Factorizing Convolutions with Large Filter Size</h5><p>为了提高计算效率，可以将大的卷积核分解。</p>
<p><strong>Factorization into smaller convolutions</strong></p>
<p>例如5x5的卷积和3x3的卷积在其他条件相同的情况下，前者的计算量是后者的$\frac{25}{9}$。</p>
<p>用两个3x3的卷积代替5x5，两者具有相同的感受野，可以降低28%的计算量，而且经过实验证明，中间还是要有ReLU，而不是线性激活，见Figure 4和Figure 5</p>
<p><strong>Spatial Factorization into Asymmetric Convo- lutions</strong></p>
<p>分解成非对称的卷积，nx1的卷积。例如用3x1加1x3的两个卷积来代替3x3的卷积，这样的话可以降低33%的计算量。扩展一下，我们可以将nxn的卷积分解成1xn和nx1两个卷积，n越大，降低的计算量越大。</p>
<p>实际上，这样的分解在浅层效果并不好，只有在中层的时候效果不错，对于nxn的feature map来说，n一般从12到20，对于这些尺寸，7x1和1x7的小贵最好，见Figure 6。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo7ytt7rq5j30p60i2tab.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo7ytqofwjj30tc0mkjtw.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, pool_features)</span>:</span></div><div class="line">        super(InceptionA, self).__init__()</div><div class="line">        self.branch1x1 = BasicConv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">        self.branch5x5_1 = BasicConv2d(in_channels, <span class="number">48</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch5x5_2 = BasicConv2d(<span class="number">48</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</div><div class="line"></div><div class="line">        self.branch3x3dbl_1 = BasicConv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_2 = BasicConv2d(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_3 = BasicConv2d(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line"></div><div class="line">        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        branch1x1 = self.branch1x1(x)</div><div class="line"></div><div class="line">        branch5x5 = self.branch5x5_1(x)</div><div class="line">        branch5x5 = self.branch5x5_2(branch5x5)</div><div class="line"></div><div class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</div><div class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</div><div class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</div><div class="line"></div><div class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        branch_pool = self.branch_pool(branch_pool)</div><div class="line"></div><div class="line">        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]</div><div class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo7zsanjwbj30sq0t8dj2.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionC</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, channels_7x7)</span>:</span></div><div class="line">        super(InceptionC, self).__init__()</div><div class="line">        self.branch1x1 = BasicConv2d(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">        c7 = channels_7x7</div><div class="line">        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</div><div class="line">        self.branch7x7_3 = BasicConv2d(c7, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</div><div class="line">        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</div><div class="line">        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</div><div class="line">        self.branch7x7dbl_5 = BasicConv2d(c7, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</div><div class="line"></div><div class="line">        self.branch_pool = BasicConv2d(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        branch1x1 = self.branch1x1(x)</div><div class="line"></div><div class="line">        branch7x7 = self.branch7x7_1(x)</div><div class="line">        branch7x7 = self.branch7x7_2(branch7x7)</div><div class="line">        branch7x7 = self.branch7x7_3(branch7x7)</div><div class="line"></div><div class="line">        branch7x7dbl = self.branch7x7dbl_1(x)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)</div><div class="line">        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)</div><div class="line"></div><div class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        branch_pool = self.branch_pool(branch_pool)</div><div class="line"></div><div class="line">        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]</div><div class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fo80dcq1fhj30ri0q8dk2.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionE</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels)</span>:</span></div><div class="line">        super(InceptionE, self).__init__()</div><div class="line">        self.branch1x1 = BasicConv2d(in_channels, <span class="number">320</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">        self.branch3x3_1 = BasicConv2d(in_channels, <span class="number">384</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch3x3_2a = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">        self.branch3x3_2b = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        self.branch3x3dbl_1 = BasicConv2d(in_channels, <span class="number">448</span>, kernel_size=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_2 = BasicConv2d(<span class="number">448</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.branch3x3dbl_3a = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">        self.branch3x3dbl_3b = BasicConv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        self.branch_pool = BasicConv2d(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        branch1x1 = self.branch1x1(x)</div><div class="line"></div><div class="line">        branch3x3 = self.branch3x3_1(x)</div><div class="line">        branch3x3 = [</div><div class="line">            self.branch3x3_2a(branch3x3),</div><div class="line">            self.branch3x3_2b(branch3x3),</div><div class="line">        ]</div><div class="line">        branch3x3 = torch.cat(branch3x3, <span class="number">1</span>)</div><div class="line"></div><div class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</div><div class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</div><div class="line">        branch3x3dbl = [</div><div class="line">            self.branch3x3dbl_3a(branch3x3dbl),</div><div class="line">            self.branch3x3dbl_3b(branch3x3dbl),</div><div class="line">        ]</div><div class="line">        branch3x3dbl = torch.cat(branch3x3dbl, <span class="number">1</span>)</div><div class="line"></div><div class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        branch_pool = self.branch_pool(branch_pool)</div><div class="line"></div><div class="line">        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]</div><div class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h5 id="Auxiliary-Classifiers"><a href="#Auxiliary-Classifiers" class="headerlink" title="Auxiliary Classifiers"></a>Auxiliary Classifiers</h5><p>目的是为了让梯度回传更加有效，从而加快训练，而事实是辅助分类器在训练初期的时候对结果并没有特别大的帮助，在训练后期的时候会超上没有使用辅助分类器的模型。额外辅助器可以作为一个regularizer。</p>
<h5 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h5><p>在feature map深度加倍的时候，空间维度需要减半。对于一个k个dxd的feature map，主要有两种选项：</p>
<ol>
<li>首先进行步长为1的卷积，将深度加倍，然后加个pooling，这样的复杂度是$2d^2k^2$，见Figure 9。</li>
<li>扔掉pooling，也就是用一个卷积直接搞定，那么这样的复杂度是$2(\frac{d}{2})^2k^2$，变成了原来的四分之一，但是这样会有表达瓶颈。</li>
<li>用并行的步长为2卷积和pooling，见Figure 10。</li>
</ol>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo809hnvetj30sg0k6tbp.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo809udzjvj30sc0mg42c.jpg" alt=""></p>
<h5 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception-v3"></a>Inception-v3</h5><p>网络的配置见Table 1。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo80da87wrj30ra0vcagv.jpg" alt=""></p>
<p><strong>Label Smoothing</strong></p>
<p>为了防止模型预测的时候over-confident，用label smoothing来做一个正则化。</p>
<p>将label distribution从$q(k|x)=\delta_{k,y}$替换为$q(k|y)=(1-\epsilon)\delta_{k,y}+\epsilon\mu$</p>
<h5 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h5><p>首先针对不同的感受野做了实验，见Table2，299x299在保准计算效率的同时，有较高的准确率。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobcl3nkrlj30ra0a0dhh.jpg" alt=""></p>
<p>单模型比较结果见Table 3。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fobckztjttj30ra0sq7b3.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/07/Rethinking-the-Inception-Architecture-for-Computer-Vision-Inception-V3-笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/01/Identity-Mappings-in-Deep-Residual-Networks笔记/" >Identity Mappings in Deep Residual Networks笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-01  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Identity Mappings in Deep Residual Networks, ECCV 2016</p>
<p>链接：https://arxiv.org/pdf/1603.05027.pdf</p>
<p>作者: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</p>
<h5>Idea</h5>
<p>ResNet的每个unit可以表示为：
$$
y_l = h(x_l)+F(x_l,W_l) \ x_{l+1} = f(y_l)
$$
$x_l$是第l个unit的输入，$x_{l+1}$是l个单元的输出。在ResNet v1中，$h(x_l)=x_l$，表示identity mapping(做了3种尝试，选择了这种)，f是ReLU函数，F包含2-3个卷积层，中间还有BN和ReLU（见Figure 1(a))。如果h和f都是identity mapping的时候，<strong>信号可以直接从一个unit到另一个unit，无论是forward还是backward，这样可以使训练更加容易</strong>。</p>
<p>Forward:
$$
x_{l+1}=x_l+F(x_l,W_l)... \ x_L = x_l + \sum_{i=l}^{L+1}F(x_i,W_i)
$$</p>
<ul>
<li>第L层的特征$x_L$可以表示成$x_l+\sum_{i=l}^{L-1}F(x_i,W_i)$</li>
<li>$x_L=x_o+\sum_{i=0}^{L-1}F(x_i,W_i)$，可以由所有的残差函数相加得到，再加上$x_o$</li>
</ul>
<p>Backward:</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo5tv0mjxdj30my03mjrs.jpg" alt=""></p>
<p>从梯度计算来看，$x_l$的梯度与l+1到L的layer都有关，$\frac{\partial \epsilon}{\partial x_L}$保证梯度会从L穿回到l，而且梯度不会消失，因为$\frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}F(x_i, W_i)$很少为-1。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo5n89qljvj318e0v4tjb.jpg" alt=""></p>
<p><strong>Identity Skip Connection</strong></p>
<p>主要是印证当shortcut不取identity mapping时，效果为什么不好</p>
<ul>
<li>Scaling, $h(x)=\lambda x$</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo5yn3n5gsj30eo03o0sw.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fo5ynmkwmyj30k803uwev.jpg" alt=""></p>
<p>和原来的梯度相比，从1变成了$\prod_{i=l}^{L-1}\lambda_i$，分类讨论，如果$\lambda_i$都大于1，那么如果层数深，梯度爆炸！如果$\lambda_i$都小于1，那么那项就会很小，梯度弥散！堵塞了shortcut。结果见Table 1，容易发现，结果反而变差了！</p>
<ul>
<li>
<p>Exclusive gating</p>
<p>gating函数， $g(x)=\sigma(W_gx+b_g)$, $(1-g(x)) \times x + g(x)\times F(x)$,在卷积网络中，g(x)是1x1的卷积。</p>
<p>结果依然不如baseline，结果见Table 1。当g(x)=0的时候，近似于identity，但是又抑制了F(x)</p>
</li>
</ul>
<ul>
<li>Shortcut-only gating</li>
</ul>
<p>$(1-g(x))\times x+F(x)$，当$b_g$负的特别多的时候，例如-6，g(x)非常接近0的时候，近似于identity mapping，结果也比exclusive gating更好，更接近baseline，见Table 1。</p>
<ul>
<li>1x1 conv shortcut</li>
</ul>
<p>用1x1的conv代替identity，当深度加深的时候，效果会变差，见Table 1。</p>
<ul>
<li>Dropout shortcut</li>
</ul>
<p>对identity做dropout，类似于做scale，结果也不理想。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fo5yt8ozenj30ye0iy0x5.jpg" alt=""></p>
<p>总结一下：</p>
<p>上述4种对shortcuts的修改，在实验结果上看，都不如identity mapping，可能的原因是影响了梯度的回传！</p>
<p><strong>The impact of f</strong></p>
<p>在原始的ResNet是ReLU，作者探讨了激活函数可能放置的各种方案，见Figure 4</p>
<ol>
<li>f = ReLU</li>
<li>f = BN + ReLU，在CIFAR-10上的训练曲线见Figure 6，加了BN反而结果变差了！。</li>
<li>ReLU放在加前，会导致F(x)的输入都大于等于0，而残差应该是在负无穷和正无穷之间</li>
<li>非对称化，$x_{l+1}=x_l+F(\hat{f}(x_l),W_l)$，训练曲线见Figure 6，体现在训练时，收敛率更高，测试的错误率也更低。</li>
</ol>
<p>在CIFAR-10数据集上的对比结果见Table 2，可以发现，Figure 4(e)的方法错误率最低！</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo76e6t07mj30ys0kawic.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fo76u1vr05j30yi0i20yf.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo776l9s7hj31kw0hytdr.jpg" alt=""></p>
<h5>实验</h5>
<p>在ImageNet,CIFAR-10,CIFAR-100这3个数据集上做了对比试验，都取得了state-of-the-art的结果，见Table 4,5。</p>
<p>在深层网络中，体现出了pre-act的作用！</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fo77epwjbgj31900ss10h.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fo76t0q108j30ym0gagqg.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/01/Identity-Mappings-in-Deep-Residual-Networks笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/02/01/Deep-Residual-Learning-for-Image-Recognition笔记/" >Deep Residual Learning for Image Recognition笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-02-01  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Deep Residual Learning for Image Recognition, CVPR 2016</p>
<p>链接：https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</p>
<p>作者: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</p>
<ul>
<li>提出ResNet(Residual Net)</li>
<li>ILSVRC 2015的classfication task的冠军，达到3.57%的top-5 error rate(152层的模型)</li>
<li>在ILSVRC 2015和MS-COCO 2015中，横扫一切对手夺冠</li>
<li>和VGG相比，深度更深，准确率更高，计算复杂度更小！</li>
</ul>
<h5>Idea</h5>
<p>随着网络深度的增加，随之而来的就是网络的<strong>退化(degradation)</strong>，也就是训练和测试的error比浅层的网络还高，见Fig 1。这个问题并不是由overfitting造成的，但是从直觉上讲，更深的模型应该和浅层的模型表现相当，因为我们可以将浅层网络的参数拷贝，然后其余的层就是做identity mapping，但是为什么效果会更差呢？这是因为这是因为深度网络比浅层网络更难训，<strong>很难用多层的非线性层去优化近似identity mapping</strong>。因此，通过将问题转化为学习残差函数来解决网络退化的问题。x是输入，H(x)是底层的映射函数，也就是网络中堆叠的非线性层，$f(x)=H(x)-x$，等价于$H(x)=f(x)+x$。如果最优函数是identity mapping，那么整个学到的参数就会驱使$f(x)=0$(或者尽可能接近于0)。identity mapping不一定是最优的，但是可以帮助预处理问题，可以简化问题。<strong>作者也对此做了一个假设：优化残差映射比优化一个原始映射容易的多。</strong></p>
<blockquote>
<p>If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.</p>
</blockquote>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo2bil5j4xj30ts0foq6o.jpg" alt=""></p>
<h5>模型结构</h5>
<p><strong>short connection</strong>
$$
y = \mathbf{F}(x,{W_i})+x
$$
$\mathbf{F}(x,{W_i})$表示待学习的残差mapping。而后面+x这个运算可以通过shortcuts进行element-wise的加法。优点是不需要额外的参数，当维度不统一的时候，可以进行zero padding或者线性投影（虽然会引入少量的参数）使得两者的维度一致。
$$
y = \mathbf{F}(x,{W_i})+W_sx
$$
<img src="https://ws3.sinaimg.cn/large/006tNc79gy1fo2br70niwj30za0g0gnk.jpg" alt=""></p>
<p><strong>Bottleneck design</strong></p>
<p>在3x3卷积前后加入1x1卷积来降低和提高feature map的深度，实现feature map之间的线性组合，提高模型的非线性能力，同时还可以使得输入输出的维度保持一致，具体见Figure 5。</p>
<p>Bottleneck的结构也被应用到了50-layer,101-layer,152-layer的网络中。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo2c143x8uj30sk0fg0vd.jpg" alt=""></p>
<p>Table 1是各个ResNet的模型参数结构，可以发现18-layer和34-layer最终输出的feature map的深度是512，而其余的都是2048，这个由expansion ratio决定。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fo2bwdncg2j31kw0lhwkk.jpg" alt=""></p>
<h5>实验</h5>
<p><strong>ImageNet</strong></p>
<p>首先分别比较了18-layer和34-layer的plain network和residual network，见Figure 4和Table 2。总结如下：</p>
<ul>
<li>在plain network中发现网络退化的问题，18层的网络比34层的网络错误率更低，优化的困难不是由梯度弥散造成的，作者推测是因为<strong><em>深层网络的收敛率更低</em></strong></li>
<li>在ResNet中，情况正好相反</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo5lhucus7j31kw0jldmq.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fo2c4ar19tj30t20aqdhw.jpg" alt=""></p>
<p>然后，作者比较了identity shortcuts和projection shortcuts，结果见Table 4。A是zero-padding。B是维度不同用projection，相同用identity，而C则是所有都用projection</p>
<p>可以发现：</p>
<ul>
<li>B略好于A，可以理解就是通过zero-padding加到相同的维度并不是残差学习！</li>
<li>C好于B，但是C引入了大量的需要学习的参数</li>
<li>考虑到模型复杂度和计算复杂度，选择option B</li>
<li>ResNet 50, 101, 151中，随着深度变大，错误率越来越低</li>
</ul>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fo5lpzvxb1j30rw0m0432.jpg" alt=""></p>
<p>与其他state-of-the-art的方法相比，ResNet提升非常明显，见Table 4, 5！</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fo5lyc0rknj30s00jcn1h.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fo5lwnnsklj30sk0ecq63.jpg" alt=""></p>
<p><strong>CIFAR-10</strong></p>
<p>分析每一层的响应，响应值是通过卷积以后，在经过BN，ReLU以后的值，见Figure 7。可以发现，ResNet的响应值都会偏小，也基本印证了残差函数比非残差函数更偏向于0。除此之外，对于ResNet来说，深度越深，响应值也越小。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fo5m7fmpbnj30sc0jidl2.jpg" alt=""></p>
<p><strong>PASCAL VOC 2007/2012 &amp;&amp;COCO</strong></p>
<p>将faster rcnn和ResNet结合，用于做detection，结果见Table 7,8。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fo5mbkcg6tj30tm0aumzn.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fo5mbgcfvaj30sq0860uk.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/02/01/Deep-Residual-Learning-for-Image-Recognition笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/25/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift笔记/" >Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-25  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Batch Normalization: Accelerating Deep Network Training by Reducing Internal
Covariate Shift, ICML 2015</p>
<p>链接：http://proceedings.mlr.press/v37/ioffe15.pdf</p>
<p>作者: Sergey Ioffe，Christian Szegedy</p>
<h5>Idea</h5>
<p><strong>Internal Covariate Shift</strong></p>
<p>什么是Covariate Shift？</p>
<blockquote>
<p>Another assumption one can make about the connection between the source and the target domains is that given the same observation$X=x$, the conditional distributions of Y are the same in the two domains. However, the marginal distributions of X may be different in the source and the target domains. Formally, we assume that $ P_s(Y \vert X = x) = P_t(Y \vert X = x)$ for all $ x \in \mathcal{X}$! but $ P_s(X) \ne P_t(X)$. This difference between the two domains is called <em>covariate shift</em></p>
</blockquote>
<p>covariates其实就是输入特征，记做X，假设对任意的$x\in \mathcal{X}$,$P_s(Y|X=x)=P_t(Y|X=x)$，表示Y关于X的条件分布在source domain（训练集）和target domain（测试集）是一样的，但是$P_s(X)\neq P_t(X) $，两者的边缘分布是不一样的！这个问题经常发生在transfer learning中。从表面上看，对于分类问题，只要两者的条件分布相同，即使边缘分布不同，也不会影响分类的结果，但是，当模型是有参数的时候，也就是$P(Y|X,\theta)$的时候，我们需要选择最优的参数$\theta^*$来使得loss($\theta$)最小。在这个时候，如果$P_s(X)\neq P_t(X) $，也就打破了传统机器学习中训练集和测试集的数据是i.i.d（独立同分布）的，那么在target domain的最优模型会和source domain的不一样，因为参数的求解依赖于X的分布（似然函数）！
$$
P(\theta|D)=\frac{P(D|\theta)\times P(\theta)}{P(D)}
$$
Internal Covariate Shift表示的是在神经网络发生的convariate shift，因为网络参数在更新，在神经网络的特定层的输出分布就发生了改变，这就导致它的后面层需要去适应它分布的变化，这就降低了训练速度，影响了模型的训练效果。</p>
<p><strong>Vanishing Gradient</strong></p>
<p>一些容易饱和的非线性激活函数(tanh, sigmoid等)当输入很大的时候，梯度容易饱和。这使得模型在加深的时候，梯度容易消失！以及，ReLU的使用使得部分神经元“死掉”后不再有梯度传回去，随着网络的不断迭代，越来越多的神经元会“死掉”！所以这需要我们：</p>
<ul>
<li>调低学习率</li>
<li>谨慎的初始化</li>
<li>BN</li>
</ul>
<p><strong>Towards Reducing Internal Covariate Shift</strong></p>
<p>目的是在神经网训练过程中，固定layer inputs x的分布。</p>
<p>作者引用了一些研究的发现：</p>
<blockquote>
<p>Network converges faster when the inputs are <em>whitened</em> - that is, normalized to have zero mean, unit variance, and decorrelated (diagonal covariance).</p>
</blockquote>
<p>当网络的输入数据是白化的（均值为0，方差为1的高斯分布）和独立的，网络收敛会更快！但是对整个数据集做白化的话代价很大！需要计算协方差！</p>
<p>看似容易，其实并不好办，因为我们在考虑白话网络层输出的激活值的时候，需要直接修改网络或者优化算法的参数值来保证分布固定。</p>
<p>例如，某层的输入是u，加上bias b，得到输出u+b，再减掉均值做normalization, $x-E[x]$。这样做的后果就是bias会一直改变，而loss没有任何变化。</p>
<p>$u+(b+\Delta b)-E[u+(b+\Delta b)]=u+b-E[u+b]$</p>
<p>作者也经过实验发现：</p>
<blockquote>
<p>the model blows up when the normalization parameters are computed outside the gradient descent step</p>
</blockquote>
<p><strong>Batch Normalization</strong></p>
<p>所以，作者将每一层的输出在激活函数前做归一化（假设数据都是i.i.d）。利用均值和方差去归一化以后，还对数据做了平移放缩。所有的步骤都是可微的，所以是可以用bp优化的。</p>
<p>对于卷积层的BN，做法是将每个各自feature map归一化，例如feature map的大小为pxq，batch size为n，就是计算nxpxq的平均值和方差。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnsve61xlbj30sc0mg787.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fnxqbppxzbj30ja0b4myf.jpg" alt=""></p>
<p><strong>BN的好处</strong></p>
<ul>
<li>
<p>减少internal covariant shift，梯度爆炸，梯度消失，从而减少训练时间</p>
<ul>
<li>将输入的分布固定</li>
</ul>
</li>
<li>
<p>可以减少正则化的使用，例如l2正则，dropout等</p>
<ul>
<li>实验证明</li>
</ul>
</li>
<li>
<p>可以使用一些饱和的非线性函数，sigmoid等, 例如ReLU激活函数，有些neuron在不加BN的情况下已经死掉了，但是经过BN以后，还是会有梯度回传回去</p>
<ul>
<li>平移和缩放</li>
</ul>
</li>
<li>
<p>可以使用更高的学习率</p>
<p>BN(	Wu)=BN((aW)u),因为$\frac{\partial BN((aW)u)}{\partial u}=\frac{\partial BN(Wu)}{\partial u}$，以及$\frac{\partial BN((aW)u)}{\partial aW}=\frac{1}{a}\frac{\partial BN(Wu)}{\partial W}$。大的weight，反而是会让梯度更小，这样就可以使训练更加平稳。</p>
<p>​</p>
</li>
</ul>
<p><strong>代码实现：</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for batch normalization.</span></div><div class="line"><span class="string">  During training the sample mean and (uncorrected) sample variance are</span></div><div class="line"><span class="string">  computed from minibatch statistics and used to normalize the incoming data.</span></div><div class="line"><span class="string">  During training we also keep an exponentially decaying running mean of the mean</span></div><div class="line"><span class="string">  and variance of each feature, and these averages are used to normalize data</span></div><div class="line"><span class="string">  at test-time.</span></div><div class="line"><span class="string">  At each timestep we update the running averages for mean and variance using</span></div><div class="line"><span class="string">  an exponential decay based on the momentum parameter:</span></div><div class="line"><span class="string">  running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></div><div class="line"><span class="string">  running_var = momentum * running_var + (1 - momentum) * sample_var</span></div><div class="line"><span class="string">  Note that the batch normalization paper suggests a different test-time</span></div><div class="line"><span class="string">  behavior: they compute sample mean and variance for each feature using a</span></div><div class="line"><span class="string">  large number of training images rather than using a running average. For</span></div><div class="line"><span class="string">  this implementation we have chosen to use running averages instead since</span></div><div class="line"><span class="string">  they do not require an additional estimation step; the torch7 implementation</span></div><div class="line"><span class="string">  of batch normalization also uses running averages.</span></div><div class="line"><span class="string">  Input:</span></div><div class="line"><span class="string">  - x: Data of shape (N, D)</span></div><div class="line"><span class="string">  - gamma: Scale parameter of shape (D,)</span></div><div class="line"><span class="string">  - beta: Shift paremeter of shape (D,)</span></div><div class="line"><span class="string">  - bn_param: Dictionary with the following keys:</span></div><div class="line"><span class="string">    - mode: 'train' or 'test'; required</span></div><div class="line"><span class="string">    - eps: Constant for numeric stability</span></div><div class="line"><span class="string">    - momentum: Constant for running mean / variance.</span></div><div class="line"><span class="string">    - running_mean: Array of shape (D,) giving running mean of features</span></div><div class="line"><span class="string">    - running_var Array of shape (D,) giving running variance of features</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - out: of shape (N, D)</span></div><div class="line"><span class="string">  - cache: A tuple of values needed in the backward pass</span></div><div class="line"><span class="string">  """</span></div><div class="line">  mode = bn_param[<span class="string">'mode'</span>]</div><div class="line">  eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</div><div class="line">  momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</div><div class="line"></div><div class="line">  N, D = x.shape</div><div class="line">  running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</div><div class="line">  running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</div><div class="line"></div><div class="line">  out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div><div class="line">    mean = np.sum(x,axis=<span class="number">0</span>)/float(N)</div><div class="line">    x_mean = (x - mean)</div><div class="line">    sqr_x_mean = x_mean**<span class="number">2</span></div><div class="line">    var = np.sum(sqr_x_mean, axis=<span class="number">0</span>)/float(N)</div><div class="line">    sqrt_var = np.sqrt(var+eps)</div><div class="line">    inv_sqrt_var = <span class="number">1.0</span>/sqrt_var</div><div class="line">    x_hat = x_mean*inv_sqrt_var</div><div class="line">    out = gamma * x_hat + beta</div><div class="line">    cache = (x_hat,gamma,sqr_x_mean,mean,var,sqrt_var,x_mean,inv_sqrt_var)</div><div class="line"></div><div class="line">    running_mean = momentum*running_mean + (<span class="number">1.0</span>-momentum)*mean</div><div class="line">    running_var = momentum*running_var + (<span class="number">1.0</span>-momentum)*var</div><div class="line">   </div><div class="line">  <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div><div class="line"></div><div class="line">    x_hat = (x - running_mean)/np.sqrt(running_var+eps)</div><div class="line">    out = gamma * x_hat + beta</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</div><div class="line"></div><div class="line">  <span class="comment"># Store the updated running means back into bn_param</span></div><div class="line">  bn_param[<span class="string">'running_mean'</span>] = running_mean</div><div class="line">  bn_param[<span class="string">'running_var'</span>] = running_var</div><div class="line">  <span class="keyword">return</span> out, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for batch normalization.</span></div><div class="line"><span class="string">  For this implementation, you should write out a computation graph for</span></div><div class="line"><span class="string">  batch normalization on paper and propagate gradients backward through</span></div><div class="line"><span class="string">  intermediate nodes.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dout: Upstream derivatives, of shape (N, D)</span></div><div class="line"><span class="string">  - cache: Variable of intermediates from batchnorm_forward.</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient with respect to inputs x, of shape (N, D)</span></div><div class="line"><span class="string">  - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></div><div class="line"><span class="string">  - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  x_hat,gamma,sqr_x_mean,mean,var,sqrt_var,x_mean,inv_sqrt_var = cache</div><div class="line">  dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  N = x_hat.shape[<span class="number">0</span>]</div><div class="line">    </div><div class="line">  dx_hat = dout * gamma</div><div class="line">  dx_mean = dx_hat*inv_sqrt_var</div><div class="line">  dinv_sqrt_var = np.sum(dx_hat*x_mean,axis=<span class="number">0</span>)</div><div class="line">  dsqrt_var = <span class="number">-1.0</span>/(sqrt_var**<span class="number">2</span>)*dinv_sqrt_var</div><div class="line">  dvar = <span class="number">0.5</span>*inv_sqrt_var*dsqrt_var</div><div class="line">  dsqr_x_mean = <span class="number">1.0</span>/N*np.ones(sqr_x_mean.shape)*dvar</div><div class="line">  dx_mean += <span class="number">2</span>*x_mean*dsqr_x_mean</div><div class="line"></div><div class="line">  dmean = -np.sum(dx_mean,axis=<span class="number">0</span>)</div><div class="line">  dx1 = dx_mean</div><div class="line">  dx2 = <span class="number">1.0</span>/N*np.ones(mean.shape)*dmean</div><div class="line">  dx = dx1+dx2</div><div class="line">  dgamma = np.sum(x_hat*dout,axis=<span class="number">0</span>)</div><div class="line">  dbeta =  np.sum(dout,axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dgamma, dbeta</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span><span class="params">(dout, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Alternative backward pass for batch normalization.</span></div><div class="line"><span class="string">  For this implementation you should work out the derivatives for the batch</span></div><div class="line"><span class="string">  normalizaton backward pass on paper and simplify as much as possible. You</span></div><div class="line"><span class="string">  should be able to derive a simple expression for the backward pass.</span></div><div class="line"><span class="string">  Note: This implementation should expect to receive the same cache variable</span></div><div class="line"><span class="string">  as batchnorm_backward, but might not use all of the values in the cache.</span></div><div class="line"><span class="string">  Inputs / outputs: Same as batchnorm_backward</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  x_hat,gamma,sqr_x_mean,mean,var,sqrt_var,x_mean,inv_sqrt_var = cache</div><div class="line">  N = x_hat.shape[<span class="number">0</span>]</div><div class="line">  dbeta = np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">  dgamma = np.sum(dout*x_hat,axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">  dx = <span class="number">1.0</span>/N*inv_sqrt_var*gamma*(</div><div class="line">   N*dout</div><div class="line">   -np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">   -x_mean*(inv_sqrt_var**<span class="number">2</span>)*np.sum(dout*x_mean,axis=<span class="number">0</span>)</div><div class="line">  )</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dgamma, dbeta</div></pre></td></tr></table></figure></p>
<h5>实验</h5>
<p><strong>MNIST</strong></p>
<p>Fig 1(a)是一个小网络在MNIST数据集上的表现，可以看出加了BN整个训练过程更加平稳。Fig 1(b,c)是输入分布的变化，三条线分别是15，50，85，可以发现BN的数值更加分开，更有区分度，分布更加稳定。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fnxqqs6d9sj30m40eewhn.jpg" alt=""></p>
<p><strong>IMAGENET</strong></p>
<p>做了一些修改：</p>
<ul>
<li>提高学习率</li>
<li>移除Dropout</li>
<li>样本更加分布均匀，完全打乱</li>
<li>减少L2正则</li>
<li>加速learning rate decay</li>
<li>移除LRN</li>
<li>减少一些数据增强的方法</li>
</ul>
<p>见Figure 3, 可以发现BN对结果提升还是比较明显的</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fnxs1k4lpej30kc0c2di6.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/25/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/25/Going-Deeper-with-Convolutions笔记/" >Going Deeper with Convolutions笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-25  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文 ：Going Deeper with Convolutions,CVPR 2015</p>
<p>链接：https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf</p>
<p>作者: Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fnsrwl5iuuj31040kc1dy.jpg" alt=""></p>
<p><strong>Contribution</strong></p>
<ul>
<li>提出了GoogleNet的网络结构，由若干Inception Module堆叠而成。</li>
<li>在ILSVRC 2014上拿到了classification task和detection task的冠军。</li>
</ul>
<p><strong>Motivation</strong></p>
<p>对于DCNN来说，增大网络的大小是一个直接提升网络性能的方法，增大网络大小包括增加网络的深度，增大网络的宽度，也就是每个level的神经元数量。 但是增大网络也伴随着问题：</p>
<ol>
<li>大量的参数需要优化</li>
<li>容易过拟合</li>
<li>需要大量的喂大量的标签数据，而数据是很昂贵的！</li>
<li>模型越大，所需要的计算资源也越多</li>
</ol>
<p>所以，本着<em>Occam's Razor</em>，简洁才是王道的思想，需要更加精细地优化网络结构。</p>
<p>作者也引用了一些理论依据：</p>
<blockquote>
<p>if the probability distribution of the dataset is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer after layer by analyzing the correlation statistics of the preceding layer activations and clustering neurons with highly correlated outputs.</p>
</blockquote>
<blockquote>
<p>Hebbian principle – neurons that fire together, wire together</p>
</blockquote>
<p>还有一些客观事实：</p>
<blockquote>
<p>Steadily improving and highly tuned numerical libraries that allow for extremely fast dense matrix multiplication</p>
</blockquote>
<p>所以，作者想到的是用一些readily avaliable dense blocks去模拟近似构造local sparse structure。</p>
<p><strong>Inception Module:</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fnsryufma2j30o20titbv.jpg" alt=""></p>
<p>Fig 2是Inception Module的图示，从Fig 2(a)可以看出主要有5个部分：</p>
<ol>
<li>$1\times 1$ convolution</li>
<li>$3\times 3$ convolution</li>
<li>$5\times 5$ convolution</li>
<li>$3\times 3$ max pooling</li>
<li>filter concatenation</li>
</ol>
<p>运用了不同感受野的卷积，1x1的卷积可以capture dense information clusters，3x3 and 5x5的卷积可以capture more spatially spread out clusters</p>
<p>但是容易发现，这样的结构会导致channel的数量增长很快！</p>
<p>以下图为例： 输入大小是$28\times28\times256$，输出分别是$28\times28\times128$，$28\times28\times192$，$28\times28\times96$，$28\times28\times256$，filter concatenation以后，输出是$28\times28\times672$!!!</p>
<p>再看一下计算量:</p>
<p>$[1\times1 conv, 128]$  $28\times28\times128\times1\times1\times256$</p>
<p>$[3\times3 conv, 192]$ $28\times28\times192\times3\times3\times256$
$[5\times5 conv, 96]$ $28\times28\times96\times5\times5\times256$</p>
<p>总共大概有854M ops。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fnss7rs69gj30qg0gijtb.jpg" alt=""></p>
<p>优化：通过$1\times 1$卷积进行降维（保留空间信息，降低深度），其实可以等价成做embedding，将高维的信息映射到低维，同时保证低维的embeddings包含高维数据的大部分信息。</p>
<p>下图是经过优化后的结构，再统计一下卷积计算量：</p>
<p>$[1\times1 conv, 64]$ $28\times28\times64\times1\times1\times256$
$[1\times1 conv, 64]$ $28\times28\times64\times1\times1\times256$
$[1\times1 conv, 128]$ $28\times28\times128\times1\times1\times256$
$[3\times3 conv, 192]$ $28\times28\times192\times3\times3\times64$
$[5\times5 conv, 96]$ $28\times28\times96\times5\times5\times64$
$[1\times1 conv, 64]$ $28\times28\times64\times1\times1\times256$
总共358M ops</p>
<p>和naive的版本比较，可以说少了一半！</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnsshqu6hij30qg0kowhs.jpg" alt=""></p>
<p>Fig 3是模型的全图，可以发现，整个模型基本上是由多个inception module堆叠而成，同时模型也利用多multi-scale的特征建立auxiliary classifiers，也可以帮助梯度的回传。Table 1是模型可视化的另一个形式。可以发现，随着深度的增加，inception module里面，1x1卷积的fitlers的数量和3x3卷积或5x5卷积的比率逐渐升高。<strong><em>其实深度越深，空间信息对特征抽象的重要性在逐渐降低（as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease.。</em></strong></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnssmysygpj30ba0r8go0.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fnssrbjv00j312i0qm0zf.jpg" alt=""></p>
<p><strong>训练细节：</strong></p>
<ul>
<li>没有使用额外数据</li>
<li>独立训练7个相同结构的模型</li>
<li>将每个图像resize到四个不同的scale(256,288,320,352)，然后拿到left,center,right的square image，在对每个square image取四个角落，中间，以及resized这6个图像，然后再对它们做镜像，所以每张图像一共有4x3x6x2=144张！</li>
<li>将每个crop的softmax加起来取平均</li>
</ul>
<p><strong>实验：</strong></p>
<p>Table 2-5都是实验对比结果,Table 2中，可以发现GoogLeNet在ILSVRC 2014分类比赛中，拿到第一，并取得了Error(top-5) 6.67%的成绩。Table 4是在detection任务上的表现，获得了第一。Table 5是单模型在detection任务上的表现，也非常不错。Table 3是在预测图像时选用不同模型数量ensemble和crop的数量的表现对比，可以发现，多模型和尽可能多的crop对表现提升最大。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnst7i0modj30py0hoq5r.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fnst7k1klyj30q80foacb.jpg" alt=""></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fnst7ottrsj313q0by415.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnst7mjw0hj30oo0newhg.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/25/Going-Deeper-with-Convolutions笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/18/Visualizing-and-UnderstandingConvolutional-Networks笔记/" >Visualizing and UnderstandingConvolutional Networks笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>​论文：Visualizing and UnderstandingConvolutional Networks</p>
<p>作者：Matthew D Zeiler, Rob Fergus, ECCV, 2014</p>
<p>链接：https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</p>
<p>博客链接：http://wulimengmeng.top/2018/01/18/Visualizing-and-UnderstandingConvolutional-Networks%E7%AC%94%E8%AE%B0/</p>
<p>代码实现： https://github.com/mowayao/Deconvnet</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkxf4e7rcj318g0dgdke.jpg" alt=""></p>
<p><strong>Idea:</strong> 模型的可视化对模型的优化非常重要，更好的理解模型可以帮助我们更好地改进模型。作者将网络的任一层输出的feature maps通过Deconvnet反馈到最终的输入（input pixel space）上（reveals the input stimuli that excite individual feature maps at any layer in the model）。</p>
<h5>Deconvnet</h5>
<p>Deconvnet，顾名思义，就是将卷积，pooling，ReLU等运算做逆运算，将feature maps映射回pixels，可视化该feature map被原图哪部分特征激活，从而理解该feature map从原图像学习了何种特征。</p>
<p>随机选择一些不同层的feature maps，经过“反向运算”，<strong><em>unpooling-&gt;ReLU-&gt;Transposed Conv</em></strong>，映射到输入，具体可以见Fig. 1。左边为Deconvnet，右边是Convnet，Unpooling层和pooling层一一对应。convnet是输入图像提取特征，而deconvnet是从特征映射到输入图像。</p>
<p><strong>Unpooling:</strong> 通过记录每个pooling region的最大值的位置，在运算的时候将最大值复制到原来的位置，其他位置的值设为0。</p>
<p><strong>ReLU</strong>：和forward时的ReLU运算相同</p>
<p><strong>Filtering</strong>: 就是卷积的逆运算，卷积核的转置。</p>
<p>考虑一个简单的卷积层运算，其参数为(feature map dim=4, kernel size=3, stride=1, padding=0, output dim=2)</p>
<p>我们可以将$3\times3$ kernel转换成等价的稀疏矩阵C：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fnqehkh9h4j30kc02hweb.jpg" alt=""></p>
<p>再将$[4\times 4]$的输入转换成等价的16维的向量，那么$y=Cx$的输出就是一个4维向量，再将其转换成$2\times2$的矩阵，得到最终的结果。</p>
<p>而Deconv也称Transposed conv，就是变成$C^Ty$。</p>
<p>其实整个过程和convolution的back-propagation是很像的。
$$
\frac{\partial L}{\partial x_i} = \sum_j\frac{\partial y_j}{\partial  x_i}\cdot\frac{\partial L}{\partial y_j} \ = \sum_{j}C_{i,j}\cdot \frac{\partial L}{\partial y_j} \ = C^T_{*,i} \frac{\partial L}{\partial y}
$$
还有一个很重要的概念需要厘清：为什么用这种类似back-propogation的方法将feature map映射回输入图像，可以反应出feature map在原图中学到的东西？</p>
<p>我们假设CNN是一个high level的非线性函数$f(X)$，输出是feature map。我们这里把所有的参数看成一个整体，做一个近似:
$$
f(X) \approx  W_iX
$$
这里的$W_i$表示的是第i个pattern，那么$f(X)$求关于X的梯度其实就是pattern W。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkv3ckslgj30us0qowkv.jpg" alt=""></p>
<h5>Model</h5>
<p>基于AlexNet修改，将第一层$11\times 11$的大小减小到$7\times 7$, 并将步长从4减少到2， 并ImageNet 2012数据集上训练，具体见Fig. 3。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkzcp3ebcj30uy0i2791.jpg" alt=""></p>
<h5>实验</h5>
<p><strong>可视化：</strong></p>
<p>结果见Fig. 2。</p>
<p>Layer 1: 可以看到基本上是一些图像最基本的元素，例如，edge，corner等。</p>
<p>Layer 2-5： 随着深度的加深，其特征越来越具体，variance也越来越大！具有越来越强的辨别能力。</p>
<p>说明CNN学习到的特征是层次化的！</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkzpsayskj30ji0sc1kx.jpg" alt=""></p>
<p><strong>遮挡实验：</strong></p>
<p>将图片中的一些区域进行遮挡，将其换成灰色方块，结果见Fig 6。具体以第一行为例，可以发现，当狗的身体一部分被遮挡时，网络还是显示亮蓝色，表示预测的是网球。说明，网络已经学会根据图像的context信息去剔除与目标无关的区域或者物体。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fnl01ru982j30q40s84hp.jpg" alt=""></p>
<p><strong>Varying ImageNet Model Sizes：</strong></p>
<p>对AlextNet进行模型调优，结果见Table 2。主要有几个发现：</p>
<ul>
<li>去掉全连接层反而降低了错误率</li>
<li>增大中间层的卷积的数量可以大大提升模型性能</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fnl14d4h7dj30zu0m2q8s.jpg" alt=""></p>
<p><strong>Feature Generalization:</strong></p>
<p>测试模型的泛化能力，在Caltech-101, Caltech-256，PASCAL VOC 2012这三个数据集上测试模型提取特征的泛化性。 具体的做法就是1-7层的参数不动，然后训练一个新的softmax分类器。具体结果见Table 3, 4, 5，显示出了很强的泛化能力。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fnl1hi1a55j30z20b8tav.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnl1hwfrjwj30rs0ay40f.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnl1hydlkcj30zm0iy0yg.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/18/Visualizing-and-UnderstandingConvolutional-Networks笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/18/Network-In-Network算法笔记/" >Network In Network算法笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：Network In Network</p>
<p>作者：Min Lin, Qiang Chen, Shuicheng Yan  ICLR 2014</p>
<p>链接：https://arxiv.org/pdf/1312.4400.pdf</p>
<p><strong>Idea:</strong> 传统的CNN一般就是通过linear filter和输入的每个和filter大小相同的local patches做内积，然后再跟一个非线性激活函数。CNN的linear filter对于输入的每个local patch，是一个GLM（generalized linear model）。作者argue：GLM的抽象层次比较低(the level of abstraction of GLM is low)，这里的level of abstrction可以理解成不变性的抽象层次（invariant to the variants of some concepts）。GLM依赖于data或者concept线性可分的assumption，当data或者concept线性不可分的时候抽象能力就大大下降，而实际上现实中图像数据往往是low-dim manifold in high-dim space，所以往往是线性不可分的。因此作者提出用一些非线性的函数逼近器(nonlinear function approximator)来代替GLM，从而能够提取local patches更抽象的特征，提高模型对于local patch的判别能力（discrimminability）。Figure 1就是传统的CNN和NIN的比较。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkplf4900j319g0kigpu.jpg" alt=""></p>
<h5>模型细节</h5>
<p>本质上是将convolution -&gt; relu替换为convolution -&gt; relu -&gt; convolution (1x1 filter) -&gt; relu</p>
<p>对于$1\times 1$的卷积，在二维的情况下只是起到scale的作用，例如输入是$[32×32]$做1×1的卷积，只是对输入的每个元素做放大或者缩小，而如果输入是$[32×32×3]$，再做1×1的卷积，那么它其实就是对该位置的所有通道的线性组合，也可以叫做feature pooling或coordinate-dependent transformation。</p>
<p>最大的贡献是:</p>
<ol>
<li>$1\times 1$ 卷积的用法</li>
<li>Global average pooling的应用</li>
</ol>
<p>优点：</p>
<ol>
<li>local patch的抽象能力强</li>
<li>通过global average pooling减少overfitting</li>
<li>参数少,用GAP代替全连接层</li>
</ol>
<p><strong>MLP Convolution Layers</strong></p>
<p>作者选择多层的感知机(其实也就是 $1\times1$  的卷积层)作为function approximator，并列举了两个理由：</p>
<ol>
<li>和CNN兼容，可以用BP训练</li>
<li>自身可以作为一个deep model，可以用 $1\times 1$ 的卷积替代</li>
</ol>
<p>传统的CNN的feature maps的计算如下,ReLU为激活函数：
$$
f_{i,j,k} = \max(w_k^Tx_{i,j},0)
$$
maxout层feature maps的计算如下：
$$
f_{i,j,k}=\max_m(w_{k_m}^Tx_{i,j})
$$
maxout其实就是ReLU和Leaky ReLU的扩展，提升非线性能力，弥补了两者的缺点，例如应用ReLU激活函数，训练到后面，大部分neron会“挂掉”，因为已经死掉的neuron不会再有梯度了！除此之外，maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnlus4z8aaj30e904d74l.jpg" alt=""></p>
<p>mlpconv layer的计算如下：
$$
f_{i,j,k_1}^1=\max({w_{k_1}^1}^Tx_{i,j}+b_{k_1},0) \ ... \ f_{i,j,k_n}^n=\max({w_{k_n}^n}^Tx_{i,j}+b_{k_n},0)
$$</p>
<p>n是多层感知机的层数。Figure 2是NIN的整体结构。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fnkq60369mj31780gsn1j.jpg" alt=""></p>
<p><strong>Global Average Pooling</strong></p>
<p>传统的CNN网络在做完所有卷积运算后，会把feature maps拉成一条向量，然后再接几层全连接层做分类。这样的问题是，多层的全连接经常会overfitting，当然也可以加Dropout来作为regularizer提高泛化性。作者提出global average pooling来取代全连接层，顾名思义，就是对每个feature map求全局平均，这样整个输出就变成了长度为feature maps深度的向量，这样能够有更好的空间不变形。除此之外，因为减少了待优化的参数避免了过拟合的发生。</p>
<h5>实现</h5>
<p><strong>Pytorch</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.num_classes = num_classes</div><div class="line">        self.classifer = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">160</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">160</span>,  <span class="number">96</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.Dropout(<span class="number">0.5</span>),</div><div class="line"></div><div class="line">                nn.Conv2d(<span class="number">96</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.Dropout(<span class="number">0.5</span>),</div><div class="line"></div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>,  num_classes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.AvgPool2d(kernel_size=<span class="number">8</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.classifer(x)</div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_classes)</div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure></p>
<h5>实验</h5>
<p>作者在CIFAR-10, CIFAR-100, SVHN和MNIST上进行测试。</p>
<p><strong>CIFAR-10</strong>: 50,000个训练样本，10,000个测试样本，图像大小为 $32\times32$ ，实验结果见Table 1。Dropout和data augmentation对结果提升明显。It turns out in our experiment that using dropout in between the mlpconv layers in NIN boosts the performance  of  the  network  by  improving  the  generalization  ability  of  the  model.</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fnkr9b10ylj30xk0fe0wc.jpg" alt=""></p>
<p><strong>CIFAR-100</strong>: 数据规模和图像大小和cifar-10一样，不同的是它有100类，结果见Table 2。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fnkrewcgz6j30fc066aaq.jpg" alt=""></p>
<p><strong>SVHN</strong>：包含630,420张 $32\times32$ 的图像，将其分类训练集，验证集，测试集。具体结果见Table 3。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fnkrhat3qrj30fy07wt9o.jpg" alt=""></p>
<p><strong>MNIST</strong>：包含60,000张$28\times 28$的训练图像，和10,000张测试图像，具体结果见Table 4。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fnkrjy6lgqj30ey05ejrx.jpg" alt=""></p>
<p><strong>Global Average Pooling</strong></p>
<p>通过控制变量，发现GAP对结果的提升还是比较明显的，可以作为regularizer，具体见Table 5。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fnkrld8nt8j30g8056mxo.jpg" alt=""></p>
<p><strong>Visualization</strong></p>
<p>Figure 4 展示了一些样例图片采样自cifar-10和它们对应类别的features maps。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fnkrpb3zc7j30nu0f80y5.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/18/Network-In-Network算法笔记/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2018/01/18/AlexNet算法笔记/" >AlexNet算法笔记</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2018-01-18  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<p>论文：ImageNet Classification with Deep Convolutional Neural Networks</p>
<p>链接：https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</p>
<p>AlexNet是发表在NIPS 2012的一篇文章，可以称作是深度学习的经典之作，获得了ImageNet LSVRC-2010的冠军，达到了15.3%的top-5 error。</p>
<p><strong>模型结构：</strong></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndsfv7yy4j31a80fu0y3.jpg" alt=""></p>
<p>下面是模型的具体描述：</p>
<p>$[227\times227\times3]$ 输入
$[55\times55\times96]$ CONV1: 96 $11\times11$ filters at stride 4, pad 0   &lt;u&gt;(227-11)/4+1 = 55&lt;/u&gt;
$[27\times27\times96]$  MAX POOL1: $3\times3$ filters at stride 2   &lt;u&gt;(55-3)/2+1=27&lt;/u&gt;
$[27\times27\times96]$ NORM1: Normalization layer
$[27\times27\times256]$ CONV2: 256 $5\times5$ filters at stride 1, pad 2   &lt;u&gt;(27+2*2-5)/1 + 1=27&lt;/u&gt;
$[13\times13\times256]$ MAX POOL2: $3\times3$ filters at stride 2   &lt;u&gt;(27-3)/2+1=13&lt;/u&gt;
$[13\times13\times256]$ NORM2: Normalization layer
$[13\times13\times384]$ CONV3: 384 $3\times3$ filters at stride 1, pad 1
$[13\times13\times384]$ CONV4: 384 $3\times3$ filters at stride 1, pad 1
$[13\times13\times256]$ CONV5: 256 $3\times3$ filters at stride 1, pad 1
$[6\times6\times256]$ MAX POOL3: $3\times3$ filters at stride 2    &lt;u&gt;(13-3)/2+1=6&lt;/u&gt;
$[4096]$ FC6: 4096 neurons
$[4096]$ FC7: 4096 neurons
$[1000]$ FC8: 1000 neurons (class scores)</p>
<p>包含了5层卷积层和3层全连接层。</p>
<p><strong>创新点：</strong></p>
<ol>
<li>
<p>第一次使用了ReLU激活函数。传统的sigmoid和tanh激活函数的问题在于梯度容易饱和，造成训练困难，下图是sigmoid函数的梯度。而$f(x)=\max(0,x)$看出，ReLU是一个非线性激活函数，而且它的梯度不会饱和，当x&gt;0的时候，梯度一直是1，这样和sigmoid和tanh函数相比，加快了训练的速度。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnhlvvatogj30my0egtbr.jpg" alt=""></p>
</li>
<li>
<p>使用了Norm Layer，对局部区域进行归一化，对相同空间位置上相邻深度的卷积做归一化。$b_{x,y}^i=\frac{a_{x,y}^i}{(k+\alpha\sum_{j=\max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{i,j})^2)^\beta}$,其中$a_{x,y}^i$表示的是第i个通道的卷积核在$(x,y)$位置处的输出结果，随后经过ReLU激活函数作用。a是每一个神经元的激活值，n是kernel的大小，N是kernel总数，k,alpha,beta都是预设的hyper-parameters.，$k=2,n=5,\alpha=1e-4,\beta=0.75$。从公式可以看出，给原来的激活值$a$加了一个权重，生成了新的激活值b,也就是在不同map的同一空间位置进行了归一化，提高了计算效率。但是这些值为什么这么设置就不得而知了。</p>
</li>
<li>
<p>大量的数据增强，水平翻转，镜像等。调整RGB channel的值，对数据集所有图像的RGB值做PCA变换，完成去噪功能，同时为了保证图像的多样性，在特征值上加了一个随机的尺度因子，每一轮重新生成一个尺度因子，起到了正则化的作用。</p>
</li>
<li>
<p>Dropout, hidden layer的输出有0.5的几率会被置为0，那些被droped的点不会参与forward pass和backprogation，这样起到了正则化的作用。需要注意的是，在测试过程中，需要将输出乘上0.5。这是因为在训练的过程中，我们只选择了其中的一半，训练出来的结果相当于原来方法的两倍，所以当测试的时候需要乘上0.5来消除这个影响。</p>
</li>
</ol>
<p><strong>训练细节：</strong></p>
<ul>
<li>batch size为128，momentum为0.9，weight decay为0.0005，其实weight decay是l2正则是有区别的，详细可见：https://arxiv.org/pdf/1711.05101.pdf</li>
<li>初始的learning rate设为1e-2, 当验证集的正确率停止的时候乘0.1</li>
</ul>
<p><strong>实验结果：</strong></p>
<p>最终的实验结果见Table 1。可以发现，CNN的结果在Top-1 error和Top-5上都超出了传统方法一大截。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndtpbwzn9j30ke0bcabv.jpg" alt=""></p>
<p>Table 2就是模型ensemble的结果。Averaging the predictions of five similar CNNs gives an error rate of 16.4%。Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndtpewjn8j30te0egq6a.jpg" alt=""></p>

	
	</div>
  <a type="button" href="/2018/01/18/AlexNet算法笔记/#more" class="btn btn-default more">Read More</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
    
           <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>Prev</a>
        

        <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
 
       <a href="/page/2/" type="button" class="btn btn-default ">Next<i class="fa fa-arrow-circle-o-right"></i></a>     
        

  
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/algorithms/">algorithms<span>12</span></a></li>
		
			<li><a href="/categories/competition/">competition<span>1</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/face-detection/">face detection<span>1</span></a></li>
		
			<li><a href="/tags/deep-learning/">deep learning<span>18</span></a></li>
		
			<li><a href="/tags/algorithms/">algorithms<span>2</span></a></li>
		
			<li><a href="/tags/classfication/">classfication<span>3</span></a></li>
		
			<li><a href="/tags/Deep-Learning/">Deep Learning<span>1</span></a></li>
		
			<li><a href="/tags/machine-learning/">machine learning<span>1</span></a></li>
		
			<li><a href="/tags/杂/">杂<span>1</span></a></li>
		
			<li><a href="/tags/computer-vision/">computer vision<span>4</span></a></li>
		
			<li><a href="/tags/paper-notes/">paper notes<span>4</span></a></li>
		
			<li><a href="/tags/notes/">notes<span>6</span></a></li>
		
			<li><a href="/tags/GAN/">GAN<span>1</span></a></li>
		
			<li><a href="/tags/math/">math<span>2</span></a></li>
		
			<li><a href="/tags/summary/">summary<span>2</span></a></li>
		
			<li><a href="/tags/RNN/">RNN<span>1</span></a></li>
		
			<li><a href="/tags/paper/">paper<span>1</span></a></li>
		
			<li><a href="/tags/classifcation/">classifcation<span>8</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2018/02/09/Kaggle比赛小结-Camera-Model-Identification/" ><i class="fa fa-file-o"></i>Kaggle比赛小结-Camera Model Ide...</a>
      </li>
    
      <li>
        <a href="/2018/02/07/Inception-v4-Inception-ResNet-and-Inception-v4-笔记/" ><i class="fa fa-file-o"></i>Inception-v4, Inception-Res...</a>
      </li>
    
      <li>
        <a href="/2018/02/07/Rethinking-the-Inception-Architecture-for-Computer-Vision-Inception-V3-笔记/" ><i class="fa fa-file-o"></i>Rethinking the Inception Ar...</a>
      </li>
    
      <li>
        <a href="/2018/02/01/Identity-Mappings-in-Deep-Residual-Networks笔记/" ><i class="fa fa-file-o"></i>Identity Mappings in Deep R...</a>
      </li>
    
      <li>
        <a href="/2018/02/01/Deep-Residual-Learning-for-Image-Recognition笔记/" ><i class="fa fa-file-o"></i>Deep Residual Learning for ...</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/mowayao" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.weibo.com/mowayao" title="My weibo account." target="_blank"]);">My Weibo</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 Mowayao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$'], ['\[','\]'] ], 
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
   </html>
