<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mowayao&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/16886497103686372c55fdd8ac89f177</icon>
  <subtitle>一往无前虎山行</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wulimengmeng.top/"/>
  <updated>2018-01-18T05:34:40.850Z</updated>
  <id>http://wulimengmeng.top/</id>
  
  <author>
    <name>Mowayao</name>
    <email>zpyao1992@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AlexNet算法笔记</title>
    <link href="http://wulimengmeng.top/2018/01/18/AlexNet%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/"/>
    <id>http://wulimengmeng.top/2018/01/18/AlexNet算法笔记/</id>
    <published>2018-01-18T05:12:08.000Z</published>
    <updated>2018-01-18T05:34:40.850Z</updated>
    
    <content type="html"><![CDATA[<p>论文：ImageNet Classification with Deep Convolutional Neural Networks</p><p>链接：<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p><p>AlexNet是发表在NIPS 2012的一篇文章，可以称作是深度学习的经典之作，获得了ImageNet LSVRC-2010的冠军，达到了15.3%的top-5 error。</p><p><strong>模型结构：</strong></p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndsfv7yy4j31a80fu0y3.jpg" alt=""></p><p>下面是模型的具体描述：</p><p>$[227\times227\times3]$ 输入<br>$[55\times55\times96]$ CONV1: 96 $11\times11$ filters at stride 4, pad 0   <u>(227-11)/4+1 = 55</u><br>$[27\times27\times96]$  MAX POOL1: $3\times3$ filters at stride 2   <u>(55-3)/2+1=27</u><br>$[27\times27\times96]$ NORM1: Normalization layer<br>$[27\times27\times256]$ CONV2: 256 $5\times5$ filters at stride 1, pad 2   <u>(27+2*2-5)/1 + 1=27</u><br>$[13\times13\times256]$ MAX POOL2: $3\times3$ filters at stride 2   <u>(27-3)/2+1=13</u><br>$[13\times13\times256]$ NORM2: Normalization layer<br>$[13\times13\times384]$ CONV3: 384 $3\times3$ filters at stride 1, pad 1<br>$[13\times13\times384]$ CONV4: 384 $3\times3$ filters at stride 1, pad 1<br>$[13\times13\times256]$ CONV5: 256 $3\times3$ filters at stride 1, pad 1<br>$[6\times6\times256]$ MAX POOL3: $3\times3$ filters at stride 2    <u>(13-3)/2+1=6</u><br>$[4096]$ FC6: 4096 neurons<br>$[4096]$ FC7: 4096 neurons<br>$[1000]$ FC8: 1000 neurons (class scores)</p><p>包含了5层卷积层和3层全连接层。</p><p><strong>创新点：</strong></p><ol><li><p>第一次使用了ReLU激活函数。传统的sigmoid和tanh激活函数的问题在于梯度容易饱和，造成训练困难，下图是sigmoid函数的梯度。而$f(x)=\max(0,x)$看出，ReLU是一个非线性激活函数，而且它的梯度不会饱和，当x&gt;0的时候，梯度一直是1，这样和sigmoid和tanh函数相比，加快了训练的速度。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnhlvvatogj30my0egtbr.jpg" alt=""></p></li><li><p>使用了Norm Layer，对局部区域进行归一化，对相同空间位置上相邻深度的卷积做归一化。$b_{x,y}^i=\frac{a_{x,y}^i}{(k+\alpha\sum_{j=\max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{i,j})^2)^\beta}$,其中$a_{x,y}^i$表示的是第i个通道的卷积核在$(x,y)$位置处的输出结果，随后经过ReLU激活函数作用。a是每一个神经元的激活值，n是kernel的大小，N是kernel总数，k,alpha,beta都是预设的hyper-parameters.，$k=2,n=5,\alpha=1e-4,\beta=0.75$。从公式可以看出，给原来的激活值$a$加了一个权重，生成了新的激活值b,也就是在不同map的同一空间位置进行了归一化，提高了计算效率。但是这些值为什么这么设置就不得而知了。</p></li><li><p>大量的数据增强，水平翻转，镜像等。调整RGB channel的值，对数据集所有图像的RGB值做PCA变换，完成去噪功能，同时为了保证图像的多样性，在特征值上加了一个随机的尺度因子，每一轮重新生成一个尺度因子，起到了正则化的作用。</p></li><li><p>Dropout, hidden layer的输出有0.5的几率会被置为0，那些被droped的点不会参与forward pass和backprogation，这样起到了正则化的作用。需要注意的是，在测试过程中，需要将输出乘上0.5。这是因为在训练的过程中，我们只选择了其中的一半，训练出来的结果相当于原来方法的两倍，所以当测试的时候需要乘上0.5来消除这个影响。</p></li></ol><p><strong>训练细节：</strong></p><ul><li>batch size为128，momentum为0.9，weight decay为0.0005，其实weight decay是l2正则是有区别的，详细可见：<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1711.05101.pdf</a></li><li>初始的learning rate设为1e-2, 当验证集的正确率停止的时候乘0.1</li></ul><p><strong>实验结果：</strong></p><p>最终的实验结果见Table 1。可以发现，CNN的结果在Top-1 error和Top-5上都超出了传统方法一大截。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndtpbwzn9j30ke0bcabv.jpg" alt=""></p><p>Table 2就是模型ensemble的结果。Averaging the predictions of five similar CNNs gives an error rate of 16.4%。Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndtpewjn8j30te0egq6a.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文：ImageNet Classification with Deep Convolutional Neural Networks&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classifi
      
    
    </summary>
    
      <category term="algorithms" scheme="http://wulimengmeng.top/categories/algorithms/"/>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="classfication" scheme="http://wulimengmeng.top/tags/classfication/"/>
    
  </entry>
  
  <entry>
    <title>VGGNet算法笔记</title>
    <link href="http://wulimengmeng.top/2018/01/18/VGGNet%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/"/>
    <id>http://wulimengmeng.top/2018/01/18/VGGNet算法笔记/</id>
    <published>2018-01-18T05:10:41.000Z</published>
    <updated>2018-01-18T05:22:43.667Z</updated>
    
    <content type="html"><![CDATA[<p>论文：Very Deep Convolutional Networks for Large-Scale Image Recognition</p><p>论文链接：<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="external">https://arxiv.org/abs/1409.1556</a></p><p>这篇文章发表在ICLR 2015上,作者是Karen Simonyan和Andrew Zisserman，其算法获得了ImageNet ILSVRC-2014的localization task的冠军和classification task的第二名。文章通过堆叠$3\times 3$的卷积，ReLU, $2\times 2$的max pooling逐渐加深网络的深度。所以，它的特点就是连续的Conv运算比较多，计算量比较大(与AlexNet相比)，同时也提高了模型的感受野，能够提取更high-level的特征。VGGNet被提出以后被应用在各种任务中，例如物体分类，物体检测(object proposal生成)，语义分割，特征提取(image retrieval)等任务，都取得了非常好的效果。</p><p>Table 1是其VGG Net各个变种的网络结构参数，从左到右分别是A，A-LRN，B，C，D，E这6种，各个模型的深度分别是：11，11，13，16，16，19。可以发现，作者其实将整个网络分成两个部分，第一个部分是卷积层，第二个部分是全连接层，卷积层又分成了5个卷积组，卷积组的feature maps的深度从64逐渐增加到512，所以这5个卷积组的feature maps的深度分别是64，128，256，512，512，每个卷积组后面都会加一个$2\times 2$的non-overlapping的max pooling来降低feature maps的维度。</p><p>除此之外，为了 在不影响感受野的前提下，提高决策函数的非线性能力(increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers)，作者还在结构C中加入了$1\times1$的卷积。$1\times1$的卷积也被应用到很多的网络结构中，例如Google Net，Network in Network等。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndp5zk8eaj30t80qcaff.jpg" alt=""></p><p>以结构D为例，分析一下消耗的内存和模型的参数量：</p><table><thead><tr><th></th><th>内存</th><th>参数</th></tr></thead><tbody><tr><td>Input:$224\times 224\times3$</td><td>$224\times 224\times3=150k$</td><td>0</td></tr><tr><td>Conv3-64:$224\times 224\times64$</td><td>$224\times 224\times64=3.2M$</td><td>$3\times3\times3\times64=1728$</td></tr><tr><td>Conv3-64:$224\times 224\times64$</td><td>$224\times 224\times64=3.2M$</td><td>$3\times3\times64\times64=36864$</td></tr><tr><td>maxpool:$112\times112\times64$</td><td>$112\times112\times64=800K$</td><td>0</td></tr><tr><td>Conv3-128:$112\times112\times128$</td><td>$112\times112\times128=1.6M$</td><td>$3\times3\times64\times128=73728$</td></tr><tr><td>Conv3-128:$112\times112\times128$</td><td>$112\times112\times128=1.6M$</td><td>$3\times3\times128\times128=147456$</td></tr><tr><td>maxpool:$56\times56\times128$</td><td>$56\times56\times128=400K$</td><td>0</td></tr><tr><td>Conv3-256:$56\times56\times256$</td><td>$56\times56\times256=800K$</td><td>$3\times3\times128\times256=294912$</td></tr><tr><td>Conv3-256:$56\times56\times256$</td><td>$56\times56\times256=800K$</td><td>$3\times3\times256\times256=589824$</td></tr><tr><td>Conv3-256:$56\times56\times256$</td><td>$56\times56\times256=800K$</td><td>$3\times3\times256\times256=589824$</td></tr><tr><td>maxpool:$28\times28\times256$</td><td>$28\times28\times256=200K$</td><td>0</td></tr><tr><td>Conv3-512:$28\times28\times512$</td><td>$28\times28\times512=400K$</td><td>$3\times3\times256\times512=1179648$</td></tr><tr><td>Conv3-512:$28\times28\times512$</td><td>$28\times28\times512=400K$</td><td>$3\times3\times512\times512=2359296$</td></tr><tr><td>Conv3-512:$28\times28\times512$</td><td>$28\times28\times512=400K$</td><td>$3\times3\times512\times512=2359296$</td></tr><tr><td>maxpool:$14\times14\times512$</td><td>$14\times14\times512=100K$</td><td>0</td></tr><tr><td>Conv3-512:$14\times14\times512$</td><td>$14\times14\times512=100K$</td><td>$3\times3\times512\times512=2359296$</td></tr><tr><td>Conv3-512:$14\times14\times512$</td><td>$14\times14\times512=100K$</td><td>$3\times3\times512\times512=2359296$</td></tr><tr><td>Conv3-512:$14\times14\times512$</td><td>$14\times14\times512=100K$</td><td>$3\times3\times512\times512=2359296$</td></tr><tr><td>maxpool:$7\times7\times512$</td><td>$7\times7\times512=25K$</td><td>0</td></tr><tr><td>FC-4096 $1\times1\times4096$</td><td>4096</td><td>$25088\times4096=102760448$</td></tr><tr><td>FC-4096 $1\times1\times4096$</td><td>4096</td><td>$4096\times4096=102760448$</td></tr><tr><td>FC-1000 $1\times1\times1000$</td><td>1000</td><td>$4096\times1000=4096000$</td></tr><tr><td></td><td></td></tr></tbody></table><p>各个模型的具体参数量可以见Table 2。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fndqy7970tj30jy030gm2.jpg" alt=""></p><p>除此之外，作者还解释了为什么不用$5\times5$和$7\times7$的卷积，是因为一个$5\times5$卷积的感受野和两个连续的 $3\times3$的卷积是相同的，一个$7\times7$的卷积的感受野等价于3个连续的$3\times3$的卷积，一个$7\times 7$的卷积需要$7^2C^2$的参数，而3个连续的$3\times3$卷积需要$3(3^2C^2)$,所以用$3\times3$卷积的意义在于保证感受野的同时，可以降低参数数量和增加模型深度来提高模型的非线性能力，模型容量(model capacity)和模型复杂度(model complexity)。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fnhm3fh4d0j30jg0swdkp.jpg" alt=""></p><p><strong>训练策略：</strong></p><p>训练的时候，作者先将图像scale到S（S大于等于224），然后再crop得到$224\times224$的图像。</p><p>In our experiments, we evaluated models trained at two fixed scales: S = 256 and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}​$.</p><p><strong>实验结果：</strong></p><p>作者在ILSVRC-2012 dataset做了模型性能的评估。各个模型评估的结果见Table 3。我们可以发现从左到右随着深度的加深，模型的错误率逐渐降低，VGG 19的效果最好，取得了25.5%的 top-1 val error和8.0%的top-5 val. error。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndqzo17l7j30qy0bstax.jpg" alt=""></p><p>作者也分析了各个图片尺度对结果的影响。作者对比了两个策略：单尺度策略和多尺度策略。单尺度策略是在训练集上选择尺寸S，测试集的大小为${S-32,S,S+32}$。而多尺度策略是选择尺度[$S_{min}$;$S_{max}$]，然后测试集的尺度为${S_{min},0.5(S_{min}+S_{max}),S_{max}}$，可以发现后者的效果会比前者更好一点。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndqzqx423j30rs0aomz8.jpg" alt=""></p><p>训练的图像大小为S，测试的大小为Q。</p><p>dense就是用 fully-convolutional替代fully connected，这样就不需要将测试图像rescale到相同的尺度。multi-crop，顾名思义，就是sample多个crop来进行分类，在评估dense和multi-crop时(见Table 5)，发现这两者是可以互补的。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fndrhfe79qj30t607uq4q.jpg" alt=""></p><p>最后就是需要将各个模型进行融合，做最后的ensemble。通过将最后输出的softmax其平均，得到最后的概率分布。Table 6就是最终模型fusion的结果。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fndrhr8gxjj30um09u0uz.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文：Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/p&gt;
&lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot; target=&quot;_blan
      
    
    </summary>
    
      <category term="algorithms" scheme="http://wulimengmeng.top/categories/algorithms/"/>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="classifcation" scheme="http://wulimengmeng.top/tags/classifcation/"/>
    
  </entry>
  
  <entry>
    <title>Comic Generation</title>
    <link href="http://wulimengmeng.top/2018/01/05/Comic-Generation/"/>
    <id>http://wulimengmeng.top/2018/01/05/Comic-Generation/</id>
    <published>2018-01-05T12:41:39.000Z</published>
    <updated>2018-01-09T06:15:10.305Z</updated>
    
    <content type="html"><![CDATA[<p>最近写了一下李宏毅的MLDS 2017的<a href="https://www.csie.ntu.edu.tw/~yvchen/f106-adl/A4" target="_blank" rel="external">HW4-Comics Generation</a>，正好总结一下GAN以及assignment的做法。</p><h2 id="Basic-Idea-of-GAN"><a href="#Basic-Idea-of-GAN" class="headerlink" title="Basic Idea of GAN"></a>Basic Idea of GAN</h2><p>给定数据分布：$P_{data}(x)$</p><p>我们有一个分布$P_G(x;\theta)$</p><p>从$P_{data}(x)$采样m个样本${x_1,x_2,…x_m}$</p><p>我们的目的是找到这样的$\theta$使得分布$P_G(x;\theta)$尽可能的和$P_{data}(x)$接近</p><p>如果给定参数$\theta$，我们就可以计算产生这一对样本的似然：<br>$$<br>L=\prod_{i=1}^mP_G(x_i;\theta)<br>$$<br>然后找到$\theta^\ast$最大化似然L：<br>$$<br>\theta^\ast = arg \max_\theta\prod_{i=1}^mP_G(x_i;\theta)=arg\max_\theta\log\prod_{i=1}^mP_G(x_i;\theta) =arg\max_\theta\sum_{i=1}^m\log P_G(x_i;\theta) \ \approx arg\max_\theta E_{x\sim P_{data}}[\log P_G(x;\theta)]<br>$$<br>现在，我们可以用NN来模拟$P_G(x;\theta)$<br>$$<br>P_G(x) = \int_z P_{prior}(z) I_{[G(z)=x]}dz<br>$$<br>z服从unit gaussian，但是这样似然明显很难计算！</p><ul><li>Generator G<ul><li>G is a function, input z, output x</li><li>Given a prior distribution$ P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G</li></ul></li><li>Discriminator D<ul><li>D is a function, input x, output scalar</li><li>Evaluate the “difference” between $P_G(x)$ and $P_{data}(x)$</li></ul></li></ul><p>目的是找到最佳的G：<br>$$<br>G^\ast = arg\min_G\max_DV(G,D)<br>$$</p><p>$$<br>V= E_{x\sim P_{data}}[\log D(x)] + E_{x\sim P_G}[\log (1-D(x))]<br>$$</p><p>下面是将上述问题转化为：</p><p>首先最优的D：<br>$$<br>P_{data}(x)\log D(x) + P_G(x) \log (1-D(x))<br>$$</p><p>$$<br>f(D) = a\log(D) + b\log(1-D)<br>$$</p><p>求极值，得到：<br>$$<br>D^\ast(x) =\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}<br>$$<br>所以<br>$$<br>\max_DV(G,D) = V(G,D^*)=E_{x\sim P_{data}}[\log\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x\sim P_G}[\log\frac{P_G(x)}{P_{data}(x)+P_G(x)}] \ = -2\log 2+E_{x\sim P_{data}}[\log\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}] + E_{x\sim P_G}[\log\frac{P_G(x)}{(P_{data}(x)+P_G(x))/2}] \ =-2\log 2 + KL(P_{data}(x)||\frac{P_{data}(x)+P_G(x)}{2}) + KL(P_{G}(x)||\frac{P_{data}(x)+P_G(x)}{2})<br>$$<br>将分母项 $P_{data}(x)+P_G(x)$ 除以2，那么整个式子就需要减去 $2\log\frac{1}{2}$ ，这就等价成了JS散度，定义了两个分布的相似性：</p><p>$$<br>JSD(P||Q) = \frac{1}{2}KL(P||M)+\frac{1}{2}KL(Q||M), M = \frac{1}{2}(P+Q)<br>$$</p><h3 id="一些tricks"><a href="#一些tricks" class="headerlink" title="一些tricks:"></a>一些tricks:</h3><p>有时候在训练的时候会碰到discriminator loss几乎一直是平的（0），这样就会让discriminator的作用变小（telling little information），也就意味着$P_{data}$和$P_{G}$几乎没有overlap，这是因为两者都是low dim manifold in high-dim space。</p><ul><li>add noise，增加两个分布的接触点或面，而且noise要随机事件decay。</li><li>​</li></ul><h3 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h3><p>z是噪声,y是条件，在初始的GAN加入了额外的条件y，y可以是任何形式的额外信息，包括类的属性等。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fn60wm16qej30oe0ki40z.jpg" alt=""></p><h3 id="FGAN"><a href="#FGAN" class="headerlink" title="FGAN"></a>FGAN</h3><p>用f-divergence代替原始的KL divergence</p><p>f-divergence, f is convex:<br>$$<br>D_f(P||Q) = \int_xq(x)f(\frac{p(x)}{q(x)})dx<br>$$<br>例如：<br>$$<br>f(x) = x\log x \ f(x) = -\log x \ f(x) = (x-1)^2<br>$$</p><h4 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate"></a>Fenchel Conjugate</h4><p>$$<br>f^\ast(t) = \max_{x\in dom(f)}(xt-f(x))<br>$$</p><p>$$<br>f(x) = \max_{t\in dom(f^<em>)}{xt-f^\ast(t} \ D_f(P||Q) = \int_x q(x)(\max_{t\in dom(f^</em>)}{\frac{p(x)}{q(x)}t-f^\ast(t)}dx<br>$$</p><p>$$<br>D_f(P||Q) \ge \int_x q(x)(x\frac{p(x)}{q(x)}D(x)-f^\ast(D(x)))dx \ = \int_xp(x)D(x)dx-\int_x q(x)f^\ast(D(x))dx \ =\max_DE_{x_\sim P}(D(x))-E_{x\sim Q}f^\ast(D(x))<br>$$</p><p>D is a function whose input is x and output is t</p><p>这就相当于定义了一个新的V(G,D)</p><h3 id="LSGAN（Least-Squares-GANs）"><a href="#LSGAN（Least-Squares-GANs）" class="headerlink" title="LSGAN（Least Squares GANs）"></a>LSGAN（Least Squares GANs）</h3><p>使用最小二乘损失函数代替了GAN的损失函数,事实上，作者认为使用JS散度并不能拉近真实分布和生成分布之间的距离，使用最小二乘可以将图像的分布尽可能的接近决策边界<br>$$<br>\min_DV_{LSGAN}(D) = \frac{1}{2}E_{x\sim p_{data}(x)}[(D(x)-b)^2]+\frac{1}{2}E_{x\sim p_{z}(z)}[(D(G(z))-a)^2]<br>$$</p><p>$$<br>\min_GV_{LSGAN}(G)= \frac{1}{2}E_{x\sim p_{data}(x)}[(D(G(z))-c)^2]<br>$$</p><h3 id="infoGAN"><a href="#infoGAN" class="headerlink" title="infoGAN"></a>infoGAN</h3><p>$$<br>\min_{G,Q}\max_DV_{infoGAN}(D,G,Q) = V(D,G) - \lambda L_I(G,Q)<br>$$</p><p>其中，$L_1(G,Q)=E_{c\sim P(c),x\sim G(z,c)}[\log Q(c|x)]+H(c)$</p><p>也就是：<br>$$<br>L_{D,Q}=L_D^{GAN} - \lambda L_1(c,c’) \ L_{G} = L_G^{GAN} - \lambda L_1(c,c’)<br>$$</p><p>###WGAN</p><p>WGAN的假设<br>$$<br>L_D^{WGAN} = E[D(x)]-E[D(G(z))]<br>$$</p><p>$$<br>L_G^{WGAN} = E[D(G(Z))]<br>$$</p><p>$$<br>W_D\leftarrow clip_by_value(W_D, -0.01, 0.01)<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近写了一下李宏毅的MLDS 2017的&lt;a href=&quot;https://www.csie.ntu.edu.tw/~yvchen/f106-adl/A4&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;HW4-Comics Generation&lt;/a&gt;，正好
      
    
    </summary>
    
      <category term="algorithms" scheme="http://wulimengmeng.top/categories/algorithms/"/>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="GAN" scheme="http://wulimengmeng.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>CS224n-assignment2</title>
    <link href="http://wulimengmeng.top/2017/12/20/CS224n-assignment2/"/>
    <id>http://wulimengmeng.top/2017/12/20/CS224n-assignment2/</id>
    <published>2017-12-20T06:31:18.000Z</published>
    <updated>2017-12-21T09:01:56.596Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Tensorflow-Softmax"><a href="#Tensorflow-Softmax" class="headerlink" title="Tensorflow Softmax"></a>Tensorflow Softmax</h2><p>(a) 用TensorFlow实现softmax</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the softmax function in tensorflow.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        x:   tf.Tensor with shape (n_samples, n_features).</span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">        out: tf.Tensor with shape (n_sample, n_features). You need to construct this</span></div><div class="line"><span class="string">                  tensor in this problem.</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    x -= tf.reduce_max(x, axis=<span class="number">1</span>, keep_dims=<span class="keyword">True</span>)</div><div class="line">    out = tf.exp(x) / tf.reduce_sum(tf.exp(x), axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> out</div></pre></td></tr></table></figure><p>(b) 实现TensorFlow实现cross entropy loss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_loss</span><span class="params">(y, yhat)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the cross entropy loss in tensorflow.</span></div><div class="line"><span class="string">    The loss should be summed over the current minibatch.</span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.</span></div><div class="line"><span class="string">        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a</span></div><div class="line"><span class="string">                    probability distribution and should sum to 1.</span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this</span></div><div class="line"><span class="string">                    tensor in the problem.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    out = -tf.reduce_sum((tf.to_float(y)*tf.log(yhat)))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> out</div></pre></td></tr></table></figure><p>(c), (d), (e) 实现简单的softmax classifer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxModel</span><span class="params">(Model)</span>:</span></div><div class="line">    <span class="string">"""Implements a Softmax classifier with cross-entropy loss."""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_placeholders</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""Generates placeholder variables to represent the input tensors.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        These placeholders are used as inputs by the rest of the model building</span></div><div class="line"><span class="string">        and will be fed data during training.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Adds following nodes to the computational graph</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        input_placeholder: Input placeholder tensor of shape</span></div><div class="line"><span class="string">                                              (batch_size, n_features), type tf.float32</span></div><div class="line"><span class="string">        labels_placeholder: Labels placeholder tensor of shape</span></div><div class="line"><span class="string">                                              (batch_size, n_classes), type tf.int32</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Add these placeholders to self as the instance variables</span></div><div class="line"><span class="string">            self.input_placeholder</span></div><div class="line"><span class="string">            self.labels_placeholder</span></div><div class="line"><span class="string">        """</span></div><div class="line">        self.input_placeholder = tf.placeholder(dtype=tf.float32, shape=(Config.batch_size, Config.n_features))</div><div class="line">        self.labels_placeholder = tf.placeholder(dtype=tf.int32, shape=(Config.batch_size, Config.n_classes))</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_feed_dict</span><span class="params">(self, inputs_batch, labels_batch=None)</span>:</span></div><div class="line">        <span class="string">"""Creates the feed_dict for training the given step.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        A feed_dict takes the form of:</span></div><div class="line"><span class="string">        feed_dict = &#123;</span></div><div class="line"><span class="string">                &lt;placeholder&gt;: &lt;tensor of values to be passed for placeholder&gt;,</span></div><div class="line"><span class="string">                ....</span></div><div class="line"><span class="string">        &#125;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        If label_batch is None, then no labels are added to feed_dict.</span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            inputs_batch: A batch of input data.</span></div><div class="line"><span class="string">            labels_batch: A batch of label data.</span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            feed_dict: The feed dictionary mapping from placeholders to values.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        feed_dict = &#123;</div><div class="line">            self.input_placeholder: inputs_batch,</div><div class="line">            self.labels_placeholder: labels_batch</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> feed_dict</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_prediction_op</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""Adds the core transformation for this model which transforms a batch of input</span></div><div class="line"><span class="string">        data into a batch of predictions. In this case, the transformation is a linear layer plus a</span></div><div class="line"><span class="string">        softmax transformation:</span></div><div class="line"><span class="string">        y = softmax(Wx + b)</span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            input_data: A tensor of shape (batch_size, n_features).</span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            pred: A tensor of shape (batch_size, n_classes)</span></div><div class="line"><span class="string">        """</span></div><div class="line">        W = tf.Variable(tf.zeros(shape=[self.config.n_features, self.config.n_classes]))</div><div class="line">        b = tf.Variable(tf.zeros(shape=[self.config.n_classes]))</div><div class="line">        pred = tf.add(tf.matmul(self.input_placeholder, W), b)</div><div class="line">        pred = softmax(pred)</div><div class="line">        <span class="keyword">return</span> pred</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_loss_op</span><span class="params">(self, pred)</span>:</span></div><div class="line">        <span class="string">"""Adds cross_entropy_loss ops to the computational graph.</span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            pred: A tensor of shape (batch_size, n_classes)</span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            loss: A 0-d tensor (scalar)</span></div><div class="line"><span class="string">        """</span></div><div class="line">        loss = cross_entropy_loss(self.labels_placeholder, pred)</div><div class="line">        <span class="keyword">return</span> loss</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_training_op</span><span class="params">(self, loss)</span>:</span></div><div class="line">        <span class="string">"""Sets up the training Ops.</span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            loss: Loss tensor, from cross_entropy_loss.</span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            train_op: The Op for training.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        optimizer = tf.train.GradientDescentOptimizer(Config.lr)</div><div class="line">        train_op = optimizer.minimize(loss)</div><div class="line">        <span class="keyword">return</span> train_op</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(self, sess, inputs, labels)</span>:</span></div><div class="line">        <span class="string">"""Runs an epoch of training.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            sess: tf.Session() object</span></div><div class="line"><span class="string">            inputs: np.ndarray of shape (n_samples, n_features)</span></div><div class="line"><span class="string">            labels: np.ndarray of shape (n_samples, n_classes)</span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            average_loss: scalar. Average minibatch loss of model on epoch.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        n_minibatches, total_loss = <span class="number">0</span>, <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> input_batch, labels_batch <span class="keyword">in</span> get_minibatches([inputs, labels], self.config.batch_size):</div><div class="line">            n_minibatches += <span class="number">1</span></div><div class="line">            total_loss += self.train_on_batch(sess, input_batch, labels_batch)</div><div class="line">        <span class="keyword">return</span> total_loss / n_minibatches</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, sess, inputs, labels)</span>:</span></div><div class="line">        <span class="string">"""Fit model on provided data.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            sess: tf.Session()</span></div><div class="line"><span class="string">            inputs: np.ndarray of shape (n_samples, n_features)</span></div><div class="line"><span class="string">            labels: np.ndarray of shape (n_samples, n_classes)</span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">            losses: list of loss per epoch</span></div><div class="line"><span class="string">        """</span></div><div class="line">        losses = []</div><div class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(self.config.n_epochs):</div><div class="line">            start_time = time.time()</div><div class="line">            average_loss = self.run_epoch(sess, inputs, labels)</div><div class="line">            duration = time.time() - start_time</div><div class="line">            <span class="keyword">print</span> <span class="string">'Epoch &#123;:&#125;: loss = &#123;:.2f&#125; (&#123;:.3f&#125; sec)'</span>.format(epoch, average_loss, duration)</div><div class="line">            losses.append(average_loss)</div><div class="line">        <span class="keyword">return</span> losses</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></div><div class="line">        <span class="string">"""Initializes the model.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Args:</span></div><div class="line"><span class="string">            config: A model configuration object of type Config</span></div><div class="line"><span class="string">        """</span></div><div class="line">        self.config = config</div><div class="line">        self.build()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Tensorflow-Softmax&quot;&gt;&lt;a href=&quot;#Tensorflow-Softmax&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow Softmax&quot;&gt;&lt;/a&gt;Tensorflow Softmax&lt;/h2&gt;&lt;p&gt;(a) 用T
      
    
    </summary>
    
      <category term="algorithms" scheme="http://wulimengmeng.top/categories/algorithms/"/>
    
    
      <category term="notes" scheme="http://wulimengmeng.top/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>CS224n assignment1</title>
    <link href="http://wulimengmeng.top/2017/12/18/CS224n-assignment1/"/>
    <id>http://wulimengmeng.top/2017/12/18/CS224n-assignment1/</id>
    <published>2017-12-18T03:30:15.000Z</published>
    <updated>2017-12-19T03:17:43.973Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>(a) 证明softmax(x) = softmax(x+c), 这样就可以把c设为$\max(x)$来保证数值计算的稳定性</p><p>$$<br>softmax(x)_i = \frac{e^{x_i}}{\sum_je^{x_j}}<br>$$</p><p>$$<br>(softmax(x+c))_i = \frac{\exp(x_i+c)}{\sum_{j=1}\exp(x_j+c)}=\ \frac{\exp(x_i)\exp(c)}{\exp(c)\sum_{j=1}\exp(x_j)} = \frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}<br>$$</p><p>(b) 实现q1_softmax.py: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""Compute the softmax function for each row of the input x.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    x -- A N dimensional vector or M x N dimensional numpy matrix.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    x -- You are allowed to modify x in-place</span></div><div class="line"><span class="string">    """</span></div><div class="line">    orig_shape = x.shape</div><div class="line"></div><div class="line">    <span class="keyword">if</span> len(x.shape) &gt; <span class="number">1</span>:</div><div class="line">        <span class="comment"># Matrix</span></div><div class="line">        x -= np.max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">        x = np.exp(x) / np.sum(np.exp(x), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># Vector</span></div><div class="line">        x -= np.max(x)</div><div class="line">        x = np.exp(x) / np.sum(np.exp(x))</div><div class="line"></div><div class="line">    <span class="keyword">assert</span> x.shape == orig_shape</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure><h2 id="Neural-Network-Basics"><a href="#Neural-Network-Basics" class="headerlink" title="Neural Network Basics"></a>Neural Network Basics</h2><p>(a) 推导一下sigmoid函数的导数：<br>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$</p><p>$$<br>\sigma^\prime(x) = \sigma(x)(1 − \sigma(x))<br>$$</p><p>(b) 推导一下softmax函数的导数：<br>$$<br>CE(y,\hat{y}) = -\sum_i y_i \log(\hat{y}_i), \hat{y} = softmax(\theta)<br>$$<br>k是目标类<br><span>$$\frac{\partial CE(y,\hat{y})}{\partial \theta_i} =  \left\{\begin{align} &amp;\hat{y_i} - 1,i=k \\ &amp;\hat{y_i}, otherwise\end{align}\right.$$</span><!-- Has MathJax --><br>等价于：</p><p>$$<br>\frac{\partial CE(y,\hat{y})}{\partial \theta} = \hat{y} -y<br>$$</p><p>(c) x是一层神经网络的输入，推导x的梯度也就是$\frac{\partial J}{\partial x}$, $J = CE(y, \hat{y})$，神经网络的隐藏层激活函数是$sigmoid$，而最后一层的是$softmax$</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-18%20%E4%B8%8B%E5%8D%8812.15.18.png" alt=""><br>$$<br>z1 = xW_1 + b_1, h = sigmoid(z_1), z_2=hW_2 + b_2, \hat{y} = softmax(z_2),<br>$$</p><p>$$<br>\frac{\partial J}{\partial x} = \frac{\partial J}{\partial z_2} \frac{\partial z_2}{\partial h}\frac{\partial h}{\partial z_1}\frac{\partial z_1}{\partial x}<br>$$</p><p>$$<br>\frac{\partial J}{\partial z_2} = \hat{y} -y<br>$$</p><p>$$<br>\frac{\partial z_2}{\partial h} = W_2<br>$$</p><p>$$<br>\frac{\partial h}{\partial z_1} = sigmoid(z_1) (1-sigmoid(z_1))<br>$$</p><p>$$<br>\frac{\partial z_1}{\partial x} = W_1<br>$$</p><p>(d) 上个网络的参数个数, 输入的维度是$D_x$,输出的维度是$D_y$, 隐藏层是H：<br>$$<br>D_x \cdot H + H + H \cdot D_y + D_y<br>$$<br>(e) 实现q2 sigmoid.py:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the sigmoid function for the input here.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    x -- A scalar or numpy array.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    s -- sigmoid(x)</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    s = <span class="number">1</span> / (<span class="number">1</span>+np.exp(-x))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> s</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the gradient for the sigmoid function here. Note that</span></div><div class="line"><span class="string">    for this implementation, the input s should be the sigmoid</span></div><div class="line"><span class="string">    function value of your original input x.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    s -- A scalar or numpy array.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    ds -- Your computed gradient.</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    ds = s * (<span class="number">1</span>-s)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> ds</div></pre></td></tr></table></figure><p>(f) 实现梯度检查: q2 gradcheck.py<br>$$<br>\frac{\partial J(\theta)}{\partial \theta} = \lim_{\epsilon\rightarrow0}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradcheck_naive</span><span class="params">(f, x)</span>:</span></div><div class="line">    <span class="string">""" Gradient check for a function f.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    f -- a function that takes a single argument and outputs the</span></div><div class="line"><span class="string">         cost and its gradients</span></div><div class="line"><span class="string">    x -- the point (numpy array) to check the gradient at</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    rndstate = random.getstate()</div><div class="line">    random.setstate(rndstate)</div><div class="line">    fx, grad = f(x) <span class="comment"># Evaluate function value at original point</span></div><div class="line">    h = <span class="number">1e-4</span>        <span class="comment"># Do not change this!</span></div><div class="line"></div><div class="line">    <span class="comment"># Iterate over all indexes in x</span></div><div class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</div><div class="line">        ix = it.multi_index</div><div class="line"></div><div class="line">        <span class="comment"># Try modifying x[ix] with h defined above to compute</span></div><div class="line">        <span class="comment"># numerical gradients. Make sure you call random.setstate(rndstate)</span></div><div class="line">        <span class="comment"># before calling f(x) each time. This will make it possible</span></div><div class="line">        <span class="comment"># to test cost functions with built in randomness later.</span></div><div class="line"></div><div class="line">        old_xix = x[ix]</div><div class="line">        x[ix] = old_xix + h</div><div class="line">        random.setstate(rndstate)</div><div class="line">        fp = f(x)[<span class="number">0</span>]</div><div class="line">        x[ix] = old_xix - h</div><div class="line">        random.setstate(rndstate)</div><div class="line">        fm = f(x)[<span class="number">0</span>]</div><div class="line">        x[ix] = old_xix</div><div class="line">        <span class="comment">#random.setstate(rndstate)</span></div><div class="line">        numgrad = (fp-fm) / (<span class="number">2</span>*h)</div><div class="line">        <span class="comment"># Compare gradients</span></div><div class="line">        reldiff = abs(numgrad - grad[ix]) / max(<span class="number">1</span>, abs(numgrad), abs(grad[ix]))</div><div class="line">        <span class="keyword">if</span> reldiff &gt; <span class="number">1e-5</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Gradient check failed."</span></div><div class="line">            <span class="keyword">print</span> <span class="string">"First gradient error found at index %s"</span> % str(ix)</div><div class="line">            <span class="keyword">print</span> <span class="string">"Your gradient: %f \t Numerical gradient: %f"</span> % (</div><div class="line">                grad[ix], numgrad)</div><div class="line">            <span class="keyword">return</span></div><div class="line"></div><div class="line">        it.iternext() <span class="comment"># Step to next dimension</span></div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Gradient check passed!"</span></div></pre></td></tr></table></figure><p>(g) 实现: q2 neural.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Forward and backward propagation for a two-layer sigmoidal network</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Compute the forward propagation and for the cross entropy cost,</span></div><div class="line"><span class="string">    and backward propagation for the gradients for all parameters.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    data -- M x Dx matrix, where each row is a training example.</span></div><div class="line"><span class="string">    labels -- M x Dy matrix, where each row is a one-hot vector.</span></div><div class="line"><span class="string">    params -- Model parameters, these are unpacked for you.</span></div><div class="line"><span class="string">    dimensions -- A tuple of input dimension, number of hidden units</span></div><div class="line"><span class="string">                  and output dimension</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    <span class="comment">### Unpack network parameters (do not modify)</span></div><div class="line">    ofs = <span class="number">0</span></div><div class="line">    Dx, H, Dy = (dimensions[<span class="number">0</span>], dimensions[<span class="number">1</span>], dimensions[<span class="number">2</span>])</div><div class="line"></div><div class="line">    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))</div><div class="line">    ofs += Dx * H</div><div class="line">    b1 = np.reshape(params[ofs:ofs + H], (<span class="number">1</span>, H))</div><div class="line">    ofs += H</div><div class="line">    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))</div><div class="line">    ofs += H * Dy</div><div class="line">    b2 = np.reshape(params[ofs:ofs + Dy], (<span class="number">1</span>, Dy))</div><div class="line"></div><div class="line">    </div><div class="line">    z1 = np.dot(data, W1) + b1</div><div class="line">    h1 = sigmoid(z1)</div><div class="line">    z2 = np.dot(h1, W2) + b2</div><div class="line">    y = softmax(z2)</div><div class="line"></div><div class="line">    cost = -np.sum(labels * np.log(y))</div><div class="line"></div><div class="line">    gradz2 = y - labels</div><div class="line"></div><div class="line"></div><div class="line">    gradW2 = np.dot(h1.T, gradz2)</div><div class="line">    gradb2 = np.sum(gradz2, axis=<span class="number">0</span>).reshape((<span class="number">1</span>, Dy))</div><div class="line"></div><div class="line">    gradh1 = np.dot(gradz2, W2.T)</div><div class="line">    gradz1 = gradh1 * sigmoid_grad(h1)</div><div class="line"></div><div class="line">    gradW1 = np.dot(data.T, gradz1)</div><div class="line">    gradb1 = np.sum(gradz1, axis=<span class="number">0</span>).reshape((<span class="number">1</span>, H))</div><div class="line"></div><div class="line">    <span class="keyword">assert</span> gradW1.shape == W1.shape</div><div class="line">    <span class="keyword">assert</span> gradW2.shape == W2.shape</div><div class="line">    <span class="comment">### Stack gradients (do not modify)</span></div><div class="line">    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),</div><div class="line">        gradW2.flatten(), gradb2.flatten()))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, grad</div></pre></td></tr></table></figure><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>主要包括word embeeding中的两个模型： Skip-gram和CBOW</p><ol><li>skipgram:Predict context words given target (position independent)</li></ol><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-18%20%E4%B8%8B%E5%8D%883.47.29.png" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fml280dpabj313q0to1kx.jpg" alt=""></p><ol><li>Predict target word from bag-of-words context</li></ol><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fml67e6yo4j30fa0cpdht.jpg" alt=""></p><p>(a) 求skipgram的关于$v_c$和$\mu_w$的梯度： </p><p>$$<br>\hat{y}_o= p(o|c)=\frac{\exp(\mu_o^T v_c)}{\sum_{w=1}^W\exp(\mu_w^T v_c)}<br>$$</p><p>o表示输出词的下标，c表示的是中心词的下标，$u_o$表示输出向量</p><p>预测的词向量$v_c$代表第c个中心词，$w$表示的是第w个词, i表示目标。<br>$$<br>J_{softmax-CE}(o,v_c, U) = CE(y, \hat{y}), U= [u_1,u_2,…,u_W]<br>$$</p><p>$$<br>\frac{\partial J}{\partial v_c} = -u_i + \sum_{w=1}^Wu_w\hat{y}_w = U(\hat{y}-y)<br>$$</p><span>$$\frac{\partial J}{\partial u_w} =  \left\{\begin{align} &amp;(\hat{y_w} - 1)v_c,w=o \\ &amp;\hat{y_w}v_c, otherwise\end{align}\right.$$</span><!-- Has MathJax --><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmaxCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset)</span>:</span></div><div class="line">    <span class="string">""" Softmax cost function for word2vec models</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    predicted -- numpy ndarray, predicted word vector (\hat&#123;v&#125; in</span></div><div class="line"><span class="string">                 the written component)</span></div><div class="line"><span class="string">    target -- integer, the index of the target word</span></div><div class="line"><span class="string">    outputVectors -- "output" vectors (as rows) for all tokens</span></div><div class="line"><span class="string">    dataset -- needed for negative sampling, unused here.   </span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    cost -- cross entropy cost for the softmax word prediction</span></div><div class="line"><span class="string">    gradPred -- the gradient with respect to the predicted word</span></div><div class="line"><span class="string">           vector</span></div><div class="line"><span class="string">    grad -- the gradient with respect to all the other word</span></div><div class="line"><span class="string">           vectors</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    """</span></div><div class="line">    out = np.dot(outputVectors, predicted)</div><div class="line">    score = out[target]</div><div class="line">    exp_sum = np.sum(np.exp(out))</div><div class="line">    cost = np.log(exp_sum) - score</div><div class="line">    margin = np.exp(out) / np.sum(np.exp(out))</div><div class="line">    margin[target] -= <span class="number">1</span> </div><div class="line">    gradPred = np.dot(margin.T, outputVectors)</div><div class="line">    grad = np.dot(margin, predicted.T)</div><div class="line">    <span class="keyword">return</span> cost, gradPred, grad</div></pre></td></tr></table></figure><p>(b) negative sampling:  更新全词表的代价有点大，从而负采样K个，更新。$v_c$是预测的词向量，$o$是期望输出词<br>$$<br>J_{neg-sample}(o,v_c,U) = -\log(\sigma(u_o^Tv_c)) - \sum_{k=1}^K \log(\sigma(-u_k^Tv_c))<br>$$</p><p>$$<br>\frac{\partial J}{\partial v_c} =(\sigma(u_o^Tv_c)-1)u_o-\sum_{k=1}^K(\sigma(-u_k^Tv_c)-1)u_k<br>$$</p><p>$$<br>\frac{\partial J}{\partial u_o} =(\sigma(u_o^Tv_c)-1)v_c<br>$$</p><p>$$<br>\frac{\partial J}{\partial u_k} =-(\sigma(-u_k^Tv_c)-1)v_c, k = 1,2,…,K<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNegativeSamples</span><span class="params">(target, dataset, K)</span>:</span></div><div class="line">    <span class="string">""" Samples K indexes which are not the target """</span></div><div class="line"></div><div class="line">    indices = [<span class="keyword">None</span>] * K</div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> xrange(K):</div><div class="line">        newidx = dataset.sampleTokenIdx()</div><div class="line">        <span class="keyword">while</span> newidx == target:</div><div class="line">            newidx = dataset.sampleTokenIdx()</div><div class="line">        indices[k] = newidx</div><div class="line">    <span class="keyword">return</span> indices</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset,</span></span></div><div class="line"><span class="function"><span class="params">                               K=<span class="number">10</span>)</span>:</span></div><div class="line">    <span class="string">""" Negative sampling cost function for word2vec models</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Implement the cost and gradients for one predicted word vector</span></div><div class="line"><span class="string">    and one target word vector as a building block for word2vec</span></div><div class="line"><span class="string">    models, using the negative sampling technique. K is the sample</span></div><div class="line"><span class="string">    size.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Note: See test_word2vec below for dataset's initialization.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments/Return Specifications: same as softmaxCostAndGradient</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    <span class="comment"># Sampling of indices is done for you. Do not modify this if you</span></div><div class="line">    <span class="comment"># wish to match the autograder and receive points!</span></div><div class="line">    indices = [target]</div><div class="line">    indices.extend(getNegativeSamples(target, dataset, K))</div><div class="line"></div><div class="line">    labels = -np.ones((K+<span class="number">1</span>,))</div><div class="line">    labels[<span class="number">0</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">    out = np.dot(outputVectors[indices], predicted) * labels</div><div class="line">    </div><div class="line">    scores = sigmoid(out)</div><div class="line">    cost = -np.sum(np.log(scores))</div><div class="line"></div><div class="line">    d = labels * (scores<span class="number">-1</span>)</div><div class="line">    gradPred = np.dot(d.reshape((<span class="number">1</span>, <span class="number">-1</span>)), outputVectors[indices]).flatten()</div><div class="line">    gradtemp = np.dot(d.reshape((<span class="number">-1</span>, <span class="number">1</span>)), predicted.reshape((<span class="number">1</span>,<span class="number">-1</span>)))</div><div class="line">    grad = np.zeros_like(outputVectors)</div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(K+<span class="number">1</span>):</div><div class="line">        grad[indices[k]] += gradtemp[k,:]</div><div class="line">    <span class="keyword">return</span> cost, gradPred, grad</div></pre></td></tr></table></figure><p>(c) 推导skip gram和CBOW的梯度：</p><p>给定一系列的上下文单词$[word_{c-m},…,word_{c-1},word_c,word_{c+1},…,word_{c+m}]$</p><p>输入词向量为$v_k$,输出词向量为$u_k$, $\hat{v}=v_c$</p><p>这里， skip gram的cost函数为：<br>$$<br>J_{skip_gram}(word_{c-m…c+m}) = \sum_{-m\le j\le m, j\ne0} F(w_{c+j}, v_c)<br>$$</p><p>$$<br>\frac{\partial J_{skip_gram}(word_{c-m…c+m})}{\partial U} =\sum_{-m\le j\le m, j\ne0} \frac{\partial F(w_{c+j}, v_c)}{\partial U}<br>$$</p><p>$$<br>\frac{\partial J_{skip_gram}(word_{c-m…c+m})}{\partial v_c} =\sum_{-m\le j\le m, j\ne0} \frac{\partial F(w_{c+j}, v_c)}{\partial v_c}<br>$$</p><p>$$<br>\frac{\partial J_{skip_gram}(word_{c-m…c+m})}{\partial v_j} =0, j \ne c<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">skipgram</span><span class="params">(currentWord, C, contextWords, tokens, inputVectors, outputVectors,</span></span></div><div class="line"><span class="function"><span class="params">             dataset, word2vecCostAndGradient=softmaxCostAndGradient)</span>:</span></div><div class="line">    <span class="string">""" Skip-gram model in word2vec</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    currrentWord -- a string of the current center word</span></div><div class="line"><span class="string">    C -- integer, context size</span></div><div class="line"><span class="string">    contextWords -- list of no more than 2*C strings, the context words</span></div><div class="line"><span class="string">    tokens -- a dictionary that maps words to their indices in</span></div><div class="line"><span class="string">              the word vector list</span></div><div class="line"><span class="string">    inputVectors -- "input" word vectors (as rows) for all tokens</span></div><div class="line"><span class="string">    outputVectors -- "output" word vectors (as rows) for all tokens</span></div><div class="line"><span class="string">    word2vecCostAndGradient -- the cost and gradient function for</span></div><div class="line"><span class="string">                               a prediction vector given the target</span></div><div class="line"><span class="string">                               word vectors, could be one of the two</span></div><div class="line"><span class="string">                               cost functions you implemented above.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    cost -- the cost function value for the skip-gram model</span></div><div class="line"><span class="string">    grad -- the gradient with respect to the word vectors</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    cost = <span class="number">0.0</span></div><div class="line">    gradIn = np.zeros(inputVectors.shape)</div><div class="line">    gradOut = np.zeros(outputVectors.shape)</div><div class="line"></div><div class="line">    </div><div class="line">    center = tokens[currentWord]</div><div class="line">    predicted = inputVectors[center]</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> target_word <span class="keyword">in</span> contextWords:</div><div class="line">        target = tokens[target_word]</div><div class="line">        cost_i, gradPred, grad = word2vecCostAndGradient(predicted, target, outputVectors, dataset)</div><div class="line">        cost += cost_i</div><div class="line">        gradIn[center] += gradPred</div><div class="line">        gradOut += grad</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, gradIn, gradOut</div></pre></td></tr></table></figure><p>而CBOW有点不同，首先：<br>$$<br>\hat{v} = \sum_{-m\le j\le m, j\ne0} v_{c+j}<br>$$<br>它的cost函数为：<br>$$<br>J_{CBOW}(word_{c-m…c+m})=F(w_c, \hat{v})<br>$$</p><p>$$<br>\frac{\partial J_{CBOW}(word_{c-m…c+m})}{\partial U} = \frac{\partial F(w_c, v_c)}{\partial U}<br>$$</p><p>$$<br>\frac{\partial J_{CBOW}(word_{c-m…c+m})}{\partial v_j} = \frac{\partial F(w_c, v_c)}{\partial \hat{v}}, j\in{c-m,…,c-1,c+1,…,c+m}<br>$$</p><p>$$<br>\frac{\partial J_{CBOW}(word_{c-m…c+m})}{\partial v_j} =0, j\notin{c-m,…,c-1,c+1,…,c+m}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cbow</span><span class="params">(currentWord, C, contextWords, tokens, inputVectors, outputVectors,</span></span></div><div class="line"><span class="function"><span class="params">         dataset, word2vecCostAndGradient=softmaxCostAndGradient)</span>:</span></div><div class="line">    <span class="string">"""CBOW model in word2vec</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Implement the continuous bag-of-words model in this function.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments/Return specifications: same as the skip-gram model</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    cost = <span class="number">0.0</span></div><div class="line">    gradIn = np.zeros(inputVectors.shape)</div><div class="line">    gradOut = np.zeros(outputVectors.shape)</div><div class="line"></div><div class="line">    </div><div class="line">    target = tokens[currentWord]</div><div class="line">    target_vec = inputVectors[target]</div><div class="line">    source_idx = map(<span class="keyword">lambda</span> x: tokens[x], contextWords)</div><div class="line">    predicted = np.sum(inputVectors[source_idx], axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">    cost, gradPred, gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> source_idx:</div><div class="line">        gradIn[idx] += gradPred</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, gradIn, gradOut</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Softmax&quot;&gt;&lt;a href=&quot;#Softmax&quot; class=&quot;headerlink&quot; title=&quot;Softmax&quot;&gt;&lt;/a&gt;Softmax&lt;/h2&gt;&lt;p&gt;(a) 证明softmax(x) = softmax(x+c), 这样就可以把c设为$\max(x)
      
    
    </summary>
    
      <category term="algorithms" scheme="http://wulimengmeng.top/categories/algorithms/"/>
    
    
      <category term="notes" scheme="http://wulimengmeng.top/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Network Architecture of Deblurring</title>
    <link href="http://wulimengmeng.top/2017/12/09/Network-Architecture-of-Deblurring/"/>
    <id>http://wulimengmeng.top/2017/12/09/Network-Architecture-of-Deblurring/</id>
    <published>2017-12-09T09:03:32.000Z</published>
    <updated>2017-12-09T10:03:45.792Z</updated>
    
    <content type="html"><![CDATA[<p>Wieschollek P, Hirsch M, Schölkopf B, et al. Learning Blind Motion Deblurring. arXiv preprint arXiv:1708.04208, 2017. <a href="https://github.com/cgtuebingen/learning-blind-motion-deblurring" target="_blank" rel="external">Codes</a>, <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wieschollek_Learning_Blind_Motion_ICCV_2017_paper.pdf" target="_blank" rel="external">Paper</a></p><p>这篇文章主要是针对视频的去噪，利用前几帧的信息来帮助预测当前帧，用到一些常用的skip-connection的结构来结合low-level and high resolution的feature map。</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.07.49.png" alt=""></p><p>Wang L, Li Y, Wang S. DeepDeblur: Fast one-step blurry face images restoration. arXiv preprint arXiv:1711.09515, 2017.</p><p>这篇文章主要针对的是人脸的运动噪声去模糊，其中kernel是人工模拟的，利用高斯过程生成，网络结构的话就是利用多个inception module和resnet的结构。</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.15.21.png" alt=""></p><p>Kupyn O, Budzan V, Mykhailych M, et al. DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks. arXiv preprint arXiv:1711.07064, 2017. </p><p>这篇文章的结构比较接单，就是利用多个ResBlocks来作为generater，然后在discriminator loss中加入critic loss（用Wasserstein GAN）和perceptual loss(features dissimilarity)。</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.18.01.png" alt=""></p><p>Noroozi M, Chandramouli P, Favaro P. Motion Deblurring in the Wild. arXiv preprint arXiv:1701.01486, 2017. </p><p>主要利用了mutli-scale和skip-connection</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%885.57.08.png" alt=""></p><p>Nah S, Kim T H, Lee K M. Deep multi-scale convolutional neural network for dynamic scene deblurring. arXiv preprint arXiv:1612.02177, 2016. </p><p>这篇文章主要用到了一些残差学习的方法，不仅用了ResBlock，还将小尺度的结果作为残差传给大尺度，简化学习的难度。</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-09%20%E4%B8%8B%E5%8D%886.00.27.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Wieschollek P, Hirsch M, Schölkopf B, et al. Learning Blind Motion Deblurring. arXiv preprint arXiv:1708.04208, 2017. &lt;a href=&quot;https://gi
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="summary" scheme="http://wulimengmeng.top/tags/summary/"/>
    
      <category term="computer vision" scheme="http://wulimengmeng.top/tags/computer-vision/"/>
    
  </entry>
  
  <entry>
    <title>Generate Motion Blur</title>
    <link href="http://wulimengmeng.top/2017/12/05/Generate-Motion-Blur/"/>
    <id>http://wulimengmeng.top/2017/12/05/Generate-Motion-Blur/</id>
    <published>2017-12-05T06:06:27.000Z</published>
    <updated>2017-12-11T07:12:51.799Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍几种常用的人工合成运动噪声的方法：</p><p>###基于spline平滑的方法</p><p>在一个$n\times n$大小的矩阵内，随机采样6个点，再用三阶的spline平滑拟合，这样采样得到若干个在矩阵内的整数点，这些整数点上的值，再用高斯采样得到，然后就是归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_kernel_spline</span><span class="params">(steps, n_samples)</span>:</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(n_samples):</div><div class="line">psz = <span class="number">24</span>  <span class="comment">##矩阵大小</span></div><div class="line">kern = np.zeros((psz, psz))</div><div class="line">x = np.random.randint(<span class="number">1</span>, psz+<span class="number">1</span>, (steps,))</div><div class="line">y = np.random.randint(<span class="number">1</span>, psz+<span class="number">1</span>, (steps,))</div><div class="line"></div><div class="line">x = interpolate.spline(xk=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps), yk=x, xnew=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps*<span class="number">5000</span>))</div><div class="line">y = interpolate.spline(xk=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps), yk=y, xnew=np.linspace(<span class="number">0</span>, <span class="number">1</span>, steps*<span class="number">5000</span>))</div><div class="line"></div><div class="line"></div><div class="line">x = np.round(np.maximum(<span class="number">1</span>, np.minimum(psz, x)))</div><div class="line"></div><div class="line">y = np.round(np.maximum(<span class="number">1</span>, np.minimum(psz, y)))</div><div class="line"></div><div class="line">idxs = (x<span class="number">-1</span>) * psz + y</div><div class="line"></div><div class="line">idxs = np.unique(idxs).astype(int)</div><div class="line"></div><div class="line">wt = np.maximum(<span class="number">0</span>, np.random.randn(idxs.shape[<span class="number">0</span>],) * <span class="number">0.5</span> + <span class="number">1</span>)</div><div class="line"><span class="keyword">if</span> np.sum(wt) == <span class="number">0</span>:</div><div class="line"><span class="keyword">continue</span></div><div class="line">wt /= np.sum(wt)</div><div class="line"><span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(idxs):</div><div class="line">x = idx % psz</div><div class="line">y = idx / psz</div><div class="line">kern[x, y] = wt[i]</div></pre></td></tr></table></figure><h3 id="基于高斯过程的方法"><a href="#基于高斯过程的方法" class="headerlink" title="基于高斯过程的方法"></a>基于高斯过程的方法</h3><blockquote><p>In <a href="https://en.wikipedia.org/wiki/Probability_theory" target="_blank" rel="external">probability theory</a> and <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank" rel="external">statistics</a>, a <strong>Gaussian process</strong> is a particular kind of statistical model where <a href="https://en.wikipedia.org/wiki/Random_variate" target="_blank" rel="external">observations</a> occur in a continuous domain, e.g. time or space. In a Gaussian process, every point in some continuous input space is associated with a <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="external">normally distributed</a> <a href="https://en.wikipedia.org/wiki/Random_variable" target="_blank" rel="external">random variable</a>. Moreover, every finite collection of those random variables has a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" target="_blank" rel="external">multivariate normal distribution</a>, i.e. every finite <a href="https://en.wikipedia.org/wiki/Linear_combination" target="_blank" rel="external">linear combination</a> of them is normally distributed. The distribution of a Gaussian process is the <a href="https://en.wikipedia.org/wiki/Joint_distribution" target="_blank" rel="external">joint distribution</a> of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.</p></blockquote><p>高斯过程其实就是多元高斯分布的无限维度扩展，我们通过观察无限维度的数据的子集（这些子集也服从多元高斯分布），然后构造函数来对数据进行建模。</p><p>例如，我们需要测量一年中每天中午的温度（温度明显是一个有连续空间的变量），这里GP就是一个函数f, 输入${x_n}_{n=1}^{365}$, $f(x_n)$就是每天温度的预测值。 GP函数主要包含两部分： mean function, $m(x)$和kernel function, $k(x, x^\prime)$。</p><p>我们需要对x坐标和y坐标进行采样：<br>$$<br>f_x(t), f_y(t) \sim GP(0, k(t, t’)), k(t,t’) = \sigma_f^2(1+\frac{\sqrt(5)|t-t’|}{l}+\frac{5(t-t’)^2}{3l^2})\exp(-\frac{\sqrt 5|t-t’|}{l})<br>$$<br>这里，$l=0.3$, $\sigma_f=0.25$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(x1, x2)</span>:</span></div><div class="line">sigma_f = <span class="number">1.</span>/<span class="number">4</span></div><div class="line">l = <span class="number">0.3</span></div><div class="line">delta = np.abs(x1-x2)</div><div class="line"><span class="keyword">return</span> sigma_f * sigma_f * (<span class="number">1</span>+np.sqrt(<span class="number">5</span>)*delta/l + <span class="number">5</span> * delta*delta/(<span class="number">3</span>*l*l)) * np.exp(-np.sqrt(<span class="number">5</span>)*delta/l)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(xs)</span>:</span></div><div class="line"><span class="keyword">return</span> [[kernel(x1, x2) <span class="keyword">for</span> x2 <span class="keyword">in</span> xs] <span class="keyword">for</span> x1 <span class="keyword">in</span> xs]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要介绍几种常用的人工合成运动噪声的方法：&lt;/p&gt;
&lt;p&gt;###基于spline平滑的方法&lt;/p&gt;
&lt;p&gt;在一个$n\times n$大小的矩阵内，随机采样6个点，再用三阶的spline平滑拟合，这样采样得到若干个在矩阵内的整数点，这些整数点上的值，再用高斯采样得到，然
      
    
    </summary>
    
    
      <category term="summary" scheme="http://wulimengmeng.top/tags/summary/"/>
    
      <category term="computer vision" scheme="http://wulimengmeng.top/tags/computer-vision/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Capsule Network</title>
    <link href="http://wulimengmeng.top/2017/11/24/capsule/"/>
    <id>http://wulimengmeng.top/2017/11/24/capsule/</id>
    <published>2017-11-24T06:19:55.000Z</published>
    <updated>2017-11-27T05:35:26.652Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">Dynamic Routing Between Capsules</a></p><p>这是Hinton发表在NIPS2017的一篇文章，提出了capsule的概念。</p><p>其实可以把capsule看成是neuron的一个特殊形式，neuron的输出是一个scalar，而capsule则会输出vector。除此之外，neuron可以detect到一个特定的pattern，但是这又存在很大的局限性，会有pattern冗余，例如：</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.11.12.png" alt=""></p><p>所以capsule输出vector就可以避免这样的情况，输出特征v的每个维度表示的是对应pattern的特性，以上图为例，可能某个表示鸟嘴方向的维度，分别对应1和-1。</p><p>再以人脸为例，传统的CNN可能可以detect到眼睛的pattern, 嘴巴的pattern等，只能表示它的存在，但是无法表示五官的属性，例如相对位置，大小，相对角度等等。而向量的大小表示的是整个pattern的概率，或者可以叫做confidence, 例如下图：</p><p><img src="https://jhui.github.io/assets/capsule/face4.jpg" alt=""><img src="https://jhui.github.io/assets/capsule/face5.jpg" alt=""></p><p>具体的计算过程见下图：</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.15.27.png" alt=""></p><p>$$<br>u^1=W^1V^1, u^2=W^2v^2 \ s=c_1u^1 \ v=Squash(s) , v = \frac{|s|}{1+|s|^2}\frac{s}{|s|}<br>$$<br>接下来就是核心，dynamic routing:</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.51.30.png" alt=""></p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-24%20%E4%B8%8B%E5%8D%883.24.52.png" alt=""></p><p>和传统CNN简单粗暴的max pooling不同的是它动态地调整routing的系数，系数是在testing的时候online地决定的，调整的方法就是通过T次迭代，根据aggrement，其实就是提高越相关的v的系数。 上图中，如果$a^r$和$u^i$相关性较强的话，就可以得到更大的$b_i$。</p><p>也可以将dynamic routing的过程看成是一个不断排除outlier的一个过程，例如现在$u^1$,$u^2$很接近，而$u^3$与他们差距很大，他们两个队最终的$a^r$贡献很大，那么随着不断迭代，$u^3$就被消除了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这是Hinton发表在NIPS2017
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="notes" scheme="http://wulimengmeng.top/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Fixing  Weight Decay Regularization in Adam</title>
    <link href="http://wulimengmeng.top/2017/11/21/FIXING-WEIGHT-DECAY-REGULARIZATION-IN-ADAM/"/>
    <id>http://wulimengmeng.top/2017/11/21/FIXING-WEIGHT-DECAY-REGULARIZATION-IN-ADAM/</id>
    <published>2017-11-21T11:05:33.000Z</published>
    <updated>2017-11-27T05:52:22.760Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="external">文章的链接</a></p><p>首先，文章理清了l2正则和weight decay的区别，它们并不是对等的。 weight decay可以表示成:</p><p>$$<br>x_{t+1} = (1-w_t)x_{t}-\alpha_t\nabla f_t(x_t)<br>$$</p><p>而l2正则的表示是：</p><p>$$<br>f_{t,reg(x_t)} = f_t(x_t)+ \frac{w_t}{2} |x_t|_2^2<br>$$</p><p>所以：</p><p>$$<br>\nabla f_{t,reg(x_t)} = \nabla f_t(x_t)+w_tx_t<br>$$</p><p>注意到weight decay的系数只有$w_t$，那么，在大部分框架中，例如tensorflow, keras, pytorch等把weight decay和l2正则等价了，</p><p>我们切换到SGD Mometum中来：因求完梯度以后，需要累加mometum，在$x_t$前面就存在了三个参数：$\alpha$学习率,$w_t$,$\eta_t$平滑系数。那么就和weight decay不对等了，当然可以把这三者乘积看成一个系数，但是这样还是削弱了原本的weight decay（系数变小了）。</p><p>因此，作者把传统的SGD with momentum做了以下修改：</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.17.09.png" alt=""></p><p>简单总结一下： 就是除了在梯度计算中加入weight decay，在mometum也加入了weight decay。这样就增强了weight decay的作用。</p><p>除了对SGD with Mometum有影响，作者还对Adam进行了修改：</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.26.39.png" alt=""></p><p>看一下Adam的公式：</p><p>$$x_t = x_{t-1} - \eta_t\alpha \frac{\beta_1m_{t-1}+(1-\beta_1)g_t}{\sqrt{\beta_2v_{t-1}+(1-\beta_2)g_t^2+\epsilon}}$$</p><p>with $g_t=\nabla f_t(x_{t-1})+w_tx_{t-1}$</p><p>这里可以看到$g_t$被归一化了，同时$w_t$也带着被归一化了，这样$w_t$就被减弱了。</p><p>实验结果：</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.38.45.png" alt=""></p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-21%20%E4%B8%8B%E5%8D%887.38.53.png" alt=""></p><p>横纵坐标分别是不同的weight decay和learning rate的组合, 可以看到learning rate和weight decay的相关性很大，固定weight decay，去调整learning rate，那么效果会变化较大，从图中看到，明显作者提出的算法，最有区域较大，更利于找出最优的参数组合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1711.05101.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;文章的链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先，文章理清了l2正则和weight decay的区别，它们并不是对等的。 wei
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
  </entry>
  
  <entry>
    <title>Dual Path Networks</title>
    <link href="http://wulimengmeng.top/2017/11/14/Networks/"/>
    <id>http://wulimengmeng.top/2017/11/14/Networks/</id>
    <published>2017-11-14T10:05:50.000Z</published>
    <updated>2017-11-27T05:35:58.419Z</updated>
    
    <content type="html"><![CDATA[<p>This paper propose a novel deep CNN architecture called <strong>Dual Path Networks(DPN)</strong>. This idea is  based on the fact that the ResNet enables feature re-usage while DenseNet enable new features exploration which are both important for learning good feature representation.  </p><p><img src="http://ovshqtujw.bkt.clouddn.com/image.png" alt=""></p><blockquote><p>Basically, the ResNet and DenseNet differ in the way of “wiring”. ResNet provides a path with which a layer can get access to both the output and the input of the immediately previous layer. The DenseNet provides a path that can access the outputs of multiple previous layers. </p></blockquote><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-14%20%E4%B8%8B%E5%8D%886.25.45.png" alt="Architecture comparison of different networks"></p><p>The DPN balance ResNet and DenseNet in a tricky way, and can be formulated as:<br>$$<br>x^k = \sum_{t=1}^{k-1} f_t^k (h^t),\ y^k = \sum_{t=1}^{k-1} v_t(h^t) = y^{k-1} +\phi^{k-1}(y^{k-1}),\\r^k=x^k+y^k,\\h^k=g^k(r^k)<br>$$<br>where $x_k$ and $y_k$ denote the extracted information at k-th step from individual path, $v_t(\cdot)$is a feature learning function as $f_k^t(\cdot)$, $\phi_k(\cdot) = f_k(g_k(\cdot))$.  The dual path means the left side is ResNet, the right side is DenseNet. The block parameters are shared between them. The outputs of two sides will be concated as next block’s input.</p><p>This is the implementation of dual path block</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DualPathBlock</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, G, _type=<span class="string">'normal'</span>)</span>:</span></div><div class="line">        super(DualPathBlock, self).__init__()</div><div class="line">        self.num_1x1_c = num_1x1_c</div><div class="line"></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'proj'</span>:</div><div class="line">            key_stride = <span class="number">1</span></div><div class="line">            self.has_proj = <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'down'</span>:</div><div class="line">            key_stride = <span class="number">2</span></div><div class="line">            self.has_proj = <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'normal'</span>:</div><div class="line">            key_stride = <span class="number">1</span></div><div class="line">            self.has_proj = <span class="keyword">False</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.has_proj:</div><div class="line">            self.c1x1_w = self.BN_ReLU_Conv(in_chs=in_chs, out_chs=num_1x1_c+<span class="number">2</span>*inc, kernel_size=<span class="number">1</span>, stride=key_stride)</div><div class="line"></div><div class="line">        self.layers = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'c1x1_a'</span>, self.BN_ReLU_Conv(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)),</div><div class="line">            (<span class="string">'c3x3_b'</span>, self.BN_ReLU_Conv(in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=<span class="number">3</span>, stride=key_stride, padding=<span class="number">1</span>, groups=G)),</div><div class="line">            (<span class="string">'c1x1_c'</span>, self.BN_ReLU_Conv(in_chs=num_3x3_b, out_chs=num_1x1_c+inc, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BN_ReLU_Conv</span><span class="params">(self, in_chs, out_chs, kernel_size, stride, padding=<span class="number">0</span>, groups=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="keyword">return</span> nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'norm'</span>, nn.BatchNorm2d(in_chs)),</div><div class="line">            (<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">            (<span class="string">'conv'</span>, nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        data_in = torch.cat(x, dim=<span class="number">1</span>) <span class="keyword">if</span> isinstance(x, list) <span class="keyword">else</span> x</div><div class="line">        <span class="keyword">if</span> self.has_proj:</div><div class="line">            data_o = self.c1x1_w(data_in)</div><div class="line">            data_o1 = data_o[:,:self.num_1x1_c,:,:]</div><div class="line">            data_o2 = data_o[:,self.num_1x1_c:,:,:]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            data_o1 = x[<span class="number">0</span>]</div><div class="line">            data_o2 = x[<span class="number">1</span>]</div><div class="line"></div><div class="line">        out = self.layers(data_in)</div><div class="line"></div><div class="line">        summ = data_o1 + out[:,:self.num_1x1_c,:,:]</div><div class="line">        dense = torch.cat([data_o2, out[:,self.num_1x1_c:,:,:]], dim=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> [summ, dense]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This paper propose a novel deep CNN architecture called &lt;strong&gt;Dual Path Networks(DPN)&lt;/strong&gt;. This idea is  based on the fact that th
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
  </entry>
  
  <entry>
    <title>Meet in the middle的一些实例</title>
    <link href="http://wulimengmeng.top/2017/11/12/meet-in-the-middle%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E4%BE%8B/"/>
    <id>http://wulimengmeng.top/2017/11/12/meet-in-the-middle的一些实例/</id>
    <published>2017-11-12T08:13:10.000Z</published>
    <updated>2017-11-27T05:35:51.455Z</updated>
    
    <content type="html"><![CDATA[<p>Meet in the Middle是在搜索问题经常会用到的一个技巧，其核心思想就是解决一个A&lt;-&gt;B的问题，分别从A端和B端出发，向对方进发，当他们在中点相遇的时候，就找到了A&lt;-&gt;B的一个解。</p><h3 id="先看一道简单题："><a href="#先看一道简单题：" class="headerlink" title="先看一道简单题："></a>先看一道简单题：</h3><p><a href="http://codeforces.com/contest/888/problem/E" target="_blank" rel="external">CF888E Maximum Subsequence</a></p><p>You are given an array <em>a</em> consisting of <em>n</em> integers, and additionally an integer <em>m</em>. You have to choose some sequence of indices $b_1, b_2, …, b_k (1 \le b_1 \lt b_2 \lt … \lt b_k\le n)$ in such a way that the value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img"> is maximized. Chosen sequence can be empty.</p><p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p><p><strong>Input</strong></p><p>The first line contains two integers <em>n</em> and <em>m</em> ($1 \le n\le 35$, $1 \le m \le 10^9$).</p><p>The second line contains <em>n</em> integers $a_1, a_2, …, a_n$ ($1 \le a_i \le10^9$).</p><p><strong>Output</strong></p><p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p><p>题目意思很简单，就是从一个大小为n的数组中挑选k个，使他们的和对m求余最大。</p><p>如果直接枚举a的所有子集，大小为$2^{35}$， 明显会超时。</p><p>如果利用meet in the middle的思路： 先枚举左边17，右边17，再让他们meet in the middle，然后利用求余的性质，左边和右边的和都小于m，进行排序，二分即可： $L_i+R_i \lt m$   or  $ m \lt L_i + R_i \lt 2*m$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">40</span>;</div><div class="line"></div><div class="line"><span class="keyword">int</span> a[maxn];</div><div class="line"></div><div class="line"><span class="keyword">int</span> L[<span class="number">1</span>&lt;&lt;<span class="number">18</span>], R[<span class="number">1</span>&lt;&lt;<span class="number">18</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="comment">//freopen("in", "r", stdin);</span></div><div class="line"><span class="keyword">int</span> n, m;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</div><div class="line"></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; a[i];</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> ans = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;<span class="number">18</span>); mask++) &#123;</div><div class="line"><span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">18</span>; j++) &#123;</div><div class="line"><span class="keyword">if</span> ((mask &gt;&gt; j) &amp; <span class="number">1</span>) &#123;</div><div class="line">res = (res+a[j]) % m;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">L[mask] = res;</div><div class="line">ans = max(ans, res);</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (n &lt;= <span class="number">18</span>) &#123;</div><div class="line"><span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>)); mask++) &#123;</div><div class="line"><span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">18</span>; j &lt; n; j++) &#123;</div><div class="line"><span class="keyword">if</span> ((mask &gt;&gt; (j<span class="number">-18</span>)) &amp; <span class="number">1</span>) &#123;</div><div class="line">res = (res+a[j]) % m;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">R[mask] = res;</div><div class="line">ans = max(ans, res);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">int</span> Lsz = (<span class="number">1</span>&lt;&lt;<span class="number">18</span>);</div><div class="line"><span class="keyword">int</span> Rsz = (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>));</div><div class="line">sort(R, R+Rsz);</div><div class="line"></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Lsz; i++) &#123;</div><div class="line"><span class="keyword">int</span> res = L[i];</div><div class="line"><span class="keyword">int</span> *t1 = upper_bound(R, R+Rsz, m<span class="number">-1</span>-res);</div><div class="line"><span class="keyword">if</span> (t1 != R) &#123;</div><div class="line">t1--;</div><div class="line">ans = max(ans, (res+(*t1))%m);</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> *t2 = upper_bound(R, R+Rsz, <span class="number">2</span>*m<span class="number">-1</span>-res);</div><div class="line"><span class="keyword">if</span> (t2 != R) &#123;</div><div class="line">t2--;</div><div class="line">ans = max(ans, (res+(*t2))%m);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="进阶一点"><a href="#进阶一点" class="headerlink" title="进阶一点"></a>进阶一点</h3><p><a href="https://community.topcoder.com/stat?c=problem_statement&amp;pm=11644&amp;rd=14548" target="_blank" rel="external">topcoder srm 523 AlphabetPath</a></p><p><strong>Problem Statement</strong></p><p>The original Latin alphabet contained the following 21 letters: </p><p>A B C D E F Z H I K L M N O P Q R S T V X</p><p>You are given a 2-dimensional matrix of characters represented by the String[] letterMaze. The i-th character of the j-th element of letterMaze will represent the character at row i and column j. The matrix will contain each of the 21 letters at least once. It may also contain empty cells marked as ‘.’ (quotes for clarity).</p><p>A path is a sequence of matrix elements such that the second element is (horizontally or vertically) adjacent to the first one, the third element is adjacent to the second one, and so on. No element may be repeated on a path. A Latin alphabet path is a path consisting of exactly 21 elements, each containing a different letter of the Latin alphabet. The letters are not required to be in any particular order.</p><p>Return the total number of Latin alphabet paths in the matrix described by letterMaze.</p><p>题目意思就是给定一个$R\times C$的矩阵，格子里要么是空，要么包含0~20的整数，长度为21的路径，每个整数恰出现一次， $R,C\le 21$</p><p>那么，我们枚举middle点，这样有$R\times C$种选择，假设Middle点为x，从Middle点出发DFS10步，令$S(P)$为不包含Middle点的10个格子的数值的集合。那么$S(P_1)\cup S(P_2)….\cup {x}$ = {0,1…20},那么整个时间复杂度就变成了$O(RC\times 4 \times 3^9)$</p><h3 id="密码学中的应用"><a href="#密码学中的应用" class="headerlink" title="密码学中的应用"></a>密码学中的应用</h3><h4 id="DES"><a href="#DES" class="headerlink" title="DES"></a>DES</h4><p>首先介绍一下DES（Data Encryption Standard），DES是一种分组的对称加密技术，具体见下图（coursera crypto stanford笔记）：</p><p><img src="http://ovshqtujw.bkt.clouddn.com/WechatIMG11.jpeg" alt="DES"></p><p>那么如何attack DES呢？</p><p>Lemma: Suppose that DES is an ideal cipher ($2^{56}$ random invertible functions, key是56位)</p><p>为什么不能用double DES呢？因为我们可以用meet in the middle attack来攻击：</p><p>对于double DES来说：</p><ol><li>我们需要找到这样的$k_1$和$k_2$：$E(k_1, E(k_2, M))=C$,这和$E(k_2, M) = D(k_1, C)$一个意思</li><li>首先，我们用表M记录$k_2$和$C^\prime=DES(k_2, M)$的所有值，时间复杂度为$O(2^{56})$</li><li>然后我们就可以暴力枚举$k_1$，计算$C^{\prime\prime} =DES^{-1}(k_1, C)$, 看是否有对应的值在表中</li><li>这样attack的时间复杂度就变成了$O(2^{56}+2^{56}) \lt O(2^{63})$ ,这比期望的$2^{112}$要小很多，以及空间复杂度为$O(2^{56})$。</li></ol><p>而换成3DES就没有这样的问题了！</p><p>$C = E(K_3, D(K_2, E(K_1,P) ) ) $</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Meet in the Middle是在搜索问题经常会用到的一个技巧，其核心思想就是解决一个A&amp;lt;-&amp;gt;B的问题，分别从A端和B端出发，向对方进发，当他们在中点相遇的时候，就找到了A&amp;lt;-&amp;gt;B的一个解。&lt;/p&gt;
&lt;h3 id=&quot;先看一道简单题：&quot;&gt;&lt;a h
      
    
    </summary>
    
    
      <category term="algorithms" scheme="http://wulimengmeng.top/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>GatedRNN</title>
    <link href="http://wulimengmeng.top/2017/11/12/GatedRNN/"/>
    <id>http://wulimengmeng.top/2017/11/12/GatedRNN/</id>
    <published>2017-11-12T05:35:24.000Z</published>
    <updated>2017-11-27T07:23:16.767Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gated-RNN"><a href="#Gated-RNN" class="headerlink" title="Gated RNN"></a>Gated RNN</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>$$<br>h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime)<br>$$</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.11.44.png" alt="RNN"></p><p>下面是RNN的实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></div><div class="line"><span class="string">  activation function.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for this timestep, of shape (N, D).</span></div><div class="line"><span class="string">  - prev_h: Hidden state from previous timestep, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h = np.tanh(x.dot(Wx)+prev_h.dot(Wh)+b)</div><div class="line">  cache = (next_h,x,prev_h,Wx,Wh)</div><div class="line">  <span class="keyword">return</span> next_h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for a single timestep of a vanilla RNN.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dnext_h: Gradient of loss with respect to next hidden state</span></div><div class="line"><span class="string">  - cache: Cache object from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradients of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></div><div class="line"><span class="string">  - db: Gradients of bias vector, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  next_h,x,prev_h,Wx,Wh = cache</div><div class="line">  dout = dnext_h*(<span class="number">1</span>-next_h**<span class="number">2</span>)</div><div class="line">  db = np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">  dx = dout.dot(Wx.T)</div><div class="line">  dprev_h = dout.dot(Wh.T)</div><div class="line">  dWx = np.dot(x.T,dout)</div><div class="line">  dWh = np.dot(prev_h.T,dout)</div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the RNN forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for the entire timeseries, of shape (N, T, D).</span></div><div class="line"><span class="string">  - h0: Initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></div><div class="line"><span class="string">  - cache: Values needed in the backward pass</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  N,T,D = x.shape</div><div class="line">  N,H = h0.shape</div><div class="line">  cache = []</div><div class="line">  prev_h = h0</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">    prev_h,cache_n = rnn_step_forward(x[:,t,:],prev_h,Wx,Wh,b)</div><div class="line">    cache.append(cache_n)</div><div class="line">    h[:,t,:] = prev_h</div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of all hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of inputs, of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  N,T,H = dh.shape</div><div class="line">  N,D = cache[<span class="number">0</span>][<span class="number">1</span>].shape</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dWx = np.zeros((D,H))</div><div class="line">  dWh = np.zeros((H,H))</div><div class="line">  db = np.zeros((H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">    dx[:,t,:],dprev_h, dWx_n, dWh_n, db_n = rnn_step_backward(dprev_h+dh[:,t,:],cache[t])</div><div class="line">    dWh += dWh_n</div><div class="line">    dWx += dWx_n</div><div class="line">    db += db_n</div><div class="line">  dh0 = dprev_h</div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure><h3 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h3><p>$$<br>h^\prime, y = f_1(h,x) \ b^\prime, c = f_2(b, y) \ ….<br>$$</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.13.12.png" alt="Deep RNN"></p><h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>$$<br>h^\prime, a = f_1(h, x), b^\prime, c= f_2(b, x), y = f_3(a, c)<br>$$</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.15.10.png" alt="双向RNN"></p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>$$<br>z^i = \tanh(W^ix^t+W^ih^{t-1})\\<br>z^f=\tanh(W^fx^t+W^fh^{t-1})\\<br>z^o=\tanh(W^0x^t+W^oh^{t-1})\\<br>$$</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.18.36.png" alt="LSTM"></p><p>对比分析：</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.23.59.png" alt="LSTM对比"></p><p>最左边的是标准的LSTM， 左边第二个是GRU， </p><p>可以看出： 没有output gate，forget gate, input gate, input activation function, output activation function都会对结果变差。forget gate和关于$c^t$的$\tanh$激活函数对性能影响较大。</p><p>下面是LSTM的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for a single timestep of an LSTM.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data, of shape (N, D)</span></div><div class="line"><span class="string">  - prev_h: Previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - prev_c: previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases, of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - next_c: Next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h, next_c, cache = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for a single timestep of an LSTM.        #</span></div><div class="line">  <span class="comment"># You may want to use the numerically stable sigmoid implementation above.  #</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  a = x.dot(Wx)+prev_h.dot(Wh)+b</div><div class="line">  N,H = prev_h.shape</div><div class="line">  i = sigmoid(a[:,:H])</div><div class="line">  f = sigmoid(a[:,H:<span class="number">2</span>*H])</div><div class="line">  o = sigmoid(a[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div><div class="line">  g = np.tanh(a[:,<span class="number">3</span>*H:])</div><div class="line"></div><div class="line">  next_c = f*prev_c + i*g</div><div class="line">  next_h = o*np.tanh(next_c)</div><div class="line">  cache = (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h)</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> next_h, next_c, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass foLSTM: forward</span></div><div class="line"><span class="string">  - dnext_h: Gradients of next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dnext_c: Gradients of next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dprev_c: Gradient of previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dprev_c, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h) = cache</div><div class="line">  (N,H) = dnext_h.shape</div><div class="line">  (N,D) = x.shape</div><div class="line"></div><div class="line"></div><div class="line">  dx = np.zeros(x.shape)</div><div class="line">  dprev_c = np.zeros(prev_c.shape)</div><div class="line">  dprev_h = np.zeros(prev_h.shape)</div><div class="line">  dWx = np.zeros(Wx.shape)</div><div class="line">  dWh = np.zeros(Wh.shape)</div><div class="line">  db = np.zeros(b.shape)</div><div class="line"></div><div class="line"></div><div class="line">  di = dnext_c*g</div><div class="line">  df = dnext_c*prev_c</div><div class="line">  do = dnext_h*np.tanh(next_c)</div><div class="line">  dg = dnext_c*i</div><div class="line"></div><div class="line">  da = np.zeros(a.shape)</div><div class="line"></div><div class="line">  da[:,:H] = di*i*(<span class="number">1</span>-i) <span class="comment">#i</span></div><div class="line">  da[:,H:<span class="number">2</span>*H] = df*f*(<span class="number">1</span>-f) <span class="comment">#f</span></div><div class="line">  da[:,<span class="number">2</span>*H:<span class="number">3</span>*H] = do*o*(<span class="number">1</span>-o) <span class="comment">#o</span></div><div class="line">  da[:,<span class="number">3</span>*H:] = dg*(<span class="number">1</span>-g**<span class="number">2</span>) <span class="comment">#g</span></div><div class="line"></div><div class="line">  dprev_h = np.dot(da,Wh.T)</div><div class="line">  dWx = np.dot(x.T,da)</div><div class="line">  dWh = np.dot(prev_h.T,da)</div><div class="line">  db = np.sum(da,axis=<span class="number">0</span>)</div><div class="line">  dprev_c = dnext_c*f</div><div class="line">  dx = np.dot(da,Wx.T)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for an LSTM over an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the LSTM forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Note that the initial cell state is passed as input, but the initial cell</span></div><div class="line"><span class="string">  state is set to zero. Also note that the cell state is not returned; it is</span></div><div class="line"><span class="string">  an internal variable to the LSTM and is not accessed from outside.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - h0: Initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,T,D) = x.shape</div><div class="line">  (N,H) = h0.shape</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  cache = []</div><div class="line">  prev_c = np.zeros((N,H))</div><div class="line">  prev_h = h0</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">      prev_h,prev_c,cache_n = lstm_step_forward(x[:,t,:],prev_h,prev_c,Wx,Wh,b)</div><div class="line">      cache.append(cache_n)</div><div class="line">      h[:,t,:] = prev_h</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for an LSTM over an entire sequence of data.]</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,D) = cache[<span class="number">0</span>][<span class="number">0</span>].shape</div><div class="line">  (N,T,H) = dh.shape</div><div class="line"></div><div class="line">  dprev_c = np.zeros((N,H))</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dh0 = np.zeros((N,H))</div><div class="line">  dWx = np.zeros((D,<span class="number">4</span>*H))</div><div class="line">  dWh = np.zeros((H,<span class="number">4</span>*H))</div><div class="line">  db= np.zeros((<span class="number">4</span>*H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line"></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">      dx_n, dprev_h, dprev_c, dWx_n, dWh_n, db_n = lstm_step_backward(dh[:,t,:]+dprev_h,dprev_c,cache[t])</div><div class="line">      dWx += dWx_n</div><div class="line">      dWh_n += dWh_n</div><div class="line">      db += db_n</div><div class="line">      dx[:,t,:] = dx_n</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure><p>###GRU</p><p>z在GRU充当的是LSTM里面forget gate和input gate一样的作用，将两者耦合在一起。</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.19.17.png" alt="GRU"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Gated-RNN&quot;&gt;&lt;a href=&quot;#Gated-RNN&quot; class=&quot;headerlink&quot; title=&quot;Gated RNN&quot;&gt;&lt;/a&gt;Gated RNN&lt;/h2&gt;&lt;h3 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink
      
    
    </summary>
    
    
      <category term="RNN" scheme="http://wulimengmeng.top/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://wulimengmeng.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>mixup-Beyond Empirical Risk Minimization</title>
    <link href="http://wulimengmeng.top/2017/11/01/mixup/"/>
    <id>http://wulimengmeng.top/2017/11/01/mixup/</id>
    <published>2017-11-01T13:40:57.000Z</published>
    <updated>2017-11-27T05:35:45.222Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Gist</strong>: The authors propose a new training strategy  dubbed <strong>mixup</strong> that trains a neural network on convex combinations of pairs of examples and their labels and improves the generalization of state-of-the-art neural network architectures.    </p><p>​    </p><p><strong>Pytorch Code</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (x1, y1), (x2, y2) <span class="keyword">in</span> zip(loader1, loader2): </div><div class="line">  lam = numpy.random.beta(alpha, alpha)</div><div class="line">x = Variable(lam * x1 + (<span class="number">1.</span> - lam) * x2)</div><div class="line">y = Variable(lam * y1 + (<span class="number">1.</span> - lam) * y2) optimizer.zero_grad()</div><div class="line">    loss(net(x), y).backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure><p><strong>Empirical Risk Minimization</strong></p><p>We need to minimize the <strong>expected risk</strong>, that is the average of the loss function $l$ over the data distribution $P$</p><p>$$R(f ) = \int l(f (x), y)dP (x, y)$$</p><p>$l$ is the loss function, $P(x,y)$ is a joint data distribution, $f\in F$ is a function that describes the relationship between a random vector X and a random target vector Y .</p><p>Unsually, the distribution of P is unknown. In most pracitical situation, we may approximate $P$ by the <strong><em>empirical distribution</em></strong>, though it is easy to compute, it ofen leads to the undesirable behaviour of $f$ outside the training data.</p><p>$$P_\sigma(x,y)=\frac{1}{n}\sum_{i=1}^{n}\sigma(x=x_i, y=y_i)$$</p><p>where $\sigma(x = x_i, y = y_i)$ is a Dirac mass centered at $(x_i, y_i)$</p><p>$$R_\sigma(f) = \frac{1}{n}\sum_{i=1}^nl(f(x_i), y_i)$$</p><p><strong>Vicinal Risk Minimization</strong></p><p>$$P_v (\widetilde{x}, \widetilde{y})=\frac{1}{n}\sum_{i=1}^nv(\widetilde{x}, \widetilde{y}|x_i,y_i)$$<br>where $v(\widetilde{x}, \widetilde{y}|x_i,y_i)$ is  a vicinity distribution that measures the probability of finding the virtual feature-target pair $(\widetilde{x}, \widetilde{y})$ in the vicinity of the training feature-target pair $(x_i,y_i)$</p><p>This paper propose a generic vicinal distribution, <strong><em>mixup</em></strong>:</p><p>$$\mu(\widetilde{x}, \widetilde{y}|x_i,y_i)=\frac{1}{n}\sum_j^n\mathbb{E}_\lambda[\sigma(\widetilde{x}=\lambda \cdot x_i+(1-\lambda)\cdot x_j,\widetilde{y} =\lambda \cdot y_i + (1-\lambda) \cdot y_j)]$$<br>where $\lambda \sim Beta(\alpha, \alpha)$ , for $\alpha \in (0, \infty)$Sampling from the mixup vicinal distribution:<br>$$\widetilde{x} = \lambda \cdot x_i + (1 − \lambda)\cdot x_j$$<br>$$\widetilde{y} = \lambda \cdot y_i + (1 − \lambda)\cdot y_j$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Gist&lt;/strong&gt;: The authors propose a new training strategy  dubbed &lt;strong&gt;mixup&lt;/strong&gt; that trains a neural network on convex 
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
  </entry>
  
  <entry>
    <title>Single Shot Scale-invariant Face Detector</title>
    <link href="http://wulimengmeng.top/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/"/>
    <id>http://wulimengmeng.top/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/</id>
    <published>2017-11-01T11:16:56.000Z</published>
    <updated>2017-11-01T12:01:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>The authors propose to tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besides, they try to improve the recall rate of small faces by a scale compensation anchor matching strategy. Max-out background label is used to reduce the false positive rate of small faces.</p><p>Key points:</p><ul><li>VGG net (throgh Pool5 layer) and some extra convolutional layers</li><li>Anchor  is 1:1 aspect ratio (face annotation)</li><li>two stages to improve the anchor matching strategy<ul><li>stage one: decrese the jaccord overlap threshold from 0.5 to 0.35</li><li>stage two: decrese the threshold to 0.1 and sort to select the top-N</li></ul></li><li>max-out operation is performed on the background label scores</li></ul><p>model architecture:</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-01%20%E4%B8%8B%E5%8D%887.46.53.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The authors propose to tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besi
      
    
    </summary>
    
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
      <category term="face detection" scheme="http://wulimengmeng.top/tags/face-detection/"/>
    
  </entry>
  
  <entry>
    <title>A List of Saliency Detection Papers</title>
    <link href="http://wulimengmeng.top/2017/10/20/A-List-of-Saliency-Detection-Papers/"/>
    <id>http://wulimengmeng.top/2017/10/20/A-List-of-Saliency-Detection-Papers/</id>
    <published>2017-10-20T03:29:37.000Z</published>
    <updated>2017-10-20T03:49:59.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Deep%20Spatial%20Contextual%20Long-term%20Recurrent%20Convolutional%20Network%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Fast%20and%20Compact%20Saliency%20Score%20Regression%20Network%20Based%20on%20Fully%20Convolutional%20Network.pdf" target="_blank" rel="external">A Fast and Compact Saliency Score Regression Network Based on Fully Convolutional Network</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Amulet.pdf" target="_blank" rel="external">Amulet</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/DHSNet:%20Deep%20Hierarchical%20Saliency%20Network%20for%20Salient%20Object%20Detection%20.pdf" target="_blank" rel="external">DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Group-wise%20Deep%20Co-saliency%20Detection.pdf" target="_blank" rel="external">Group-wise Deep Co-saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Large-Scale%20Optimization%20of%20Hierarchical%20Features%20for%20Saliency%20Prediction%20in%20Natural%20Images.pdf" target="_blank" rel="external">Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Learning%20Uncertain%20Convolutional%20Features%20for%20Accurate%20Saliency%20Detection.pdf" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/PiCANet.pdf" target="_blank" rel="external">PiCANet</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Recurrent%20Attentional%20Networks%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">Recurrent Attentional Networks for Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/SalGAN:%20Visual%20Saliency%20Prediction%20with%20Generative%20Adversarial%20Networks.pdf" target="_blank" rel="external">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Forward%20and%20Backward%20Cues%20in%20Deep-CNNs.pdf" target="_blank" rel="external">Saliency Detection by Forward and Backward Cues in Deep-CNNs</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Multi-Context%20Deep%20Learning.pdf" target="_blank" rel="external">Saliency Detection by Multi-Context Deep Learning.pdf</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Shallow%20and%20Deep%20Convolutional%20Networks%20for%20Saliency%20Prediction.pdf" target="_blank" rel="external">Shallow and Deep Convolutional Networks for Saliency Prediction</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Supervised%20Adversarial%20Networks%20for%20Image%20Saliency%20Detection.pdf" target="_blank" rel="external">Supervised Adversarial Networks for Image Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Two-Stream%20Convolutional%20Networks%20for%20Dynamic%20Saliency%20Prediction.pdf" target="_blank" rel="external">Two-Stream Convolutional Networks for Dynamic Saliency Prediction</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Detection%20Based%20on%20Multiscale%20Deep%20CNN%20Features.pdf" target="_blank" rel="external">Visual Saliency Detection Based on Multiscale Deep CNN Features</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Prediction%20Using%20a%20Mixture%20of%20Deep%20Neural%20Networks.pdf" target="_blank" rel="external">Visual Saliency Prediction Using a Mixture of Deep Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://7xkgro.com1.z0.glb.clouddn.com/A%20Deep%20Spatial%20Contextual%20Long-term%20Recurrent%20Convolutional%20Network%20
      
    
    </summary>
    
    
      <category term="paper" scheme="http://wulimengmeng.top/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>图个新鲜</title>
    <link href="http://wulimengmeng.top/2017/09/05/%E5%9B%BE%E4%B8%AA%E6%96%B0%E9%B2%9C/"/>
    <id>http://wulimengmeng.top/2017/09/05/图个新鲜/</id>
    <published>2017-09-05T09:16:00.000Z</published>
    <updated>2017-11-26T13:31:53.552Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xkgro.com1.z0.glb.clouddn.com/2017-09-05%2017-15-19%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://7xkgro.com1.z0.glb.clouddn.com/2017-09-05%2017-15-19%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="杂" scheme="http://wulimengmeng.top/tags/%E6%9D%82/"/>
    
  </entry>
  
  <entry>
    <title>Overview of Object Detection</title>
    <link href="http://wulimengmeng.top/2017/09/05/overview-of-object-detection/"/>
    <id>http://wulimengmeng.top/2017/09/05/overview-of-object-detection/</id>
    <published>2017-09-05T04:19:00.000Z</published>
    <updated>2017-11-27T05:36:14.593Z</updated>
    
    <content type="html"><![CDATA[<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>主要有三个步骤：</p><ol><li>用selective search提取可能的objects<ol><li>使用<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph Based Image Segmentation</a>中的方法来得到region</li><li>得到所有region之间两两的相似度</li><li>合并最像的两个region</li><li>重新计算新合并region与其他region的相似度</li><li>重复上述过程直到整张图片都聚合成一个大的region</li><li>使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果</li></ol></li><li>用CNN提取特征</li><li>用SVM对区域进行分类</li></ol><p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/rcnn.jpg" alt="[Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; 2014.](https://arxiv.org/abs/1311.2524)"></p><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>在feature map加入RoI Pooling,然今后做分类和回归（位置），这样可以end-to-end的训练了，缺点是依然依赖于selective search</p><p>每一个RoI都有一个四元组$（r,c,h,w）$表示，其中$（r，c）$表示左上角，而$（h，w）$则代表高度和宽度。这一层使用最大池化（max pooling）来将RoI区域转化成固定大小的$H<em>W$的特征图。假设一个RoI的窗口大小为$h</em>w$,则转换成$H<em>W$之后，每一个网格都是一个$h/H </em> w/W$大小的子网，利用最大池化将这个子网中的值映射到$H*W$窗口即可。Pooling对每一个特征图通道都是独立的</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/ROI.png" alt=""></p><p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/fastrcnn.jpg" alt=""></p><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>加入region proposal network，为了代替selective search使得模型能够完全的end-to-end的训练。</p><p>这样的话就存在４个loss:</p><ol><li>RPN分类：是否是object</li><li>RPN box坐标回归</li><li>object分类</li><li>最终的坐标回归</li></ol><p><img src="http://shartoo.github.io/images/blog/rcnn9.png" alt=""></p><p>Anchor:</p><p>Anchors是一组大小固定的参考窗口：三种尺度{ $128^2，256^2，512^2$ }×三种长宽比{1:1，1:2，2:1}，如下图所示，<strong>表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小</strong>，相当于模板，对任意图像任意滑窗位置都是这9种模板。<strong>继而根据图像大小计算滑窗中心点对应原图区域的中心点</strong>，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，由此原图位置并根据与Ground Truth重复率贴上正负标签，让RPN学习该Anchors是否有物体即可。对于每个滑窗位置，产生<strong>k=9</strong>个anchor对于一个大小为$W*H$的卷积feature map，总共会产生$WHk$个anchor。</p><p><img src="http://shartoo.github.io/images/blog/rcnn12.png" alt=""></p><p><img src="https://tryolabs.com/images/blog/post-images/2017-08-30-object-detection/fasterrcnn.jpg" alt="[Ren, Shaoqing, et al. &quot;Faster R-CNN: Towards real-time object detection with region proposal networks.&quot; 2015.](https://arxiv.org/abs/1506.01497)"></p><p><img src="http://img.blog.csdn.net/20160414164536029" alt=""></p><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><strong>同时采用lower和upper的feature map做检测</strong></p><p>假设每个feature map cell有k个default box，那么对于每个default box都需要预测c个类别score和4个offset，那么如果一个feature map的大小是$m\times n$，也就是有<strong>$m\times n$</strong>个feature map cell，那么这个feature map就一共有$（c+4)\times k\times m\times n$ 个输出。这些输出个数的含义是：采用$3\times3$的卷积核对该层的feature map卷积时卷积核的个数，包含两部分：数量$c\times k\times m\times n$是confidence输出，表示每个default box的confidence，也就是类别的概率；数量$4\times k\times m\times n$是localization输出，表示每个default box回归后的坐标）。训练中还有一个东西：<strong>prior box</strong>，是指实际中选择的default box（每一个feature map cell 不是k个default box都取）。</p><ul><li>feature map cell 就是将 feature map 切分成 8×8 或者 4×4 之后的一个个格子；</li><li>而 default box 就是每一个格子上，一系列固定大小的 box，即图中虚线所形成的一系列 boxes。</li></ul><p><img src="http://img.blog.csdn.net/20160918092529925" alt=""></p><p><img src="http://img.blog.csdn.net/20160918092701558" alt=""></p><h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p><img src="http://upload-images.jianshu.io/upload_images/75110-91ee171b49f3ea20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>YOLO首先将图像分为S×S的格子（grid cell）。如果一个目标的中心落入格子，该格子就负责检测该目标。每一个格子（grid cell）预测bounding boxes和该boxes的置信值（confidence score）。置信值代表box包含一个目标的置信度。然后，我们定义置信值为。如果没有目标，置信值为零。另外，我们希望预测的置信值和ground truth的intersection over union (IOU)相同。</p><p>每一个bounding box包含5个值：$x，y，w，h$和confidence。$（x，y）$代表与格子相关的box的中心。$（w，h）$为与全图信息相关的box的宽和高。confidence代表预测boxes的IOU和gound truth。</p><p>每个格子（grid cell）预测条件概率值C($Pr(Class_i|Object) $)。概率值C代表了格子包含一个目标的概率，每一格子只预测一类概率。在测试时，每个box通过类别概率和box置信度相乘来得到特定类别置信分数：<br>$$<br>Pr(Class_i|Object) \cdot Pr(Object)\cdot IOU_{pred}^{truth} = Pr(Class_i)\cdot IOU_{pred}^{truth}<br>$$<br>它将图片划分为S×S的网格，对于每个网格单元预测边界框(B)、边界框的置信度以及类别概率(C)，因此这些预测值可以表示为S×S×(B∗5+C)的张量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;R-CNN&quot;&gt;&lt;a href=&quot;#R-CNN&quot; class=&quot;headerlink&quot; title=&quot;R-CNN&quot;&gt;&lt;/a&gt;R-CNN&lt;/h2&gt;&lt;p&gt;主要有三个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用selective search提取可能的objects&lt;ol&gt;
&lt;l
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
      <category term="notes" scheme="http://wulimengmeng.top/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>如何在圆内均匀采样</title>
    <link href="http://wulimengmeng.top/2017/05/18/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%9C%86%E5%86%85%E5%9D%87%E5%8C%80%E9%87%87%E6%A0%B7/"/>
    <id>http://wulimengmeng.top/2017/05/18/如何在圆内均匀采样/</id>
    <published>2017-05-18T04:19:00.000Z</published>
    <updated>2017-11-27T05:36:27.150Z</updated>
    
    <content type="html"><![CDATA[<p>这个问题之前在面试网易游戏的时候碰到过，当时只想了一个很朴素(naive)的做法，现在有时间重新推导了一下：</p><p>假设圆是一个单元圆，半径为1, 面积为$\pi$。</p><p>当时面试的时候想的方法是用一个长度为1的外接正方形来代替采样，如果在圆内，则返回，不然继续采样，这样的采样方法明显是不稳定的，但是期望的采样步数很容易计算，是$\frac{4}{\pi}$。</p><p>后来想到说可以先采样角度，每个角度确定一个半径，再在半径上均匀采样，面试官有提示说在半径上是均匀的吗？ 明显在半径上采样是不均匀的，因为在半径上的每一个点对应的周长是不一样的！！</p><p>正确的姿势是这样的：</p><p>我们计算长度为$r(0\le r\le1)$的概率为$p(r)$:</p><p>首先我们需要计算落在$r\sim r+\Delta r$的概率，很直观，就是面积/$\pi$，然后因为均匀采样所以需要除以$\Delta r$求得$p(r)$也就是：<br>$$<br>p(r\sim r+\Delta r) = \lim_{\Delta r\rightarrow0}\frac{\pi(r+\Delta r)^2-\pi r^2}{\pi}<br>$$</p><p>$$<br>p(r) = \lim_{\Delta r\rightarrow0}\frac{\pi(r+\Delta r)^2-\pi r^2}{\pi \Delta r}=2r<br>$$</p><p>接下来就是求$p(r)$的CDF,$P(r)$，积分即可：<br>$$<br>P(r) = \int_0^r p(x) dx=r^2<br>$$<br>然后求得它的逆函数$P^{-1}(r)$:</p><p>$$P^{-1}(r) = \sqrt r$$</p><p>算到这里，答案呼之欲出，我们用一个随机变量$\zeta$ 在$[0,1]$均匀采样，然后在通过$r = \sqrt \zeta$求得r,为什么是均匀的呢，只要把$\sqrt \zeta$带入$P(r)$就可以发现$P(r)=\zeta$。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个问题之前在面试网易游戏的时候碰到过，当时只想了一个很朴素(naive)的做法，现在有时间重新推导了一下：&lt;/p&gt;
&lt;p&gt;假设圆是一个单元圆，半径为1, 面积为$\pi$。&lt;/p&gt;
&lt;p&gt;当时面试的时候想的方法是用一个长度为1的外接正方形来代替采样，如果在圆内，则返回，不
      
    
    </summary>
    
    
      <category term="math" scheme="http://wulimengmeng.top/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>一个离散概率分布中采样</title>
    <link href="http://wulimengmeng.top/2017/05/17/%E4%B8%80%E4%B8%AA%E7%A6%BB%E6%95%A3%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E4%B8%AD%E9%87%87%E6%A0%B7/"/>
    <id>http://wulimengmeng.top/2017/05/17/一个离散概率分布中采样/</id>
    <published>2017-05-17T07:40:00.000Z</published>
    <updated>2017-11-27T07:00:40.894Z</updated>
    
    <content type="html"><![CDATA[<p>先考虑一个简单的例子：</p><p>一个n-sided dice. 其每一面是均匀的，也就是每一面的概率都是$\frac{1}{n}$,我们可以把区间[0,1)分成n份，采样的过程就可以变成：从[0,1)采样得到x，然后返回$\lfloor n\times x\rfloor$。</p><blockquote><h4 id="Algorithm-Simulating-a-Fair-Die"><a href="#Algorithm-Simulating-a-Fair-Die" class="headerlink" title="Algorithm: Simulating a Fair Die"></a>Algorithm: Simulating a Fair Die</h4><ol><li>Generate a uniformly-random value xx in the range [0,1).</li><li>Return $⌊x\times n⌋$</li></ol></blockquote><h2 id="朴素的做法"><a href="#朴素的做法" class="headerlink" title="朴素的做法"></a>朴素的做法</h2><p>但如果不是均匀的呢？假设给定各个离散值的概率p(x)，先计算CDF，即 P(x)。然后从[0,1)采样，得到a，我们需要确定a所在的区间，朴素的做法就是二分搜索P（x）（单调性）。所以时间复杂度为$O(\log(n))$</p><h2 id="The-Alias-Method"><a href="#The-Alias-Method" class="headerlink" title="The Alias Method"></a>The Alias Method</h2><p>如果想要用$O(1)$的时间采样呢？</p><p>考虑如下的情况：</p><p>有四个概率：$\frac{1}{2}, \frac{1}{3}, \frac{1}{12},\frac{1}{12}$</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodInitialProbabilities.png" alt=""></p><p>首先，将他们归一化，用均值做归一化操作</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodScaled.png" alt=""></p><p>先画一个$1\times 4$的矩形：</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup.png" alt=""></p><p>可以看到$\frac{1}{2}, \frac{1}{3}$并不是完全在矩形内，如果我们允许将自身的矩阵切除，然后补到其他区域内？例如将$\frac{1}{2}$切掉一部分补到最后那个区域：</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup2.png" alt=""></p><p>到现在，还是在矩阵之外的块，接下来，把$\frac{1}{2}$切掉足够的部分补到第三个中：</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup3.png" alt=""></p><p>最后：</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/aliasMethodSetup4.png" alt=""></p><p>完美！</p><p>从上述可以看出有几条非常赞的性质：</p><ol><li>每个概率对应的面积都没有改变，随之对应的就是每个bar都是满的，这样保证了我每次采样都会命中！</li><li>每个bar最多有两种颜色。</li></ol><p>alias method主要依赖于两张表，一张概率表P还有一张alias表 Alias</p><p>构建完上述的表格以后，如何采样呢？</p><p><img src="http://www.keithschwarz.com/darts-dice-coins/images/completedAliasSetup.png" alt=""></p><p>首先对每列进行采样，列确定后，再采样，利用P和alias。过程非常简单，时间效率是$O(1)$</p><p>接下来就是证明这个alias表和P表是否一定存在！</p><blockquote><p><strong>Theorem:</strong> Given k width-one rectangles of heights $h_0,h_1,…,h_{k−1}$ such that $\sum_{i=0}^{k-1}h_i=k$, there is a way of cutting the rectangles and distributing them into k columns, each of which has height 1, such that each column contains at most two different rectangles and the $i$th column contains at least one piece of the $i$th rectangle.</p></blockquote><p>证明：</p><p>当k=1的时候，很明显是成立的。</p><p>假设当$k=x$的时候成立，那么我们就需要证明$k=x+1$时，是否满足。<br>考虑任一个宽度为$x+1$的矩形，高度分别是：$h_0, h_1, …, h_{k}$, 且满足$\sum_{i = 0}^{k}{h_i} =  x+ 1$,假设一些高度$h_l \le 1$ 还有一些$h_g\ge 1$。不可能同时大于0或者小于0。</p><p>接下来就是用$h_g$把$h_l$填满，这样我们就只剩下$x$个未解决的。所以。。成立！</p><p>具体的做法如下：</p><blockquote><h4 id="Algorithm-Naive-Alias-Method"><a href="#Algorithm-Naive-Alias-Method" class="headerlink" title="Algorithm: Naive Alias Method"></a>Algorithm: Naive Alias Method</h4><ul><li>Initialization:<ol><li>Multiply each probability $p_i$ by n.</li><li>Create arrays Alias and Prob, each of size n.</li><li>For j=1 to n−1:<ol><li>Find a probability pl satisfying $p_l\le1$.</li><li>Find a probability $p_g$ (with $l\ne g$) satisfying $p_g\ge1$</li><li>Set $Prob[l]=p_l$.</li><li>Set $Alias[l]=g$.</li><li>Remove $p_l$ from the list of initial probabilities.</li><li>Set $p_g:=p_g−(1−p_l)$.</li></ol></li><li>Let i be the last probability remaining, which must have weight 1.</li><li>Set $Prob[i]=1$.</li></ol></li><li>Generation:<ol><li>Generate a fair die roll from an n-sided die; call the side i.</li><li>Flip a biased coin that comes up heads with probability $Prob[i]$.</li><li>If the coin comes up “heads,” return i.</li><li>Otherwise, return $Alias[i]$.</li></ol></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先考虑一个简单的例子：&lt;/p&gt;
&lt;p&gt;一个n-sided dice. 其每一面是均匀的，也就是每一面的概率都是$\frac{1}{n}$,我们可以把区间[0,1)分成n份，采样的过程就可以变成：从[0,1)采样得到x，然后返回$\lfloor n\times x\rfloo
      
    
    </summary>
    
    
      <category term="math" scheme="http://wulimengmeng.top/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>CNN Case Stud</title>
    <link href="http://wulimengmeng.top/2017/05/12/cnn-case-study/"/>
    <id>http://wulimengmeng.top/2017/05/12/cnn-case-study/</id>
    <published>2017-05-12T11:00:00.000Z</published>
    <updated>2017-11-27T07:18:09.483Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>结构：</p><p>CONV1-&gt;MAX POOL1-&gt;NORM1-&gt;CONV2-&gt;MAX POOL2-&gt;NORM-&gt;CONV3-&gt;CONV4-&gt;CONV5-&gt;MAX POOL3-&gt;FC6-&gt;FC7-&gt;FC8</p><p>输入： $227\times 227\times 3$ 的图像</p><p>第一层（CONV1），96个$11\times11$的卷积和，stride为4，,因为(227-11)/4+1=55</p><p>参数的大小是$11\times11\times3\times96=35K$,输出为$55\times55\times96$</p><p>第二层（MAX POOL1）， $3\times 3$， 步数为2，因为(55-3)/2+1=27,所以，输出为$27\times27\times96$</p><p>第三层（NORM1）</p><p>第四层CONV2，256个$5\times5$的卷积和，stride为1，pad为2，因为（27-5+2*2）/1+1= 27，所以输出为$27\times27\times256$</p><p>第五层（MAX POOL2）， $3\times 3$， stride为2，因为(27-3)/2+1=13,所以，输出为$13\times13\times256$</p><p>第六层 （NORM2）</p><p>第七层（CONV3），384个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times384$</p><p>第八层（CONV4），384个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times384$</p><p>第九层（CONV5），256个$3\times3$的卷积和，stride为1，pad为1，因为(13-3+1*2)/1+1 = 13,所以输出为$13\times13\times256$</p><p>第十层（MAX POOL2）， $3\times 3$， 步数为2，因为(13-3)/2+1=6,所以，输出为$6\times6\times256$</p><p>第十一层（FC6），4096个neurons</p><p>第十二层（FC7）， 4096个neurons</p><p>第十三层（FC8）， 1000个neurons</p><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.11.48.png" alt=""></p><p>为什么要使用小的卷积核（$3\times3$ conv）?</p><p>因为3个$3\times3$stride为1的conv堆起来的receptive field是和$7\times7$的conv layer是一样的，这样的话网络可以更深，同时非线性能力提高，而且前者参数数量为：$3*(3^2C^2)$ 后者为 $7^2C^2$,前者数量较少。</p><p>INPUT: $[224\times224\times3]$        memory:  $224<em>224</em>3$=150K   params: 0</p><p>CONV3-64:$ [224\times224\times64] $ memory:  $224<em>224</em>64$=3.2M   params:$ (3<em>3</em>3)*64 = 1,728$</p><p>CONV3-64:$ [224\times224\times64]$  memory:  $224<em>224</em>64$ =3.2M   params: $(3<em>3</em>64)*64 = 36,864$</p><p>POOL2: $[112\times112\times64]$  memory:  $112<em>112</em>64$=800K   params: 0</p><p>CONV3-128: $[112\times112\times128]$  memory:  $112<em>112</em>128$=1.6M   params: $(3<em>3</em>64)*128 = 73,728$</p><p>CONV3-128: $[112\times112\times128] $ memory: $112<em>112</em>128$=1.6M   params:$ (3<em>3</em>128)*128 = 147,456 $</p><p>POOL2:$ [56\times56\times128]$  memory:  $56<em>56</em>128=400K$   params: 0</p><p>CONV3-256:$ [56\times56\times256] $ memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>128)*256 = 294,912$</p><p>CONV3-256:$ [56\times56\times256]$  memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>256)*256 = 589,824 $</p><p>CONV3-256: $[56\times56\times256] $ memory:  $56<em>56</em>256=800K$   params:$ (3<em>3</em>256)*256 = 589,824$</p><p>POOL2: $[28\times28\times256]$  memory:  $28<em>28</em>256=200K$   params: 0</p><p>CONV3-512:$ [28\times28\times512] $ memory:  $28<em>28</em>512=400K$   params:$ (3<em>3</em>256)*512 = 1,179,648$</p><p>CONV3-512: $[28\times28\times512]$  memory: $ 28<em>28</em>512=400K $  params:$ (3<em>3</em>512)*512 = 2,359,296$</p><p>CONV3-512:$ [28\times28\times512]$  memory: $ 28<em>28</em>512=400K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p><p>POOL2:$ [14\times14\times512]$  memory:  $14<em>14</em>512=100K$   params: 0 </p><p>CONV3-512:$ [14\times14\times512]$  memory: $ 14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p><p>CONV3-512:$ [14\times14\times512] $ memory: $ 14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296 $</p><p>CONV3-512: $[14\times14\times512]$  memory:  $14<em>14</em>512=100K$   params: $(3<em>3</em>512)*512 = 2,359,296$</p><p>POOL2: $[7\times7\times512] $ memory:  $7<em>7</em>512=25K$  params: 0</p><p>FC: $[1\times1\times4096]$  memory:  4096  params: $7<em>7</em>512*4096 = 102,760,448 $</p><p>FC: $[1\times1\times4096]$  memory:  4096  params: $4096*4096 = 16,777,216$</p><p>FC: $[1\times1\times1000]$  memory:  1000 params: $4096*1000 = 4,096,000 $</p><p>总结一下：对于一张图片来说，需要花费的内存是24M*4 bytes = 96MB，而总共的参数有138M </p><p>VGG的FC7的特征非常棒！通常用来提特征。</p><h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p>22层，有高效的inception module，没有FC层</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.15.37.png" alt=""></p><p>重点分析一下Inception module,下图是一个朴素的inception module</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.12.08.png" alt=""></p><p>采用的并行filter运算，有多个receptive field size的卷积，（$1\times1$,$3\times3$,$5\times5$）,从左向右第一个输出的size为$28\times28\times128$，第二个输出的size为$28\times28\times192$,第三个为$28\times28\times96$，第四个为$28\times28\times256$，concate以后的size为：$28\times28\times672$</p><p>缺点就是卷积运算过多：</p><p>$1\times1$ conv, 128=&gt;   $28\times28\times128\times1\times1\times256$</p><p> $3\times3$ conv, 192=&gt; $28\times28\times192\times3\times3\times256$</p><p> $5\times5$ conv, 96=&gt; $28\times28\times96\times5\times5\times256$</p><p>总共需要854M次运算</p><p>而且，最终的输出太大了！我们需要减少feature depth,可以用$1\times1$的卷积（$1\times1$ conv “bottleneck” layers）来解决，例如一个$56\times56\times64$的feature map经过32个$1\times1$以后，得到$56\times56\times32$,这样做就是将深度投影到较低的维度，（feature map的组合）</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.12.36.png" alt=""></p><p>卷积运算的数量：</p><p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$</p><p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$</p><p>$[1\times1 conv, 128] $ $28\times28\times128\times1\times1\times256$</p><p>$[3\times3 conv, 192]$  $28\times28\times192\times3\times3\times64$</p><p>$[5\times5 conv, 96]$  $28\times28\times96\times5\times5\times64$</p><p>$[1\times1 conv, 64]$  $28\times28\times64\times1\times1\times256$ </p><p>Total: 358M ops</p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><ul><li>每一层CONV层接BN</li><li>没有dropout</li></ul><p>具体见：</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-27%20%E4%B8%8B%E5%8D%883.17.12.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;AlexNet&quot;&gt;&lt;a href=&quot;#AlexNet&quot; class=&quot;headerlink&quot; title=&quot;AlexNet&quot;&gt;&lt;/a&gt;AlexNet&lt;/h2&gt;&lt;p&gt;结构：&lt;/p&gt;
&lt;p&gt;CONV1-&amp;gt;MAX POOL1-&amp;gt;NORM1-&amp;gt;CONV2
      
    
    </summary>
    
    
      <category term="notes" scheme="http://wulimengmeng.top/tags/notes/"/>
    
      <category term="computer vision" scheme="http://wulimengmeng.top/tags/computer-vision/"/>
    
  </entry>
  
</feed>
