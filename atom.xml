<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mowayao&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/16886497103686372c55fdd8ac89f177</icon>
  <subtitle>一往无前虎山行</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wulimengmeng.top/"/>
  <updated>2017-11-14T12:18:21.000Z</updated>
  <id>http://wulimengmeng.top/</id>
  
  <author>
    <name>Mowayao</name>
    <email>zpyao1992@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dual Path Networks</title>
    <link href="http://wulimengmeng.top/2017/11/14/Networks/"/>
    <id>http://wulimengmeng.top/2017/11/14/Networks/</id>
    <published>2017-11-14T10:05:50.000Z</published>
    <updated>2017-11-14T12:18:21.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-14%20%E4%B8%8B%E5%8D%886.25.45.png" alt="Architecture comparison of different networks"></p><p>This paper propose a novel deep CNN architecture called <strong>Dual Path Networks(DPN)</strong>. This idea is  based on the fact that the ResNet enables feature re-usage while DenseNet enable new features exploration which are both important for learning good feature representation.  The DPN can be formulated as:</p><script type="math/tex; mode=display">x^k = \sum_{t=1}^{k-1} f_t^k (h^t),\\ y^k = \sum_{t=1}^{k-1} v_t(h^t) = y^{k-1} +\phi^{k-1}(y^{k-1}),\\r^k=x^k+y^k,\\h^k=g^k(r^k)</script><p>where $x_k$ and $y_k$ denote the extracted information at k-th step from individual path, $v_t(\cdot)$is a feature learning function as $f_k^t(\cdot)$, $\phi_k(\cdot) = f_k(g_k(\cdot))$.  The dual path means the left side is DenseNet, the right side is ResNet. The block parameters are shared between DenseNet and ResNet. The outputs of DenseNet and ResNet will be concated as next block’s input.</p><p>This is the implementation of dual path block</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DualPathBlock</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, G, _type=<span class="string">'normal'</span>)</span>:</span></div><div class="line">        super(DualPathBlock, self).__init__()</div><div class="line">        self.num_1x1_c = num_1x1_c</div><div class="line"></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'proj'</span>:</div><div class="line">            key_stride = <span class="number">1</span></div><div class="line">            self.has_proj = <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'down'</span>:</div><div class="line">            key_stride = <span class="number">2</span></div><div class="line">            self.has_proj = <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> _type <span class="keyword">is</span> <span class="string">'normal'</span>:</div><div class="line">            key_stride = <span class="number">1</span></div><div class="line">            self.has_proj = <span class="keyword">False</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.has_proj:</div><div class="line">            self.c1x1_w = self.BN_ReLU_Conv(in_chs=in_chs, out_chs=num_1x1_c+<span class="number">2</span>*inc, kernel_size=<span class="number">1</span>, stride=key_stride)</div><div class="line"></div><div class="line">        self.layers = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'c1x1_a'</span>, self.BN_ReLU_Conv(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)),</div><div class="line">            (<span class="string">'c3x3_b'</span>, self.BN_ReLU_Conv(in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=<span class="number">3</span>, stride=key_stride, padding=<span class="number">1</span>, groups=G)),</div><div class="line">            (<span class="string">'c1x1_c'</span>, self.BN_ReLU_Conv(in_chs=num_3x3_b, out_chs=num_1x1_c+inc, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BN_ReLU_Conv</span><span class="params">(self, in_chs, out_chs, kernel_size, stride, padding=<span class="number">0</span>, groups=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="keyword">return</span> nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'norm'</span>, nn.BatchNorm2d(in_chs)),</div><div class="line">            (<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">            (<span class="string">'conv'</span>, nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        data_in = torch.cat(x, dim=<span class="number">1</span>) <span class="keyword">if</span> isinstance(x, list) <span class="keyword">else</span> x</div><div class="line">        <span class="keyword">if</span> self.has_proj:</div><div class="line">            data_o = self.c1x1_w(data_in)</div><div class="line">            data_o1 = data_o[:,:self.num_1x1_c,:,:]</div><div class="line">            data_o2 = data_o[:,self.num_1x1_c:,:,:]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            data_o1 = x[<span class="number">0</span>]</div><div class="line">            data_o2 = x[<span class="number">1</span>]</div><div class="line"></div><div class="line">        out = self.layers(data_in)</div><div class="line"></div><div class="line">        summ = data_o1 + out[:,:self.num_1x1_c,:,:]</div><div class="line">        dense = torch.cat([data_o2, out[:,self.num_1x1_c:,:,:]], dim=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> [summ, dense]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-14%20%E4%B8%8B%E5%8D%886.25.45.png&quot; alt=&quot;Archit
      
    
    </summary>
    
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Meet in the middle的一些实例</title>
    <link href="http://wulimengmeng.top/2017/11/12/meet-in-the-middle%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E4%BE%8B/"/>
    <id>http://wulimengmeng.top/2017/11/12/meet-in-the-middle的一些实例/</id>
    <published>2017-11-12T08:13:10.000Z</published>
    <updated>2017-11-12T08:15:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>Meet in the Middle是在搜索问题经常会用到的一个技巧，其核心思想就是解决一个A&lt;-&gt;B的问题，分别从A端和B端出发，向对方进发，当他们在中点相遇的时候，就找到了A&lt;-&gt;B的一个解。</p><h3 id="先看一道简单题："><a href="#先看一道简单题：" class="headerlink" title="先看一道简单题："></a>先看一道简单题：</h3><p><a href="http://codeforces.com/contest/888/problem/E" target="_blank" rel="external">CF888E Maximum Subsequence</a></p><p>You are given an array <em>a</em> consisting of <em>n</em> integers, and additionally an integer <em>m</em>. You have to choose some sequence of indices $b_1, b_2, …, b_k (1 \le b_1 \lt b_2 \lt … \lt b_k\le n)$ in such a way that the value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img"> is maximized. Chosen sequence can be empty.</p><p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p><p><strong>Input</strong></p><p>The first line contains two integers <em>n</em> and <em>m</em> ($1 \le n\le 35$, $1 \le m \le 10^9$).</p><p>The second line contains <em>n</em> integers $a_1, a_2, …, a_n$ ($1 \le a_i \le10^9$).</p><p><strong>Output</strong></p><p>Print the maximum possible value of <img src="http://codeforces.com/predownloaded/db/28/db283c0794aac433c817bad7534d99cc6287207c.png" alt="img">.</p><p>题目意思很简单，就是从一个大小为n的数组中挑选k个，使他们的和对m求余最大。</p><p>如果直接枚举a的所有子集，大小为$2^{35}$， 明显会超时。</p><p>如果利用meet in the middle的思路： 先枚举左边17，右边17，再让他们meet in the middle，然后利用求余的性质，左边和右边的和都小于m，进行排序，二分即可： $L_i+R_i \lt m$   or  $ m \lt L_i + R_i \lt 2*m$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">40</span>;</div><div class="line"></div><div class="line"><span class="keyword">int</span> a[maxn];</div><div class="line"></div><div class="line"><span class="keyword">int</span> L[<span class="number">1</span>&lt;&lt;<span class="number">18</span>], R[<span class="number">1</span>&lt;&lt;<span class="number">18</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="comment">//freopen("in", "r", stdin);</span></div><div class="line"><span class="keyword">int</span> n, m;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</div><div class="line"></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; a[i];</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> ans = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;<span class="number">18</span>); mask++) &#123;</div><div class="line"><span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">18</span>; j++) &#123;</div><div class="line"><span class="keyword">if</span> ((mask &gt;&gt; j) &amp; <span class="number">1</span>) &#123;</div><div class="line">res = (res+a[j]) % m;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">L[mask] = res;</div><div class="line">ans = max(ans, res);</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (n &lt;= <span class="number">18</span>) &#123;</div><div class="line"><span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> mask = <span class="number">0</span>; mask &lt; (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>)); mask++) &#123;</div><div class="line"><span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">18</span>; j &lt; n; j++) &#123;</div><div class="line"><span class="keyword">if</span> ((mask &gt;&gt; (j<span class="number">-18</span>)) &amp; <span class="number">1</span>) &#123;</div><div class="line">res = (res+a[j]) % m;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">R[mask] = res;</div><div class="line">ans = max(ans, res);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">int</span> Lsz = (<span class="number">1</span>&lt;&lt;<span class="number">18</span>);</div><div class="line"><span class="keyword">int</span> Rsz = (<span class="number">1</span>&lt;&lt;(n<span class="number">-18</span>));</div><div class="line">sort(R, R+Rsz);</div><div class="line"></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Lsz; i++) &#123;</div><div class="line"><span class="keyword">int</span> res = L[i];</div><div class="line"><span class="keyword">int</span> *t1 = upper_bound(R, R+Rsz, m<span class="number">-1</span>-res);</div><div class="line"><span class="keyword">if</span> (t1 != R) &#123;</div><div class="line">t1--;</div><div class="line">ans = max(ans, (res+(*t1))%m);</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> *t2 = upper_bound(R, R+Rsz, <span class="number">2</span>*m<span class="number">-1</span>-res);</div><div class="line"><span class="keyword">if</span> (t2 != R) &#123;</div><div class="line">t2--;</div><div class="line">ans = max(ans, (res+(*t2))%m);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="进阶一点"><a href="#进阶一点" class="headerlink" title="进阶一点"></a>进阶一点</h3><p><a href="https://community.topcoder.com/stat?c=problem_statement&amp;pm=11644&amp;rd=14548" target="_blank" rel="external">topcoder srm 523 AlphabetPath</a></p><p><strong>Problem Statement</strong></p><p>The original Latin alphabet contained the following 21 letters: </p><p>A B C D E F Z H I K L M N O P Q R S T V X</p><p>You are given a 2-dimensional matrix of characters represented by the String[] letterMaze. The i-th character of the j-th element of letterMaze will represent the character at row i and column j. The matrix will contain each of the 21 letters at least once. It may also contain empty cells marked as ‘.’ (quotes for clarity).</p><p>A path is a sequence of matrix elements such that the second element is (horizontally or vertically) adjacent to the first one, the third element is adjacent to the second one, and so on. No element may be repeated on a path. A Latin alphabet path is a path consisting of exactly 21 elements, each containing a different letter of the Latin alphabet. The letters are not required to be in any particular order.</p><p>Return the total number of Latin alphabet paths in the matrix described by letterMaze.</p><p>题目意思就是给定一个$R\times C$的矩阵，格子里要么是空，要么包含0~20的整数，长度为21的路径，每个整数恰出现一次， $R,C\le 21$</p><p>那么，我们枚举middle点，这样有$R\times C$种选择，假设Middle点为x，从Middle点出发DFS10步，令$S(P)$为不包含Middle点的10个格子的数值的集合。那么$S(P_1)\cup S(P_2)….\cup {x}$ = {0,1…20},那么整个时间复杂度就变成了$O(RC\times 4 \times 3^9)$</p><h3 id="密码学中的应用"><a href="#密码学中的应用" class="headerlink" title="密码学中的应用"></a>密码学中的应用</h3><h4 id="DES"><a href="#DES" class="headerlink" title="DES"></a>DES</h4><p>首先介绍一下DES（Data Encryption Standard），DES是一种分组的对称加密技术，具体见下图（coursera crypto stanford笔记）：</p><p><img src="http://ovshqtujw.bkt.clouddn.com/WechatIMG11.jpeg" alt="DES"></p><p>那么如何attack DES呢？</p><p>Lemma: Suppose that DES is an ideal cipher ($2^{56}$ random invertible functions, key是56位)</p><p>为什么不能用double DES呢？因为我们可以用meet in the middle attack来攻击：</p><p>对于double DES来说：</p><ol><li>我们需要找到这样的$k_1$和$k_2$：$E(k_1, E(k_2, M))=C$,这和$E(k_2, M) = D(k_1, C)$一个意思</li><li>首先，我们用表M记录$k_2$和$C^\prime=DES(k_2, M)$的所有值，时间复杂度为$O(2^{56})$</li><li>然后我们就可以暴力枚举$k_1$，计算$C^{\prime\prime} =DES^{-1}(k_1, C)$, 看是否有对应的值在表中</li><li>这样attack的时间复杂度就变成了$O(2^{56}+2^{56}) \lt O(2^{63})$ ,这比期望的$2^{112}$要小很多，以及空间复杂度为$O(2^{56})$。</li></ol><p>而换成3DES就没有这样的问题了！</p><p>$C = E(K_3, D(K_2, E(K_1,P) ) ) $</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Meet in the Middle是在搜索问题经常会用到的一个技巧，其核心思想就是解决一个A&amp;lt;-&amp;gt;B的问题，分别从A端和B端出发，向对方进发，当他们在中点相遇的时候，就找到了A&amp;lt;-&amp;gt;B的一个解。&lt;/p&gt;
&lt;h3 id=&quot;先看一道简单题：&quot;&gt;&lt;a h
      
    
    </summary>
    
    
      <category term="algorithms" scheme="http://wulimengmeng.top/tags/algorithms/"/>
    
  </entry>
  
  <entry>
    <title>GatedRNN</title>
    <link href="http://wulimengmeng.top/2017/11/12/GatedRNN/"/>
    <id>http://wulimengmeng.top/2017/11/12/GatedRNN/</id>
    <published>2017-11-12T05:35:24.000Z</published>
    <updated>2017-11-12T05:41:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gated-RNN"><a href="#Gated-RNN" class="headerlink" title="Gated RNN"></a>Gated RNN</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><script type="math/tex; mode=display">h^\prime, y = f(h, x), h^\prime = \sigma(W^hh + W^i x), y = \sigma(W^oh^\prime)</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.11.44.png" alt="RNN"></p><p>下面是RNN的实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></div><div class="line"><span class="string">  activation function.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for this timestep, of shape (N, D).</span></div><div class="line"><span class="string">  - prev_h: Hidden state from previous timestep, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h = np.tanh(x.dot(Wx)+prev_h.dot(Wh)+b)</div><div class="line">  cache = (next_h,x,prev_h,Wx,Wh)</div><div class="line">  <span class="keyword">return</span> next_h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for a single timestep of a vanilla RNN.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dnext_h: Gradient of loss with respect to next hidden state</span></div><div class="line"><span class="string">  - cache: Cache object from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradients of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></div><div class="line"><span class="string">  - db: Gradients of bias vector, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  next_h,x,prev_h,Wx,Wh = cache</div><div class="line">  dout = dnext_h*(<span class="number">1</span>-next_h**<span class="number">2</span>)</div><div class="line">  db = np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">  dx = dout.dot(Wx.T)</div><div class="line">  dprev_h = dout.dot(Wh.T)</div><div class="line">  dWx = np.dot(x.T,dout)</div><div class="line">  dWh = np.dot(prev_h.T,dout)</div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the RNN forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data for the entire timeseries, of shape (N, T, D).</span></div><div class="line"><span class="string">  - h0: Initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">  - b: Biases of shape (H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></div><div class="line"><span class="string">  - cache: Values needed in the backward pass</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  N,T,D = x.shape</div><div class="line">  N,H = h0.shape</div><div class="line">  cache = []</div><div class="line">  prev_h = h0</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">    prev_h,cache_n = rnn_step_forward(x[:,t,:],prev_h,Wx,Wh,b)</div><div class="line">    cache.append(cache_n)</div><div class="line">    h[:,t,:] = prev_h</div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of all hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of inputs, of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  N,T,H = dh.shape</div><div class="line">  N,D = cache[<span class="number">0</span>][<span class="number">1</span>].shape</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dWx = np.zeros((D,H))</div><div class="line">  dWh = np.zeros((H,H))</div><div class="line">  db = np.zeros((H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">    dx[:,t,:],dprev_h, dWx_n, dWh_n, db_n = rnn_step_backward(dprev_h+dh[:,t,:],cache[t])</div><div class="line">    dWh += dWh_n</div><div class="line">    dWx += dWx_n</div><div class="line">    db += db_n</div><div class="line">  dh0 = dprev_h</div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure><h3 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h3><script type="math/tex; mode=display">h^\prime, y = f_1(h,x)\\b^\prime, c = f_2(b, y)\\....</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.13.12.png" alt="Deep RNN"></p><h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><script type="math/tex; mode=display">h^\prime, a = f_1(h, x), b^\prime, c= f_2(b, x), y = f_3(a, c)</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.15.10.png" alt="双向RNN"></p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><script type="math/tex; mode=display">z^i = \tanh(W^ix^t+W^ih^{t-1})\\z^f=\tanh(W^fx^t+W^fh^{t-1})\\z^o=\tanh(W^0x^t+W^oh^{t-1})\\</script><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.18.36.png" alt="LSTM"></p><p>对比分析：</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.23.59.png" alt="LSTM对比"></p><p>最左边的是标准的LSTM， 左边第二个是GRU， </p><p>可以看出： 没有output gate，forget gate, input gate, input activation function, output activation function都会对结果变差。forget gate和关于$c^t$的$\tanh$激活函数对性能影响较大。</p><p>下面是LSTM的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for a single timestep of an LSTM.</span></div><div class="line"><span class="string">  The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">  a minibatch size of N.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data, of shape (N, D)</span></div><div class="line"><span class="string">  - prev_h: Previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - prev_c: previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases, of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - next_c: Next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Tuple of values needed for backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  next_h, next_c, cache = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for a single timestep of an LSTM.        #</span></div><div class="line">  <span class="comment"># You may want to use the numerically stable sigmoid implementation above.  #</span></div><div class="line">  <span class="comment">#############################################################################</span></div><div class="line">  a = x.dot(Wx)+prev_h.dot(Wh)+b</div><div class="line">  N,H = prev_h.shape</div><div class="line">  i = sigmoid(a[:,:H])</div><div class="line">  f = sigmoid(a[:,H:<span class="number">2</span>*H])</div><div class="line">  o = sigmoid(a[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div><div class="line">  g = np.tanh(a[:,<span class="number">3</span>*H:])</div><div class="line"></div><div class="line">  next_c = f*prev_c + i*g</div><div class="line">  next_h = o*np.tanh(next_c)</div><div class="line">  cache = (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h)</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> next_h, next_c, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass foLSTM: forward</span></div><div class="line"><span class="string">  - dnext_h: Gradients of next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dnext_c: Gradients of next cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data, of shape (N, D)</span></div><div class="line"><span class="string">  - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">  - dprev_c: Gradient of previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dprev_h, dprev_c, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (x,i,f,o,g,next_c,next_h,Wx,Wh,b,a,prev_c,prev_h) = cache</div><div class="line">  (N,H) = dnext_h.shape</div><div class="line">  (N,D) = x.shape</div><div class="line"></div><div class="line"></div><div class="line">  dx = np.zeros(x.shape)</div><div class="line">  dprev_c = np.zeros(prev_c.shape)</div><div class="line">  dprev_h = np.zeros(prev_h.shape)</div><div class="line">  dWx = np.zeros(Wx.shape)</div><div class="line">  dWh = np.zeros(Wh.shape)</div><div class="line">  db = np.zeros(b.shape)</div><div class="line"></div><div class="line"></div><div class="line">  di = dnext_c*g</div><div class="line">  df = dnext_c*prev_c</div><div class="line">  do = dnext_h*np.tanh(next_c)</div><div class="line">  dg = dnext_c*i</div><div class="line"></div><div class="line">  da = np.zeros(a.shape)</div><div class="line"></div><div class="line">  da[:,:H] = di*i*(<span class="number">1</span>-i) <span class="comment">#i</span></div><div class="line">  da[:,H:<span class="number">2</span>*H] = df*f*(<span class="number">1</span>-f) <span class="comment">#f</span></div><div class="line">  da[:,<span class="number">2</span>*H:<span class="number">3</span>*H] = do*o*(<span class="number">1</span>-o) <span class="comment">#o</span></div><div class="line">  da[:,<span class="number">3</span>*H:] = dg*(<span class="number">1</span>-g**<span class="number">2</span>) <span class="comment">#g</span></div><div class="line"></div><div class="line">  dprev_h = np.dot(da,Wh.T)</div><div class="line">  dWx = np.dot(x.T,da)</div><div class="line">  dWh = np.dot(prev_h.T,da)</div><div class="line">  db = np.sum(da,axis=<span class="number">0</span>)</div><div class="line">  dprev_c = dnext_c*f</div><div class="line">  dx = np.dot(da,Wx.T)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Forward pass for an LSTM over an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">  sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></div><div class="line"><span class="string">  size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">  the LSTM forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string">  Note that the initial cell state is passed as input, but the initial cell</span></div><div class="line"><span class="string">  state is set to zero. Also note that the cell state is not returned; it is</span></div><div class="line"><span class="string">  an internal variable to the LSTM and is not accessed from outside.</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - x: Input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - h0: Initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></div><div class="line"><span class="string">  - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></div><div class="line"><span class="string">  - b: Biases of shape (4H,)</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values needed for the backward pass.</span></div><div class="line"><span class="string">  """</span></div><div class="line">  h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,T,D) = x.shape</div><div class="line">  (N,H) = h0.shape</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  cache = []</div><div class="line">  prev_c = np.zeros((N,H))</div><div class="line">  prev_h = h0</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">      prev_h,prev_c,cache_n = lstm_step_forward(x[:,t,:],prev_h,prev_c,Wx,Wh,b)</div><div class="line">      cache.append(cache_n)</div><div class="line">      h[:,t,:] = prev_h</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line"><span class="string">  Backward pass for an LSTM over an entire sequence of data.]</span></div><div class="line"><span class="string">  Inputs:</span></div><div class="line"><span class="string">  - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">  - cache: Values from the forward pass</span></div><div class="line"><span class="string">  Returns a tuple of:</span></div><div class="line"><span class="string">  - dx: Gradient of input data of shape (N, T, D)</span></div><div class="line"><span class="string">  - dh0: Gradient of initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">  - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></div><div class="line"><span class="string">  - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></div><div class="line"><span class="string">  - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">  """</span></div><div class="line">  dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"></div><div class="line">  (N,D) = cache[<span class="number">0</span>][<span class="number">0</span>].shape</div><div class="line">  (N,T,H) = dh.shape</div><div class="line"></div><div class="line">  dprev_c = np.zeros((N,H))</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dh0 = np.zeros((N,H))</div><div class="line">  dWx = np.zeros((D,<span class="number">4</span>*H))</div><div class="line">  dWh = np.zeros((H,<span class="number">4</span>*H))</div><div class="line">  db= np.zeros((<span class="number">4</span>*H,))</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line"></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">      dx_n, dprev_h, dprev_c, dWx_n, dWh_n, db_n = lstm_step_backward(dh[:,t,:]+dprev_h,dprev_c,cache[t])</div><div class="line">      dWx += dWx_n</div><div class="line">      dWh_n += dWh_n</div><div class="line">      db += db_n</div><div class="line">      dx[:,t,:] = dx_n</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>z在GRU充当的是LSTM里面forget gate和input gate一样的作用，将两者耦合在一起。</p><p><img src="http://ovshqtujw.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-12%20%E4%B8%8B%E5%8D%881.19.17.png" alt="GRU"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Gated-RNN&quot;&gt;&lt;a href=&quot;#Gated-RNN&quot; class=&quot;headerlink&quot; title=&quot;Gated RNN&quot;&gt;&lt;/a&gt;Gated RNN&lt;/h2&gt;&lt;h3 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink
      
    
    </summary>
    
    
      <category term="RNN" scheme="http://wulimengmeng.top/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://wulimengmeng.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>mixup-Beyond Empirical Risk Minimization</title>
    <link href="http://wulimengmeng.top/2017/11/01/mixup/"/>
    <id>http://wulimengmeng.top/2017/11/01/mixup/</id>
    <published>2017-11-01T13:40:57.000Z</published>
    <updated>2017-11-02T23:28:32.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Gist</strong>: The authors propose a new training strategy  dubbed <strong>mixup</strong> that trains a neural network on convex combinations of pairs of examples and their labels and improves the generalization of state-of-the-art neural network architectures.    </p><p>​    </p><p><strong>Pytorch Code</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (x1, y1), (x2, y2) <span class="keyword">in</span> zip(loader1, loader2): </div><div class="line">  lam = numpy.random.beta(alpha, alpha)</div><div class="line">x = Variable(lam * x1 + (<span class="number">1.</span> - lam) * x2)</div><div class="line">y = Variable(lam * y1 + (<span class="number">1.</span> - lam) * y2) optimizer.zero_grad()</div><div class="line">    loss(net(x), y).backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure><p><strong>Empirical Risk Minimization</strong></p><p>We need to minimize the <strong>expected risk</strong>, that is the average of the loss function $l$ over the data distribution $P$</p><script type="math/tex; mode=display">R(f ) = \int l(f (x), y)dP (x, y)</script><p>$l$ is the loss function, $P(x,y)$ is a joint data distribution, $f\in F$ is a function that describes the relationship between a random vector X and a random target vector Y .</p><p>Unsually, the distribution of P is unknown. In most pracitical situation, we may approximate $P$ by the <strong><em>empirical distribution</em></strong>, though it is easy to compute, it ofen leads to the undesirable behaviour of $f$ outside the training data.</p><script type="math/tex; mode=display">P_\sigma(x,y)=\frac{1}{n}\sum_{i=1}^{n}\sigma(x=x_i, y=y_i)</script><p>where $\sigma(x = x_i, y = y_i)$ is a Dirac mass centered at $(x_i, y_i)$</p><script type="math/tex; mode=display">R_\sigma(f) = \frac{1}{n}\sum_{i=1}^nl(f(x_i), y_i)</script><p><strong>Vicinal Risk Minimization</strong></p><script type="math/tex; mode=display">P_v (\widetilde{x}, \widetilde{y})=\frac{1}{n}\sum_{i=1}^nv(\widetilde{x}, \widetilde{y}|x_i,y_i)</script><p>where $v(\widetilde{x}, \widetilde{y}|x_i,y_i)$ is  a vicinity distribution that measures the probability of finding the virtual feature-target pair $(\widetilde{x}, \widetilde{y})$ in the vicinity of the training feature-target pair $(x_i,y_i)$</p><p>This paper propose a generic vicinal distribution, <strong><em>mixup</em></strong>:</p><script type="math/tex; mode=display">\mu(\widetilde{x}, \widetilde{y}|x_i,y_i)=\frac{1}{n}\sum_j^n\mathbb{E}_\lambda[\sigma(\widetilde{x}=\lambda \cdot x_i+(1-\lambda)\cdot x_j,\widetilde{y} =\lambda \cdot y_i + (1-\lambda) \cdot y_j)]</script><p>where $\lambda \sim Beta(\alpha, \alpha)$ , for $\alpha \in (0, \infty)$Sampling from the mixup vicinal distribution:</p><script type="math/tex; mode=display">\widetilde{x} = \lambda \cdot x_i + (1 − \lambda)\cdot x_j</script><script type="math/tex; mode=display">\widetilde{y} = \lambda \cdot y_i + (1 − \lambda)\cdot y_j</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Gist&lt;/strong&gt;: The authors propose a new training strategy  dubbed &lt;strong&gt;mixup&lt;/strong&gt; that trains a neural network on convex 
      
    
    </summary>
    
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
      <category term="deep learning" scheme="http://wulimengmeng.top/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Single Shot Scale-invariant Face Detector</title>
    <link href="http://wulimengmeng.top/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/"/>
    <id>http://wulimengmeng.top/2017/11/01/Single-Shot-Scale-invariant-Face-Detector/</id>
    <published>2017-11-01T11:16:56.000Z</published>
    <updated>2017-11-01T12:01:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>The authors propose to tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besides, they try to improve the recall rate of small faces by a scale compensation anchor matching strategy. Max-out background label is used to reduce the false positive rate of small faces.</p><p>Key points:</p><ul><li>VGG net (throgh Pool5 layer) and some extra convolutional layers</li><li>Anchor  is 1:1 aspect ratio (face annotation)</li><li>two stages to improve the anchor matching strategy<ul><li>stage one: decrese the jaccord overlap threshold from 0.5 to 0.35</li><li>stage two: decrese the threshold to 0.1 and sort to select the top-N</li></ul></li><li>max-out operation is performed on the background label scores</li></ul><p>model architecture:</p><p><img src="http://7xkgro.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-01%20%E4%B8%8B%E5%8D%887.46.53.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The authors propose to tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besi
      
    
    </summary>
    
    
      <category term="paper notes" scheme="http://wulimengmeng.top/tags/paper-notes/"/>
    
      <category term="face detection" scheme="http://wulimengmeng.top/tags/face-detection/"/>
    
  </entry>
  
  <entry>
    <title>A List of Saliency Detection Papers</title>
    <link href="http://wulimengmeng.top/2017/10/20/A-List-of-Saliency-Detection-Papers/"/>
    <id>http://wulimengmeng.top/2017/10/20/A-List-of-Saliency-Detection-Papers/</id>
    <published>2017-10-20T03:29:37.000Z</published>
    <updated>2017-10-20T03:49:59.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Deep%20Spatial%20Contextual%20Long-term%20Recurrent%20Convolutional%20Network%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/A%20Fast%20and%20Compact%20Saliency%20Score%20Regression%20Network%20Based%20on%20Fully%20Convolutional%20Network.pdf" target="_blank" rel="external">A Fast and Compact Saliency Score Regression Network Based on Fully Convolutional Network</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Amulet.pdf" target="_blank" rel="external">Amulet</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/DHSNet:%20Deep%20Hierarchical%20Saliency%20Network%20for%20Salient%20Object%20Detection%20.pdf" target="_blank" rel="external">DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Group-wise%20Deep%20Co-saliency%20Detection.pdf" target="_blank" rel="external">Group-wise Deep Co-saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Large-Scale%20Optimization%20of%20Hierarchical%20Features%20for%20Saliency%20Prediction%20in%20Natural%20Images.pdf" target="_blank" rel="external">Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Learning%20Uncertain%20Convolutional%20Features%20for%20Accurate%20Saliency%20Detection.pdf" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/PiCANet.pdf" target="_blank" rel="external">PiCANet</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Recurrent%20Attentional%20Networks%20for%20Saliency%20Detection.pdf" target="_blank" rel="external">Recurrent Attentional Networks for Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/SalGAN:%20Visual%20Saliency%20Prediction%20with%20Generative%20Adversarial%20Networks.pdf" target="_blank" rel="external">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Forward%20and%20Backward%20Cues%20in%20Deep-CNNs.pdf" target="_blank" rel="external">Saliency Detection by Forward and Backward Cues in Deep-CNNs</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Saliency%20Detection%20by%20Multi-Context%20Deep%20Learning.pdf" target="_blank" rel="external">Saliency Detection by Multi-Context Deep Learning.pdf</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Shallow%20and%20Deep%20Convolutional%20Networks%20for%20Saliency%20Prediction.pdf" target="_blank" rel="external">Shallow and Deep Convolutional Networks for Saliency Prediction</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Supervised%20Adversarial%20Networks%20for%20Image%20Saliency%20Detection.pdf" target="_blank" rel="external">Supervised Adversarial Networks for Image Saliency Detection</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Two-Stream%20Convolutional%20Networks%20for%20Dynamic%20Saliency%20Prediction.pdf" target="_blank" rel="external">Two-Stream Convolutional Networks for Dynamic Saliency Prediction</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Detection%20Based%20on%20Multiscale%20Deep%20CNN%20Features.pdf" target="_blank" rel="external">Visual Saliency Detection Based on Multiscale Deep CNN Features</a></li><li><a href="http://7xkgro.com1.z0.glb.clouddn.com/Visual%20Saliency%20Prediction%20Using%20a%20Mixture%20of%20Deep%20Neural%20Networks.pdf" target="_blank" rel="external">Visual Saliency Prediction Using a Mixture of Deep Neural Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://7xkgro.com1.z0.glb.clouddn.com/A%20Deep%20Spatial%20Contextual%20Long-term%20Recurrent%20Convolutional%20Network%20
      
    
    </summary>
    
    
      <category term="paper" scheme="http://wulimengmeng.top/tags/paper/"/>
    
  </entry>
  
</feed>
