<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Network In Network算法笔记 | Mowayao&#39;s Blog</title>
  <meta name="author" content="Mowayao">
  
  <meta name="description" content="论文：Network In Network
作者：Min Lin, Qiang Chen, Shuicheng Yan  ICLR 2014
链接：https://arxiv.org/pdf/1312.4400.pdf
Idea: 传统的CNN一般就是通过linear filter和输入的每个和fi">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Network In Network算法笔记"/>
  <meta property="og:site_name" content="Mowayao&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110229492-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Mowayao&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class="fa fa-user"></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> Network In Network算法笔记</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>论文：Network In Network</p>
<p>作者：Min Lin, Qiang Chen, Shuicheng Yan  ICLR 2014</p>
<p>链接：https://arxiv.org/pdf/1312.4400.pdf</p>
<p><strong>Idea:</strong> 传统的CNN一般就是通过linear filter和输入的每个和filter大小相同的local patches做内积，然后再跟一个非线性激活函数。CNN的linear filter对于输入的每个local patch，是一个GLM（generalized linear model）。作者argue：GLM的抽象层次比较低(the level of abstraction of GLM is low)，这里的level of abstrction可以理解成不变性的抽象层次（invariant to the variants of some concepts）。GLM依赖于data或者concept线性可分的assumption，当data或者concept线性不可分的时候抽象能力就大大下降，而实际上现实中图像数据往往是low-dim manifold in high-dim space，所以往往是线性不可分的。因此作者提出用一些非线性的函数逼近器(nonlinear function approximator)来代替GLM，从而能够提取local patches更抽象的特征，提高模型对于local patch的判别能力（discrimminability）。Figure 1就是传统的CNN和NIN的比较。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkplf4900j319g0kigpu.jpg" alt=""></p>
<h5>模型细节</h5>
<p>本质上是将convolution -&gt; relu替换为convolution -&gt; relu -&gt; convolution (1x1 filter) -&gt; relu</p>
<p>对于$1\times 1$的卷积，在二维的情况下只是起到scale的作用，例如输入是$[32×32]$做1×1的卷积，只是对输入的每个元素做放大或者缩小，而如果输入是$[32×32×3]$，再做1×1的卷积，那么它其实就是对该位置的所有通道的线性组合，也可以叫做feature pooling或coordinate-dependent transformation。</p>
<p>最大的贡献是:</p>
<ol>
<li>$1\times 1$ 卷积的用法</li>
<li>Global average pooling的应用</li>
</ol>
<p>优点：</p>
<ol>
<li>local patch的抽象能力强</li>
<li>通过global average pooling减少overfitting</li>
<li>参数少,用GAP代替全连接层</li>
</ol>
<p><strong>MLP Convolution Layers</strong></p>
<p>作者选择多层的感知机(其实也就是 $1\times1$  的卷积层)作为function approximator，并列举了两个理由：</p>
<ol>
<li>和CNN兼容，可以用BP训练</li>
<li>自身可以作为一个deep model，可以用 $1\times 1$ 的卷积替代</li>
</ol>
<p>传统的CNN的feature maps的计算如下,ReLU为激活函数：
$$
f_{i,j,k} = \max(w_k^Tx_{i,j},0)
$$
maxout层feature maps的计算如下：
$$
f_{i,j,k}=\max_m(w_{k_m}^Tx_{i,j})
$$
maxout其实就是ReLU和Leaky ReLU的扩展，提升非线性能力，弥补了两者的缺点，例如应用ReLU激活函数，训练到后面，大部分neron会“挂掉”，因为已经死掉的neuron不会再有梯度了！除此之外，maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fnlus4z8aaj30e904d74l.jpg" alt=""></p>
<p>mlpconv layer的计算如下：
$$
f_{i,j,k_1}^1=\max({w_{k_1}^1}^Tx_{i,j}+b_{k_1},0) \ ... \ f_{i,j,k_n}^n=\max({w_{k_n}^n}^Tx_{i,j}+b_{k_n},0)
$$</p>
<p>n是多层感知机的层数。Figure 2是NIN的整体结构。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fnkq60369mj31780gsn1j.jpg" alt=""></p>
<p><strong>Global Average Pooling</strong></p>
<p>传统的CNN网络在做完所有卷积运算后，会把feature maps拉成一条向量，然后再接几层全连接层做分类。这样的问题是，多层的全连接经常会overfitting，当然也可以加Dropout来作为regularizer提高泛化性。作者提出global average pooling来取代全连接层，顾名思义，就是对每个feature map求全局平均，这样整个输出就变成了长度为feature maps深度的向量，这样能够有更好的空间不变形。除此之外，因为减少了待优化的参数避免了过拟合的发生。</p>
<h5>实现</h5>
<p><strong>Pytorch</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.num_classes = num_classes</div><div class="line">        self.classifer = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">160</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">160</span>,  <span class="number">96</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.Dropout(<span class="number">0.5</span>),</div><div class="line"></div><div class="line">                nn.Conv2d(<span class="number">96</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.Dropout(<span class="number">0.5</span>),</div><div class="line"></div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(<span class="number">192</span>,  num_classes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.AvgPool2d(kernel_size=<span class="number">8</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</div><div class="line">                )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.classifer(x)</div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_classes)</div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure></p>
<h5>实验</h5>
<p>作者在CIFAR-10, CIFAR-100, SVHN和MNIST上进行测试。</p>
<p><strong>CIFAR-10</strong>: 50,000个训练样本，10,000个测试样本，图像大小为 $32\times32$ ，实验结果见Table 1。Dropout和data augmentation对结果提升明显。It turns out in our experiment that using dropout in between the mlpconv layers in NIN boosts the performance  of  the  network  by  improving  the  generalization  ability  of  the  model.</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fnkr9b10ylj30xk0fe0wc.jpg" alt=""></p>
<p><strong>CIFAR-100</strong>: 数据规模和图像大小和cifar-10一样，不同的是它有100类，结果见Table 2。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fnkrewcgz6j30fc066aaq.jpg" alt=""></p>
<p><strong>SVHN</strong>：包含630,420张 $32\times32$ 的图像，将其分类训练集，验证集，测试集。具体结果见Table 3。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fnkrhat3qrj30fy07wt9o.jpg" alt=""></p>
<p><strong>MNIST</strong>：包含60,000张$28\times 28$的训练图像，和10,000张测试图像，具体结果见Table 4。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fnkrjy6lgqj30ey05ejrx.jpg" alt=""></p>
<p><strong>Global Average Pooling</strong></p>
<p>通过控制变量，发现GAP对结果的提升还是比较明显的，可以作为regularizer，具体见Table 5。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fnkrld8nt8j30g8056mxo.jpg" alt=""></p>
<p><strong>Visualization</strong></p>
<p>Figure 4 展示了一些样例图片采样自cifar-10和它们对应类别的features maps。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fnkrpb3zc7j30nu0f80y5.jpg" alt=""></p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2018/01/18/Visualizing-and-UnderstandingConvolutional-Networks笔记/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> 上一页</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2018/01/18/AlexNet算法笔记/" type="button" class="btn btn-default ">下一页<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">留言</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2018-01-18 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/algorithms/">algorithms<span>14</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/deep-learning/">deep learning<span>20</span></a></li> <li><a href="/tags/classfication/">classfication<span>3</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 Mowayao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$'], ['\[','\]'] ], 
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
   </html>
